<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>自然语言处理与机器翻译：从规则到智能的演化之路 | qmwneb946 的博客</title><meta name="author" content="qmwneb946"><meta name="copyright" content="qmwneb946"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="大家好，我是 qmwneb946，一名热爱探索技术与数学奥秘的博主。今天，我们将一同深入一个既充满挑战又令人着迷的领域——自然语言处理（NLP）与机器翻译（MT）。从早期生硬的直译，到如今流畅自然的智能翻译，这背后是数十载科研人员的智慧结晶，以及从语言学、统计学到深度学习的范式演变。 想象一下，你能够与世界上任何一个人无障碍地沟通，无论他们讲着何种语言。或者，计算机能够真正理解你的意图，而不仅仅是">
<meta property="og:type" content="article">
<meta property="og:title" content="自然语言处理与机器翻译：从规则到智能的演化之路">
<meta property="og:url" content="https://qmwneb946.dpdns.org/2025/07/18/2025-07-19-020856/index.html">
<meta property="og:site_name" content="qmwneb946 的博客">
<meta property="og:description" content="大家好，我是 qmwneb946，一名热爱探索技术与数学奥秘的博主。今天，我们将一同深入一个既充满挑战又令人着迷的领域——自然语言处理（NLP）与机器翻译（MT）。从早期生硬的直译，到如今流畅自然的智能翻译，这背后是数十载科研人员的智慧结晶，以及从语言学、统计学到深度学习的范式演变。 想象一下，你能够与世界上任何一个人无障碍地沟通，无论他们讲着何种语言。或者，计算机能够真正理解你的意图，而不仅仅是">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qmwneb946.dpdns.org/img/icon.png">
<meta property="article:published_time" content="2025-07-18T18:08:56.000Z">
<meta property="article:modified_time" content="2025-07-22T03:54:52.730Z">
<meta property="article:author" content="qmwneb946">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="数学">
<meta property="article:tag" content="自然语言处理与机器翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qmwneb946.dpdns.org/img/icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "自然语言处理与机器翻译：从规则到智能的演化之路",
  "url": "https://qmwneb946.dpdns.org/2025/07/18/2025-07-19-020856/",
  "image": "https://qmwneb946.dpdns.org/img/icon.png",
  "datePublished": "2025-07-18T18:08:56.000Z",
  "dateModified": "2025-07-22T03:54:52.730Z",
  "author": [
    {
      "@type": "Person",
      "name": "qmwneb946",
      "url": "https://github.com/qmwneb946"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qmwneb946.dpdns.org/2025/07/18/2025-07-19-020856/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '自然语言处理与机器翻译：从规则到智能的演化之路',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2845632165165414" crossorigin="anonymous"></script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="qmwneb946 的博客" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qmwneb946 的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">自然语言处理与机器翻译：从规则到智能的演化之路</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">自然语言处理与机器翻译：从规则到智能的演化之路<a class="post-edit-link" href="https://github.com/qmwneb946/blog/edit/main/source/_posts/2025-07-19-020856.md" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-18T18:08:56.000Z" title="发表于 2025-07-19 02:08:56">2025-07-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-22T03:54:52.730Z" title="更新于 2025-07-22 11:54:52">2025-07-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%95%B0%E5%AD%A6/">数学</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><p>大家好，我是 qmwneb946，一名热爱探索技术与数学奥秘的博主。今天，我们将一同深入一个既充满挑战又令人着迷的领域——自然语言处理（NLP）与机器翻译（MT）。从早期生硬的直译，到如今流畅自然的智能翻译，这背后是数十载科研人员的智慧结晶，以及从语言学、统计学到深度学习的范式演变。</p>
<p>想象一下，你能够与世界上任何一个人无障碍地沟通，无论他们讲着何种语言。或者，计算机能够真正理解你的意图，而不仅仅是识别关键词。这不再是科幻电影中的场景，而是我们正在逐步实现的未来。而这一切的核心，正是自然语言处理与机器翻译。</p>
<h3 id="第一部分：自然语言处理（NLP）基础：机器理解人类语言的基石">第一部分：自然语言处理（NLP）基础：机器理解人类语言的基石</h3>
<p>自然语言处理，顾名思义，是计算机科学、人工智能和计算语言学的一个交叉领域，旨在让计算机能够理解、解释、生成和处理人类语言。它不仅仅是简单地识别词语，而是要理解其背后的含义、情感、语境乃至人类的思维模式。</p>
<h4 id="什么是自然语言处理？">什么是自然语言处理？</h4>
<p>NLP 的目标是弥合人机交互的鸿沟。人类以自然语言进行交流，而计算机则使用结构化的数据和编程语言。NLP 的任务就是将这些非结构化、充满歧义的人类语言转化为计算机可以理解和处理的形式。</p>
<p>它的应用范围极其广泛，包括：</p>
<ul>
<li><strong>机器翻译：</strong> 将一种语言自动翻译成另一种语言。</li>
<li><strong>情感分析：</strong> 判断文本的情感倾向（积极、消极、中立）。</li>
<li><strong>文本摘要：</strong> 自动从长文本中提取关键信息并生成简洁摘要。</li>
<li><strong>问答系统：</strong> 理解用户问题并从知识库中检索或生成答案。</li>
<li><strong>语音识别与合成：</strong> 将口语转化为文本，或将文本转化为口语。</li>
<li><strong>信息检索：</strong> 搜索引擎背后的核心技术。</li>
<li><strong>聊天机器人与虚拟助手：</strong> 实现人机对话。</li>
</ul>
<h4 id="NLP-的核心挑战">NLP 的核心挑战</h4>
<p>人类语言的复杂性给 NLP 带来了诸多挑战：</p>
<ol>
<li><strong>歧义性 (Ambiguity)：</strong> 同一个词或句子在不同语境下可能有不同含义。
<ul>
<li>词汇歧义：例如“苹果”可以是水果，也可以是公司。</li>
<li>句法歧义：例如“我看到了用望远镜的男人”——是用望远镜看，还是男人拿着望远镜？</li>
<li>指代消解：例如“张三告诉李四他很高兴”，这个“他”指代谁？</li>
</ul>
</li>
<li><strong>多变性 (Variability)：</strong> 同一个意思可以用多种方式表达。</li>
<li><strong>语言演变 (Evolution)：</strong> 语言是活的，新词不断涌现，旧词含义可能改变。</li>
<li><strong>常识和世界知识 (Common Sense &amp; World Knowledge)：</strong> 理解语言往往需要大量的背景知识和常识推理，这对于机器而言极为困难。</li>
<li><strong>语法和句法结构 (Grammar &amp; Syntax)：</strong> 语言的结构复杂，规则众多且有例外。</li>
<li><strong>语用学 (Pragmatics)：</strong> 理解语言在特定情境下的实际意图和影响。</li>
</ol>
<h4 id="NLP-的传统方法：规则与统计">NLP 的传统方法：规则与统计</h4>
<p>在深度学习浪潮兴起之前，NLP 领域主要依赖于基于规则和基于统计的方法。</p>
<ul>
<li>
<p><strong>基于规则的方法 (Rule-Based Methods)：</strong></p>
<ul>
<li>核心思想：由语言学家和专家手动编写大量的语法规则、词典和模板。</li>
<li>优点：在特定、受限的领域内表现良好，易于理解和调试。</li>
<li>缺点：规则难以覆盖所有语言现象，构建和维护成本高昂，难以泛化到新领域，遇到例外情况时表现脆弱。</li>
</ul>
</li>
<li>
<p><strong>基于统计的方法 (Statistical Methods)：</strong></p>
<ul>
<li>核心思想：利用数学统计模型从大规模语料库中学习语言模式。不再依赖人工规则，而是通过数据来发现语言的概率分布。</li>
<li>核心技术：
<ul>
<li><strong>N-gram 模型：</strong> 预测下一个词出现的概率，基于前 N-1 个词。例如，二元模型 (Bigram) 考虑前一个词，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(w_i | w_{i-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。</li>
<li><strong>隐马尔可夫模型 (HMM)：</strong> 用于序列标注任务，如词性标注 (POS Tagging)。</li>
<li><strong>条件随机场 (CRF)：</strong> 比 HMM 更强大的序列标注模型，能够考虑更丰富的特征。</li>
</ul>
</li>
<li>优点：能够处理不确定性，对语言的变异性有更好的鲁棒性，更容易扩展到大规模数据。</li>
<li>缺点：需要大量标注数据，模型特征提取需要人工参与，难以捕捉长距离依赖关系。</li>
</ul>
</li>
</ul>
<p>例如，一个简单的文本分词和词性标注的传统流程可能包含：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk.tag <span class="keyword">import</span> pos_tag</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载NLTK的punkt分词器和averaged_perceptron_tagger词性标注器</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    nltk.data.find(<span class="string">&#x27;tokenizers/punkt&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> nltk.downloader.DownloadError:</span><br><span class="line">    nltk.download(<span class="string">&#x27;punkt&#x27;</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    nltk.data.find(<span class="string">&#x27;taggers/averaged_perceptron_tagger&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> nltk.downloader.DownloadError:</span><br><span class="line">    nltk.download(<span class="string">&#x27;averaged_perceptron_tagger&#x27;</span>)</span><br><span class="line"></span><br><span class="line">text = <span class="string">&quot;Apple is looking at buying U.K. startup for $1 billion.&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 文本分词 (Tokenization)</span></span><br><span class="line">tokens = word_tokenize(text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;分词结果:&quot;</span>, tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 词性标注 (Part-of-Speech Tagging)</span></span><br><span class="line">pos_tags = pos_tag(tokens)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;词性标注结果:&quot;</span>, pos_tags)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果示例：</span></span><br><span class="line"><span class="comment"># 分词结果: [&#x27;Apple&#x27;, &#x27;is&#x27;, &#x27;looking&#x27;, &#x27;at&#x27;, &#x27;buying&#x27;, &#x27;U.K.&#x27;, &#x27;startup&#x27;, &#x27;for&#x27;, &#x27;$&#x27;, &#x27;1&#x27;, &#x27;billion&#x27;, &#x27;.&#x27;]</span></span><br><span class="line"><span class="comment"># 词性标注结果: [(&#x27;Apple&#x27;, &#x27;NNP&#x27;), (&#x27;is&#x27;, &#x27;VBZ&#x27;), (&#x27;looking&#x27;, &#x27;VB&#x27;), (&#x27;at&#x27;, &#x27;IN&#x27;), (&#x27;buying&#x27;, &#x27;VBG&#x27;), (&#x27;U.K.&#x27;, &#x27;NNP&#x27;), (&#x27;startup&#x27;, &#x27;NN&#x27;), (&#x27;for&#x27;, &#x27;IN&#x27;), (&#x27;$&#x27;, &#x27;$&#x27;), (&#x27;1&#x27;, &#x27;CD&#x27;), (&#x27;billion&#x27;, &#x27;CD&#x27;), (&#x27;.&#x27;, &#x27;.&#x27;)]</span></span><br></pre></td></tr></table></figure>
<h4 id="NLP-的现代方法：深度学习的崛起">NLP 的现代方法：深度学习的崛起</h4>
<p>自2010年代中期以来，深度学习在 NLP 领域取得了突破性进展，彻底改变了研究范式。神经网络强大的特征学习能力，使得人工设计特征的需求大大降低，并能自动捕捉语言的复杂模式和长距离依赖。</p>
<ul>
<li><strong>词嵌入 (Word Embeddings)：</strong> 将词语映射到低维连续向量空间，相似的词在向量空间中距离相近。
<ul>
<li><strong>Word2Vec (Skip-gram, CBOW)：</strong> 谷歌在2013年提出的模型，通过预测上下文词或根据上下文预测中心词来学习词向量。</li>
<li><strong>GloVe (Global Vectors for Word Representation)：</strong> 基于全局词频统计和局部上下文窗口的方法。</li>
<li><strong>FastText：</strong> 在Word2Vec基础上加入了子词信息（n-gram），能更好地处理稀有词和未登录词。</li>
</ul>
</li>
<li><strong>循环神经网络 (RNN) 及其变体：</strong> 能够处理序列数据，尤其适用于语言这种具有时序依赖性的数据。
<ul>
<li><strong>长短期记忆网络 (LSTM) 和门控循环单元 (GRU)：</strong> 解决了传统 RNN 的梯度消失/爆炸问题，能够学习和记忆长距离依赖。</li>
</ul>
</li>
<li><strong>注意力机制 (Attention Mechanism)：</strong> 允许模型在处理序列时，对输入序列的不同部分赋予不同的权重，从而更好地捕捉关键信息。</li>
<li><strong>Transformer 架构：</strong> 彻底抛弃了循环和卷积结构，完全基于注意力机制，实现了并行化训练，成为当前 NLP 领域的主流模型。</li>
<li><strong>预训练语言模型 (Pre-trained Language Models)：</strong> 如 BERT, GPT, T5 等，通过在海量无标注文本上进行大规模预训练，学习通用的语言表示，然后通过微调 (fine-tuning) 适应下游任务，极大地推动了 NLP 的发展。</li>
</ul>
<p>这些深度学习技术为机器翻译的革命奠定了基础，让我们进入第二部分。</p>
<h3 id="第二部分：机器翻译（MT）简史与演进">第二部分：机器翻译（MT）简史与演进</h3>
<p>机器翻译，是 NLP 领域中最具挑战性也最引人注目的任务之一。它的目标是将一种自然语言（源语言）的文本或语音自动翻译成另一种自然语言（目标语言）。</p>
<h4 id="早期尝试：基于规则的机器翻译-RBMT">早期尝试：基于规则的机器翻译 (RBMT)</h4>
<p>机器翻译的历史可以追溯到二战后，当时的主要动机是军事情报翻译。最早的系统就是基于规则的。</p>
<ul>
<li><strong>工作原理：</strong>
<ol>
<li><strong>词法分析：</strong> 对源语言句子进行分词、词形还原等。</li>
<li><strong>句法分析：</strong> 解析源语言句子的语法结构，构建句法树。</li>
<li><strong>语义分析：</strong> 尝试理解句子的深层含义。</li>
<li><strong>转换规则：</strong> 根据语言学规则将源语言的结构和词汇映射到目标语言。这包括词典替换、词序调整、句法结构转换等。</li>
<li><strong>目标语言生成：</strong> 生成符合目标语言语法的句子。</li>
</ol>
</li>
<li><strong>核心思想：</strong> 假设语言翻译是一个可由明确定义的语言学规则系统来描述的过程。</li>
<li><strong>优点：</strong>
<ul>
<li>翻译结果在特定领域内可能非常精确和可控。</li>
<li>易于调试和理解规则的来源。</li>
<li>不需要大规模平行语料。</li>
</ul>
</li>
<li><strong>缺点：</strong>
<ul>
<li><strong>覆盖率低：</strong> 人工编写的规则无法穷尽所有语言现象和例外情况。</li>
<li><strong>可扩展性差：</strong> 增加规则或扩展到新领域成本极高。</li>
<li><strong>鲁棒性差：</strong> 对输入语法的微小偏离就可能导致翻译失败。</li>
<li><strong>译文生硬：</strong> 往往缺乏自然语言的流畅性和地道性。</li>
</ul>
</li>
</ul>
<h4 id="统计机器翻译-SMT-的黄金时代">统计机器翻译 (SMT) 的黄金时代</h4>
<p>20世纪90年代末，随着大规模平行语料库（如联合国文件、加拿大议会辩论记录等）的出现和计算能力的提升，统计机器翻译逐渐取代了基于规则的方法，成为主流。</p>
<ul>
<li><strong>核心思想：</strong> 将机器翻译视为一个统计推断问题。给定源语言句子 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span>，寻找最有可能的目标语言句子 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span>。这可以用贝叶斯公式表示：<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>arg</mi><mo>⁡</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>T</mi></munder><mi>P</mi><mo stretchy="false">(</mo><mi>T</mi><mi mathvariant="normal">∣</mi><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><mi>arg</mi><mo>⁡</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>T</mi></munder><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>S</mi><mi mathvariant="normal">∣</mi><mi>T</mi><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\arg\max_T P(T|S) = \arg\max_T \frac{P(S|T)P(T)}{P(S)} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4943em;vertical-align:-0.7443em;"></span><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em;"><span style="top:-2.3557em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7443em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em;"><span style="top:-2.3557em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7443em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
由于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(S)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span></span></span></span> 对于所有 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span> 都是常数，我们可以简化为：<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>arg</mi><mo>⁡</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>T</mi></munder><mi>P</mi><mo stretchy="false">(</mo><mi>S</mi><mi mathvariant="normal">∣</mi><mi>T</mi><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\arg\max_T P(S|T)P(T) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4943em;vertical-align:-0.7443em;"></span><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em;"><span style="top:-2.3557em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7443em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose">)</span></span></span></span></span></p>
其中：
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>S</mi><mi mathvariant="normal">∣</mi><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(S|T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose">)</span></span></span></span> 是<strong>翻译模型 (Translation Model)</strong>：衡量目标语言句子 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span> 能够生成源语言句子 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span> 的概率，它捕捉了两种语言之间的词语和短语对应关系。</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose">)</span></span></span></span> 是<strong>语言模型 (Language Model)</strong>：衡量目标语言句子 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span> 自身的流畅性和语法正确性。它确保生成的译文是自然流畅的。</li>
</ul>
</li>
<li><strong>主要流派：</strong>
<ul>
<li><strong>基于词的统计机器翻译 (Word-based SMT)：</strong> 最早的 SMT 模型，如 IBM Models。</li>
<li><strong>基于短语的统计机器翻译 (Phrase-based SMT, PBSMT)：</strong> 2000年代的主流。它不再只翻译单个词，而是将源语言句子切分成短语，然后查找短语对的翻译，并对短语进行重新排序。</li>
</ul>
</li>
<li><strong>训练过程：</strong>
<ol>
<li><strong>词对齐：</strong> 在平行语料中找出源语言词和目标语言词之间的对应关系。</li>
<li><strong>短语抽取：</strong> 基于词对齐，抽取频繁出现的短语对。</li>
<li><strong>模型训练：</strong> 训练翻译模型和语言模型，通常是 N-gram 语言模型。</li>
<li><strong>解码：</strong> 在翻译时，使用搜索算法（如集束搜索）找到最佳翻译路径。</li>
</ol>
</li>
<li><strong>优点：</strong>
<ul>
<li><strong>数据驱动：</strong> 能够自动从数据中学习复杂的语言模式。</li>
<li><strong>鲁棒性更强：</strong> 对输入的不规范性有更好的适应能力。</li>
<li><strong>翻译质量显著提升：</strong> 比 RBMT 更流畅自然。</li>
</ul>
</li>
<li><strong>缺点：</strong>
<ul>
<li><strong>特征工程：</strong> 仍然需要大量人工设计的特征来提高翻译质量。</li>
<li><strong>长距离依赖问题：</strong> 难以捕捉句子中相距较远的词之间的复杂依赖关系。</li>
<li><strong>短语独立性：</strong> 尽管是基于短语，但不同短语之间的联系仍然有限。</li>
<li><strong>计算复杂性：</strong> 解码过程涉及复杂的搜索。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SMT的简化概念：词对齐</span></span><br><span class="line"><span class="comment"># 假设我们有一个简单的词典，模拟翻译模型和语言模型</span></span><br><span class="line">translation_dict = &#123;</span><br><span class="line">    <span class="string">&quot;hello&quot;</span>: <span class="string">&quot;你好&quot;</span>,</span><br><span class="line">    <span class="string">&quot;world&quot;</span>: <span class="string">&quot;世界&quot;</span>,</span><br><span class="line">    <span class="string">&quot;how&quot;</span>: <span class="string">&quot;怎么样&quot;</span>,</span><br><span class="line">    <span class="string">&quot;are&quot;</span>: <span class="string">&quot;是&quot;</span>,</span><br><span class="line">    <span class="string">&quot;you&quot;</span>: <span class="string">&quot;你&quot;</span>,</span><br><span class="line">    <span class="string">&quot;good&quot;</span>: <span class="string">&quot;好&quot;</span>,</span><br><span class="line">    <span class="string">&quot;morning&quot;</span>: <span class="string">&quot;早上好&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟一个非常简单的语言模型，评估目标句子的流畅性</span></span><br><span class="line"><span class="comment"># 实际中会使用N-gram等模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simple_language_model</span>(<span class="params">phrase</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;你好 世界&quot;</span> <span class="keyword">in</span> phrase:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.9</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&quot;你 好世界&quot;</span> <span class="keyword">in</span> phrase: <span class="comment"># 错误的组合</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.5</span> <span class="comment"># 默认</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">translate_smt_concept</span>(<span class="params">english_sentence</span>):</span><br><span class="line">    english_words = english_sentence.lower().split()</span><br><span class="line">    translated_words = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 简单的词翻译</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> english_words:</span><br><span class="line">        translated_words.append(translation_dict.get(word, word)) <span class="comment"># 如果词典没有，则保留原词</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 简单的短语重排和流畅度评估（概念性，非实际SMT实现）</span></span><br><span class="line">    <span class="comment"># 假设我们知道 &quot;how are you&quot; 应该翻译成 &quot;你怎么样&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot; &quot;</span>.join(english_words) == <span class="string">&quot;how are you&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;你怎么样&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 否则，简单拼接并尝试评估语言模型</span></span><br><span class="line">    naive_translation = <span class="string">&quot; &quot;</span>.join(translated_words)</span><br><span class="line">    lm_score = simple_language_model(naive_translation)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;原始翻译: <span class="subst">&#123;naive_translation&#125;</span>, 语言模型分数: <span class="subst">&#123;lm_score&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> naive_translation <span class="comment"># 这里只是概念展示，实际SMT的解码器会进行复杂搜索</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(translate_smt_concept(&quot;Hello world&quot;))</span></span><br><span class="line"><span class="comment"># print(translate_smt_concept(&quot;How are you&quot;))</span></span><br></pre></td></tr></table></figure>
<h4 id="神经机器翻译-NMT-的革命">神经机器翻译 (NMT) 的革命</h4>
<p>进入2010年代中期，随着深度学习的兴起，循环神经网络（RNN）和卷积神经网络（CNN）开始被应用于机器翻译。2014年，Sutskever et al. 和 Cho et al. 几乎同时提出了基于序列到序列（Seq2Seq）模型的神经机器翻译框架，彻底改变了机器翻译的格局。</p>
<ul>
<li><strong>核心思想：</strong> 使用一个大型神经网络对整个源语言句子进行编码，生成一个上下文向量，然后用另一个神经网络从这个上下文向量解码生成目标语言句子。整个过程是一个端到端的学习过程，无需人工设计特征或短语。</li>
<li><strong>优点：</strong>
<ul>
<li><strong>端到端学习：</strong> 简化了翻译流程，不再需要独立的词对齐、短语抽取、语言模型等组件。</li>
<li><strong>更好地捕捉长距离依赖：</strong> RNNs（特别是 LSTM/GRU）能够更好地处理长序列信息。</li>
<li><strong>更流畅自然：</strong> 生成的译文更接近人工翻译的质量。</li>
<li><strong>通用性强：</strong> 同一个模型结构可以应用于不同的语言对。</li>
</ul>
</li>
<li><strong>缺点：</strong>
<ul>
<li><strong>“上下文向量”瓶颈：</strong> 传统的 Seq2Seq 模型将整个源句子压缩成一个固定长度的向量，对于长句子来说，这个向量可能无法完全捕获所有信息，导致信息丢失。</li>
<li><strong>训练速度：</strong> RNNs 的顺序计算特性使其难以并行化，训练速度较慢。</li>
</ul>
</li>
</ul>
<p>神经机器翻译的出现，使得机器翻译质量达到了前所未有的高度，甚至在某些语言对和特定领域超越了人工翻译的质量，开启了机器翻译的新篇章。</p>
<h3 id="第三部分：深入神经机器翻译：Transformer-的崛起">第三部分：深入神经机器翻译：Transformer 的崛起</h3>
<p>NMT 的核心在于其能够学习源语言和目标语言之间复杂的非线性映射关系。其中，<strong>序列到序列模型 (Seq2Seq)</strong> 是 NMT 的基础架构，而<strong>注意力机制 (Attention Mechanism)</strong> 和 <strong>Transformer 架构</strong>则将 NMT 推向了新的高峰。</p>
<h4 id="序列到序列模型-Seq2Seq-及其限制">序列到序列模型 (Seq2Seq) 及其限制</h4>
<p>Seq2Seq 模型通常由两个循环神经网络组成：一个<strong>编码器 (Encoder)</strong> 和一个<strong>解码器 (Decoder)</strong>。</p>
<ul>
<li><strong>编码器：</strong> 读取源语言输入序列 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">X = (x_1, x_2, \ldots, x_n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，将其编码成一个固定维度的上下文向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>。这个向量被认为是源句子的语义表示。</li>
<li><strong>解码器：</strong> 以这个上下文向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> 作为初始状态，并根据之前生成的词，逐步生成目标语言输出序列 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y = (y_1, y_2, \ldots, y_m)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。</li>
</ul>
<p>如下图所示的简化概念：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">源序列:  A -&gt; B -&gt; C (Encoder)</span><br><span class="line">             |</span><br><span class="line">             V</span><br><span class="line">上下文向量 C_vec</span><br><span class="line">             |</span><br><span class="line">             V</span><br><span class="line">目标序列:  D -&gt; E -&gt; F (Decoder)</span><br></pre></td></tr></table></figure>
<p><strong>数学表示（RNN Based）：</strong><br>
编码器通常是一个 RNN（如 LSTM 或 GRU），其隐藏状态在每个时间步更新：<br>
h_t = \text{RNN_Encoder}(x_t, h_{t-1})<br>
最终的上下文向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> 可以是编码器最后一个时间步的隐藏状态，或者所有隐藏状态的某种聚合。</p>
<p>解码器也是一个 RNN，它在每个时间步生成一个输出词，并更新其隐藏状态：<br>
s_t = \text{RNN_Decoder}(y_{t-1}, s_{t-1}, C)<br>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>y</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><msub><mi>W</mi><mi>s</mi></msub><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(y_t|y_{&lt;t}, S) = \text{softmax}(W_s s_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<p><strong>限制：</strong> 上下文向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> 必须编码整个源句子的信息。对于长句子，固定长度的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> 会成为信息瓶颈，导致模型在翻译长句子时表现不佳，容易丢失细节。这就像试图用一个瓶子装下整条河流的信息。</p>
<h4 id="注意力机制-Attention-Mechanism-的引入">注意力机制 (Attention Mechanism) 的引入</h4>
<p>为了解决 Seq2Seq 模型的“瓶颈”问题，注意力机制被引入。它允许解码器在生成每个目标词时，动态地“关注”源句子中不同部分的对齐信息。</p>
<ul>
<li><strong>核心思想：</strong> 当解码器生成目标序列中的一个词 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 时，它不再只依赖于一个固定的上下文向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>，而是会根据当前解码器的状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">s_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和源编码器在不同时间步的隐藏状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">h_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> 来计算一个对齐分数（或注意力权重）。这些权重表示源序列中哪些部分与当前要生成的词最相关。</li>
<li><strong>工作原理：</strong>
<ol>
<li><strong>计算对齐分数 (Alignment Scores/Energies)：</strong> 对于解码器的当前隐藏状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">s_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和编码器的每一个隐藏状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">h_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>，计算一个分数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">e_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>，表示 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">h_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> 对于生成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的重要性。<br>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mtext>score</mtext><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>h</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">e_{ij} = \text{score}(s_{i-1}, h_j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord text"><span class="mord">score</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><br>
常见的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>score</mtext></mrow><annotation encoding="application/x-tex">\text{score}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord text"><span class="mord">score</span></span></span></span></span> 函数有：
<ul>
<li>点积：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>j</mi><mi>T</mi></msubsup><msub><mi>s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_j^T s_{i-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2361em;vertical-align:-0.3948em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span></li>
<li>Luận 模型：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>v</mi><mi>a</mi><mi>T</mi></msubsup><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>W</mi><mi>a</mi></msub><mo stretchy="false">[</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">;</mo><msub><mi>h</mi><mi>j</mi></msub><mo stretchy="false">]</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">v_a^T \tanh(W_a [s_{i-1}; h_j])</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1274em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">])</span></span></span></span></li>
</ul>
</li>
<li><strong>归一化注意力权重 (Attention Weights)：</strong> 使用 softmax 函数将这些分数转化为概率分布 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>，确保所有权重之和为1。<br>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>e</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{n} \exp(e_{ik})}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.6023em;vertical-align:-0.57em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0323em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7047em;"><span style="top:-2.1786em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-2.8971em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3214em;"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">ik</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.5073em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.57em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li>
<li><strong>计算上下文向量 (Context Vector)：</strong> 用这些权重对编码器的隐藏状态进行加权求和，得到一个动态的上下文向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">c_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。<br>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">c_i = \sum_{j=1}^{n} \alpha_{ij} h_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2401em;vertical-align:-0.4358em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></li>
<li><strong>解码：</strong> 解码器结合 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">c_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和前一个预测词 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">y_{i-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span> 来生成当前词 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</li>
</ol>
</li>
</ul>
<p>通过注意力机制，解码器可以“看到”源句子中的所有信息，并根据需要关注不同的部分，从而大大提升了翻译质量，尤其是对长句子的翻译效果。</p>
<h4 id="Transformer-架构：NMT-的里程碑">Transformer 架构：NMT 的里程碑</h4>
<p>2017年，Google Brain 团队在论文《Attention Is All You Need》中提出了 <strong>Transformer</strong> 架构。它彻底放弃了 RNN 和 CNN 结构，完全基于注意力机制，实现了模型的并行化训练，并成为当前 NMT 乃至整个 NLP 领域的事实标准。</p>
<p><strong>Transformer 的创新点：</strong></p>
<ol>
<li><strong>完全并行化：</strong> 抛弃了 RNN 的顺序计算特性，所有时间步的计算可以并行进行，极大地提高了训练效率。</li>
<li><strong>远距离依赖：</strong> 每一层都能够直接计算输入序列中任意两个位置之间的关联，有效解决了长距离依赖问题。</li>
<li><strong>自注意力机制 (Self-Attention)：</strong> 不仅用于连接编码器和解码器，还用于处理输入序列自身内部的依赖关系。</li>
</ol>
<h4 id="编码器-解码器结构详解">编码器-解码器结构详解</h4>
<p>Transformer 依然遵循编码器-解码器结构，但两者的内部构造都由多个相同的层堆叠而成。</p>
<ul>
<li>
<p><strong>编码器 (Encoder)：</strong><br>
由 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 个相同的编码器层堆叠而成。每个编码器层包含两个子层：</p>
<ol>
<li><strong>多头自注意力层 (Multi-Head Self-Attention Layer)：</strong> 允许模型在对序列进行编码时，同时关注序列内不同位置的不同方面。</li>
<li><strong>前馈网络 (Feed-Forward Network)：</strong> 对注意力层的输出进行非线性变换。<br>
每个子层之后都跟着一个<strong>残差连接 (Residual Connection)</strong> 和<strong>层归一化 (Layer Normalization)</strong>。</li>
</ol>
</li>
<li>
<p><strong>解码器 (Decoder)：</strong><br>
由 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 个相同的解码器层堆叠而成。每个解码器层包含三个子层：</p>
<ol>
<li><strong>带掩码的多头自注意力层 (Masked Multi-Head Self-Attention Layer)：</strong> 与编码器类似，但为了防止解码器在生成当前词时“偷看”未来的词，需要对未来的位置进行掩码（Masking）。</li>
<li><strong>多头注意力层 (Multi-Head Attention Layer)：</strong> 也称为编码器-解码器注意力，它使得解码器能够关注编码器的输出。这里的 Query 来自解码器，而 Key 和 Value 来自编码器。</li>
<li><strong>前馈网络 (Feed-Forward Network)：</strong> 与编码器中的前馈网络类似。<br>
同样，每个子层之后也跟着残差连接和层归一化。</li>
</ol>
</li>
</ul>
<p><strong>Transformer 的输入：</strong><br>
原始的输入词向量会先通过<strong>词嵌入层 (Word Embedding)</strong> 转换为高维向量。由于 Transformer 没有 RNN 那样的序列顺序概念，还需要加入<strong>位置编码 (Positional Encoding)</strong> 来提供词的位置信息。</p>
<h4 id="位置编码-Positional-Encoding">位置编码 (Positional Encoding)</h4>
<p>Transformer 不像 RNN 那样按顺序处理输入，它同时处理所有词。为了让模型知道每个词在序列中的位置，以及词之间的相对位置，需要引入位置编码。<br>
位置编码与词嵌入向量相加，作为编码器和解码器输入的初始表示。<br>
原始 Transformer 论文中使用了正弦和余弦函数来生成位置编码：<br>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">/</mi><msup><mn>10000</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0385em;vertical-align:-0.3552em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mord">/1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><br>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">/</mi><msup><mn>10000</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0385em;vertical-align:-0.3552em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mord">/1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><br>
其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">pos</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span> 是词在序列中的位置，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 是维度，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是词嵌入的维度。这种编码方式使得模型能够学习到相对位置信息。</p>
<h4 id="多头自注意力-Multi-Head-Self-Attention">多头自注意力 (Multi-Head Self-Attention)</h4>
<p>注意力机制的核心是计算 Query (Q)、Key (K) 和 Value (V) 之间的关系。<br>
对于自注意力，Q、K、V 都来自同一个输入序列。<br>
<strong>缩放点积注意力 (Scaled Dot-Product Attention)：</strong></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4483em;vertical-align:-0.93em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em;"><span style="top:-2.2528em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span> 是查询矩阵，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 是键矩阵，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span> 是值矩阵，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是 Key 向量的维度，用于缩放，防止内积过大导致 softmax 梯度过小。</p>
<p><strong>多头注意力 (Multi-Head Attention)：</strong><br>
多头注意力将 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Q, K, V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span> 线性投影到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span></span></span></span> 个不同的子空间，分别计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span></span></span></span> 次独立的注意力，然后将它们的输出拼接起来，再进行一次线性投影。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>MultiHead</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>Concat</mtext><mo stretchy="false">(</mo><msub><mtext>head</mtext><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mtext>head</mtext><mi>h</mi></msub><mo stretchy="false">)</mo><msup><mi>W</mi><mi>O</mi></msup></mrow><annotation encoding="application/x-tex">\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">MultiHead</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Concat</span></span><span class="mopen">(</span><span class="mord"><span class="mord text"><span class="mord">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord text"><span class="mord">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">O</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mtext>where head</mtext><mi>i</mi></msub><mo>=</mo><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup><mo separator="true">,</mo><mi>K</mi><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup><mo separator="true">,</mo><mi>V</mi><msubsup><mi>W</mi><mi>i</mi><mi>V</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{where head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord text"><span class="mord">where head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2361em;vertical-align:-0.2769em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9592em;"><span style="top:-2.4231em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup><mo separator="true">,</mo><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup><mo separator="true">,</mo><msubsup><mi>W</mi><mi>i</mi><mi>V</mi></msubsup></mrow><annotation encoding="application/x-tex">W_i^Q, W_i^K, W_i^V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2361em;vertical-align:-0.2769em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9592em;"><span style="top:-2.4231em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4413em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4413em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span> 是投影矩阵，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mi>O</mi></msup></mrow><annotation encoding="application/x-tex">W^O</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">O</span></span></span></span></span></span></span></span></span></span></span> 是最终的输出投影矩阵。<br>
多头注意力允许模型在不同的“注意力头”中学习到不同的关注模式，从而捕获更丰富的语义和句法信息。</p>
<h4 id="前馈网络-Feed-Forward-Network">前馈网络 (Feed-Forward Network)</h4>
<p>每个自注意力子层之后都跟着一个简单的位置共享的前馈网络 (Position-wise Feed-Forward Network)，它独立地作用于序列中的每一个位置。<br>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>F</mi><mi>N</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo stretchy="false">)</mo><msub><mi>W</mi><mn>2</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">FFN</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><br>
这是一个两层的前馈网络，中间通常使用 ReLU 激活函数。</p>
<h4 id="残差连接与层归一化-Residual-Connection-Layer-Normalization">残差连接与层归一化 (Residual Connection &amp; Layer Normalization)</h4>
<ul>
<li><strong>残差连接 (Residual Connection)：</strong> 每个子层都被一个残差连接包裹，这意味着子层的输入会直接加到子层的输出上。<br>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>SublayerOutput</mtext><mo>=</mo><mtext>Sublayer</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">\text{SublayerOutput} = \text{Sublayer}(x) + x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">SublayerOutput</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Sublayer</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span><br>
这有助于解决深层网络的梯度消失问题，使得模型能够训练得更深。</li>
<li><strong>层归一化 (Layer Normalization)：</strong> 在每个子层输出并进行残差连接之后，会进行层归一化。它对每个样本的所有特征进行归一化，使得网络训练更加稳定。<br>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>LayerNorm</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>γ</mi><mo>⊙</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><msqrt><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\text{LayerNorm}(x) = \gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">LayerNorm</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.3924em;vertical-align:-0.538em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8544em;"><span style="top:-2.5445em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9221em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">ϵ</span></span></span><span style="top:-2.8821em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1179em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">μ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span><br>
其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">μ</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> 是层内均值和方差，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span> 是可学习的缩放和偏移参数。</li>
</ul>
<p><strong>Transformer 结构代码概念示意：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>) <span class="comment"># (max_len, 1, d_model) -&gt; (max_len, d_model) -&gt; (1, max_len, d_model) for batching</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x: (seq_len, batch_size, d_model)</span></span><br><span class="line">        <span class="comment"># pe: (max_len, 1, d_model)</span></span><br><span class="line">        x = x + <span class="variable language_">self</span>.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads</span><br><span class="line">        <span class="variable language_">self</span>.head_dim = d_model // num_heads</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.wq = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.wk = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.wv = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.fc_out = nn.Linear(d_model, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span><br><span class="line">        batch_size = q.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        Q = <span class="variable language_">self</span>.wq(q).view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        K = <span class="variable language_">self</span>.wk(k).view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        V = <span class="variable language_">self</span>.wv(v).view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Scaled Dot-Product Attention</span></span><br><span class="line">        energy = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(<span class="variable language_">self</span>.head_dim)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            energy = energy.masked_fill(mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&quot;-1e20&quot;</span>)) <span class="comment"># Apply mask</span></span><br><span class="line"></span><br><span class="line">        attention = torch.softmax(energy, dim=-<span class="number">1</span>)</span><br><span class="line">        x = torch.matmul(attention, V)</span><br><span class="line"></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.d_model)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc_out(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads, ff_dim, dropout_rate</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.self_attn = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        <span class="variable language_">self</span>.norm1 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout1 = nn.Dropout(dropout_rate)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = nn.Sequential(</span><br><span class="line">            nn.Linear(d_model, ff_dim),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(ff_dim, d_model)</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout2 = nn.Dropout(dropout_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        attn_output = <span class="variable language_">self</span>.self_attn(x, x, x, mask)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x + <span class="variable language_">self</span>.dropout1(attn_output)) <span class="comment"># Residual + LayerNorm</span></span><br><span class="line"></span><br><span class="line">        ff_output = <span class="variable language_">self</span>.feed_forward(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm2(x + <span class="variable language_">self</span>.dropout2(ff_output)) <span class="comment"># Residual + LayerNorm</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实际的 Transformer 编码器和解码器会有更复杂的堆叠和初始化，这里只展示核心组件概念。</span></span><br></pre></td></tr></table></figure>
<p>通过这些精心设计的组件，Transformer 实现了对序列数据的强大建模能力，使得机器翻译的质量和效率都达到了前所未有的水平。</p>
<h3 id="第四部分：NMT-的关键技术与挑战">第四部分：NMT 的关键技术与挑战</h3>
<p>尽管 NMT 取得了巨大成功，但它并非完美无缺。在实际应用中，仍面临诸多挑战，并且研究人员正不断探索新的技术来克服这些困难。</p>
<h4 id="数据需求与预训练模型">数据需求与预训练模型</h4>
<p>NMT 模型，尤其是大型 Transformer 模型，是数据饥渴型模型。它们需要<strong>海量的平行语料 (Parallel Corpora)</strong> 来学习两种语言之间的映射关系。获取高质量、大规模的平行语料成本高昂且耗时，尤其对于低资源语言（数据量小的语言），这是一个巨大障碍。</p>
<p><strong>解决方案：预训练语言模型 (Pre-trained Language Models)</strong><br>
近年来，预训练语言模型 (PLMs) 的兴起极大地改变了 NLP 领域，也对 NMT 产生了深远影响。</p>
<ul>
<li><strong>核心思想：</strong> 在海量<strong>单语语料 (Monolingual Corpora)</strong> 上进行无监督预训练，学习通用的语言表示和语言知识，然后通过<strong>微调 (Fine-tuning)</strong> 将这些知识迁移到特定任务（如机器翻译）上。</li>
<li><strong>代表模型：</strong>
<ul>
<li><strong>BERT (Bidirectional Encoder Representations from Transformers)：</strong> 双向编码器，通过掩码语言模型 (Masked Language Model) 和下一句预测 (Next Sentence Prediction) 任务学习。</li>
<li><strong>GPT 系列 (Generative Pre-trained Transformer)：</strong> 基于 Transformer 解码器，擅长文本生成，通过预测下一个词来预训练。</li>
<li><strong>T5 (Text-to-Text Transfer Transformer)：</strong> 将所有 NLP 任务统一建模为“文本到文本”的形式。</li>
<li><strong>BART, XLM-R 等：</strong> 针对翻译任务进行了多语言预训练，或采用编码器-解码器预训练。</li>
</ul>
</li>
</ul>
<p>这些模型通过在预训练阶段捕捉了丰富的语法、语义信息，使得 NMT 模型在有限的平行语料下也能达到更好的性能，尤其对于低资源语言对的翻译效果提升显著。它们通常作为 NMT 模型的编码器或初始化权重。</p>
<h4 id="评估指标">评估指标</h4>
<p>如何客观地衡量机器翻译的质量是一个复杂的问题。目前主要有两种评估方法：</p>
<ol>
<li><strong>自动评估指标：</strong> 通过算法计算机器译文与参考译文之间的相似度。
<ul>
<li><strong>BLEU (Bilingual Evaluation Understudy)：</strong> 最广泛使用的指标。它计算机器译文与一个或多个参考译文之间 N-gram (通常是1-gram到4-gram) 的重叠程度。<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>BLEU</mtext><mo>=</mo><mi>B</mi><mi>P</mi><mo>⋅</mo><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>w</mi><mi>n</mi></msub><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi>n</mi></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{BLEU} = BP \cdot \exp \left( \sum_{n=1}^N w_n \log p_n \right) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">BLEU</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">BP</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:3.0954em;vertical-align:-1.2671em;"></span><span class="mop">exp</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">(</span></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8829em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">)</span></span></span></span></span></span></span></p>
其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>P</mi></mrow><annotation encoding="application/x-tex">BP</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">BP</span></span></span></span> 是简短惩罚因子，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">p_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是 N-gram 精度，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">w_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是权重。
<ul>
<li><strong>优点：</strong> 快速、廉价、可重复。</li>
<li><strong>缺点：</strong> 无法完全捕捉语义等价性，对同义词或不同但正确的表达不敏感，无法直接衡量流畅度，与人类判断相关性并非100%。</li>
</ul>
</li>
<li><strong>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)：</strong> 主要用于文本摘要和评估生成性任务，侧重召回率。</li>
<li><strong>METEOR (Metric for Evaluation of Translation with Explicit Ordering)：</strong> 考虑了词干、同义词和短语匹配，在某些方面比 BLEU 更优。</li>
</ul>
</li>
<li><strong>人工评估：</strong> 由人类译员或评审员对机器译文进行评分。
<ul>
<li><strong>流利度 (Fluency)：</strong> 译文是否语法正确、拼写无误、自然流畅。</li>
<li><strong>忠实度/充分性 (Adequacy)：</strong> 译文是否准确传达了源文本的所有信息。</li>
<li><strong>错误类型分析：</strong> 细致分类错误，如词汇错误、语法错误、语义错误等。</li>
<li><strong>优点：</strong> 最准确、最可靠的评估方法。</li>
<li><strong>缺点：</strong> 成本高、耗时、主观性强，难以大规模应用。</li>
</ul>
</li>
</ol>
<h4 id="低资源语言问题-Low-Resource-Languages">低资源语言问题 (Low-Resource Languages)</h4>
<p>对于拥有丰富数字资源的语言（如英语、中文），NMT 表现优异。但对于那些缺乏大规模平行语料的“低资源语言”（全球约有7000种语言，其中绝大多数是低资源语言），NMT 的性能会显著下降。</p>
<p><strong>应对策略：</strong></p>
<ul>
<li><strong>多语言 NMT (Multilingual NMT)：</strong> 训练一个模型可以同时翻译多种语言对，通过共享参数和跨语言知识迁移来帮助低资源语言。例如，M2M-100。</li>
<li><strong>零样本翻译 (Zero-shot Translation)：</strong> 在没有直接平行语料的语言对之间进行翻译（例如，训练了英-法和英-德，通过英语作为枢纽实现法-德翻译）。</li>
<li><strong>数据增强 (Data Augmentation)：</strong>
<ul>
<li><strong>回译 (Back-translation)：</strong> 利用单语数据，先用一个目标到源的模型将目标语言文本回译成源语言，从而生成伪平行语料。</li>
<li><strong>合成数据：</strong> 利用语言学规则或预训练模型生成合成的翻译对。</li>
</ul>
</li>
<li><strong>迁移学习 (Transfer Learning) 和预训练：</strong> 利用在大规模单语语料上预训练的通用语言表示。</li>
</ul>
<h4 id="领域适应与个性化翻译">领域适应与个性化翻译</h4>
<p>通用的 NMT 模型在特定领域（如医疗、法律、科技）的翻译质量可能不尽如人意，因为这些领域有大量专业术语和特有的表达方式。</p>
<ul>
<li>
<p><strong>领域适应 (Domain Adaptation)：</strong></p>
<ul>
<li><strong>微调 (Fine-tuning)：</strong> 在通用模型的基础上，用少量领域内平行语料进行进一步训练。</li>
<li><strong>混合专家模型 (Mixture of Experts)：</strong> 结合多个领域专家模型。</li>
<li><strong>领域对抗训练：</strong> 学习领域不变的特征。</li>
</ul>
</li>
<li>
<p><strong>个性化翻译：</strong> 考虑到用户特定的语言习惯、词汇偏好等，提供更符合个人风格的翻译。这需要更精细的用户画像和更灵活的模型。</p>
</li>
</ul>
<h4 id="模型可解释性与鲁棒性">模型可解释性与鲁棒性</h4>
<p>深度学习模型通常被视为“黑箱”，难以理解其内部决策过程。对于机器翻译，理解模型为什么会出错、如何出错，对于改进模型至关重要。</p>
<ul>
<li><strong>可解释性：</strong>
<ul>
<li><strong>注意力可视化：</strong> 观察注意力权重分布，可以粗略地看出模型在翻译某个词时“关注”了源句子的哪些部分。</li>
<li><strong>探针 (Probing)：</strong> 训练一个简单的分类器来预测模型中间表示中编码的语言学特征。</li>
</ul>
</li>
<li><strong>鲁棒性：</strong> 模型对输入中的噪声、拼写错误、语法不规范等情况的抵抗能力。NMT 模型在这方面仍然有提升空间。</li>
</ul>
<h4 id="多模态翻译的未来">多模态翻译的未来</h4>
<p>未来的翻译不仅仅局限于文本。语音翻译、图像中的文本翻译、视频实时翻译等，都涉及到多模态信息的处理。</p>
<ul>
<li><strong>语音到语音翻译 (Speech-to-Speech Translation)：</strong> 直接将一种语言的语音输入转换为另一种语言的语音输出，中间可能不生成文本。</li>
<li><strong>图像到文本翻译 (Image-to-Text Translation)：</strong> 识别图像中的文字并进行翻译（如街头标牌、菜单等）。</li>
<li><strong>视频翻译：</strong> 结合语音识别、目标检测、OCR 和机器翻译，实现对视频内容的实时翻译和字幕生成。</li>
</ul>
<p>这将需要整合计算机视觉、语音识别和自然语言处理的最新技术，构建更强大的多模态 AI 模型。</p>
<h3 id="第五部分：NLP-与-MT-的未来展望">第五部分：NLP 与 MT 的未来展望</h3>
<p>我们已经见证了 NLP 和机器翻译的巨大飞跃，但探索的脚步从未停止。未来的发展将更加令人兴奋，同时也伴随着新的挑战和伦理考量。</p>
<h4 id="大语言模型-LLMs-对-NMT-的影响">大语言模型 (LLMs) 对 NMT 的影响</h4>
<p>近年来，以 GPT-3、GPT-4、Llama、PaLM2 等为代表的<strong>大语言模型 (LLMs)</strong> 展现出了惊人的文本生成和理解能力，它们正在深刻改变 NLP 乃至 AI 的面貌。</p>
<ul>
<li><strong>多任务能力：</strong> LLMs 在预训练阶段学习了海量文本，掌握了丰富的语言知识和模式，具备了强大的通用能力，包括翻译。它们可以在不经过特定微调的情况下，直接通过<strong>指令 (Prompting)</strong> 或<strong>语境学习 (In-context Learning)</strong> 来完成翻译任务，展现出惊人的零样本或少样本翻译能力。</li>
<li><strong>高质量生成：</strong> LLMs 生成的文本更具逻辑性、连贯性和流畅性，能够更好地处理复杂的语义和语境。</li>
<li><strong>翻译即指令：</strong> 翻译不再是独立模型，而是 LLM 的一种能力。例如，你可以简单地向 LLM 发送指令：“将以下英文翻译成中文：‘The quick brown fox jumps over the lazy dog.’”，它就能给出高质量的译文。</li>
<li><strong>未来展望：</strong> LLMs 可能会成为未来翻译系统的核心，通过更智能的上下文理解、领域知识整合和个性化能力，提供更准确、更符合用户需求的翻译服务。</li>
</ul>
<p>然而，LLMs 也带来了新的挑战：</p>
<ul>
<li><strong>计算成本：</strong> 训练和运行 LLMs 需要巨大的计算资源。</li>
<li><strong>幻觉 (Hallucinations)：</strong> LLMs 有时会生成看似合理但实际上是错误或捏造的信息。</li>
<li><strong>偏见：</strong> 继承了训练数据中的偏见，可能导致翻译不公或歧视。</li>
<li><strong>可控性：</strong> 难以完全控制 LLM 的输出，可能导致不准确或不恰当的翻译。</li>
</ul>
<h4 id="通用人工智能-AGI-的愿景">通用人工智能 (AGI) 的愿景</h4>
<p>NLP 和机器翻译的发展，是实现<strong>通用人工智能 (Artificial General Intelligence, AGI)</strong> 的重要里程碑。一个真正能够理解人类语言、进行复杂推理和解决各种问题的 AI，必然需要在语言理解和生成方面达到人类水平。语言是人类思维的载体，能够掌握语言，意味着 AI 离理解世界又近了一步。</p>
<p>未来的 NMT 不仅仅是“翻译”，它可能是：</p>
<ul>
<li><strong>跨文化交流助手：</strong> 不仅翻译文字，还能解释文化背景、习语和幽默。</li>
<li><strong>知识发现引擎：</strong> 从多语言文本中提取、整合知识，打破语言壁垒获取全球信息。</li>
<li><strong>多模态融合智能体：</strong> 结合视觉、听觉，实现对真实世界复杂情境的全面理解和翻译。</li>
</ul>
<h4 id="伦理、偏见与负责任的AI">伦理、偏见与负责任的AI</h4>
<p>随着 AI 技术在社会中的普及，其伦理问题也日益凸显。机器翻译作为信息传播的重要工具，其潜在的偏见和滥用风险不容忽视。</p>
<ul>
<li><strong>偏见 (Bias)：</strong> 训练数据中可能包含性别偏见、种族偏见、刻板印象等，这些偏见会被模型学习并反映到翻译结果中。例如，将“医生”翻译成“他”，将“护士”翻译成“她”，或者在敏感话题上给出带有歧视性的译文。</li>
<li><strong>隐私 (Privacy)：</strong> 用户的敏感信息可能会通过翻译系统泄露。</li>
<li><strong>信息控制与滥用：</strong> 机器翻译可能被用于传播虚假信息、审查内容或进行网络攻击。</li>
<li><strong>问责制 (Accountability)：</strong> 当翻译错误导致严重后果时，谁应该为此负责？</li>
</ul>
<p>负责任的 AI (Responsible AI) 理念强调在开发和部署 AI 系统时，应考虑到公平性、透明度、隐私保护、安全性和可解释性。对于 NMT 而言，这意味着需要：</p>
<ul>
<li><strong>偏见检测与缓解：</strong> 开发技术来识别和减少翻译中的偏见。</li>
<li><strong>可解释性提升：</strong> 努力打开“黑箱”，让用户和开发者理解模型的决策过程。</li>
<li><strong>数据伦理：</strong> 确保训练数据的合法性、多样性和无偏性。</li>
<li><strong>用户控制：</strong> 允许用户自定义翻译风格、术语，并提供错误反馈机制。</li>
<li><strong>法律法规制定：</strong> 推动相关政策和法规的完善，规范 AI 翻译的使用。</li>
</ul>
<h3 id="结论">结论</h3>
<p>自然语言处理和机器翻译的演进之路，是一部从朴素规则到统计学习，再到深度神经网络，直至如今大语言模型统治的精彩篇章。我们从逐词对照的生硬翻译，发展到能捕捉语境、理解情感、甚至进行风格转换的智能翻译。Transformer 架构和注意力机制的引入，解决了长期困扰序列建模的并行化和长距离依赖问题，而大规模预训练语言模型则将 NMT 的能力推向了前所未有的高度。</p>
<p>然而，这并非终点。低资源语言的挑战、领域适应的需求、模型可解释性的缺失以及潜在的伦理偏见，都提醒着我们，前方的道路依然充满挑战。未来的机器翻译将不仅仅是文字的转换，更是跨文化理解的桥梁，是信息自由流动的催化剂，也是通向通用人工智能的必经之路。</p>
<p>作为技术爱好者，我们很幸运能生活在这个激动人心的时代。无论是深入算法原理，还是探索前沿应用，NLP 与机器翻译都为我们提供了无限的可能。让我们一同期待，并为构建一个真正无语言障碍的智能世界贡献自己的力量！</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://github.com/qmwneb946">qmwneb946</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://qmwneb946.dpdns.org/2025/07/18/2025-07-19-020856/">https://qmwneb946.dpdns.org/2025/07/18/2025-07-19-020856/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://opensource.org/licenses/MIT" target="_blank">MIT License</a> 许可协议。转载请注明来源 <a href="https://qmwneb946.dpdns.org" target="_blank">qmwneb946 的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/%E6%95%B0%E5%AD%A6/">数学</a><a class="post-meta__tags" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%8E%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/">自然语言处理与机器翻译</a></div><div class="post-share"><div class="social-share" data-image="/img/icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/07/18/2025-07-19-020946/" title="5G技术与万物互联的未来：构建智能世界的基石"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">5G技术与万物互联的未来：构建智能世界的基石</div></div><div class="info-2"><div class="info-item-1">作者：qmwneb946  引言：从连接人到连接万物 在人类通信史上，每一代移动通信技术的革新都深刻地改变了我们的生活。从1G的模拟语音，到2G的数字短信，再到3G的移动互联网初现，以及4G时代的高速移动宽带和智能手机的普及，我们见证了信息传输速度的飞跃和连接能力的指数级增长。然而，如果说前几代技术主要是为了“连接人”，那么第五代移动通信技术——5G，则肩负着“连接万物”的宏大使命，它不仅仅是网络速度的简单提升，更是一场旨在构建万物互联（Internet of Everything, IoE）智能未来的深刻技术革命。 万物互联，顾名思义，是超越传统物联网（IoT）范畴的概念，它不仅连接设备，更将人、数据和流程整合在一起，形成一个无缝、智能、高效的数字生态系统。要实现这一愿景，我们需要一个具备超高带宽、超低时延、超大连接能力以及极致可靠性的通信基础设施。5G正是为满足这些苛刻要求而生，它不仅仅是无线电技术的演进，更是网络架构、软件定义、边缘计算等一系列前沿技术融合的结晶。 本文将深入探讨5G的核心技术原理，解析其如何赋能万物互联的各个应用场景，并展望未来可能面临的挑战与无限机遇。我...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-19-020747/" title="揭秘计算机视觉的“火眼金睛”：目标检测技术深度剖析"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">揭秘计算机视觉的“火眼金睛”：目标检测技术深度剖析</div></div><div class="info-2"><div class="info-item-1">各位技术爱好者、探索者们，大家好！我是 qmwneb946，你们的老朋友。 在计算机视觉的浩瀚星空中，有一颗璀璨的明星，它赋予机器一双“火眼金睛”，能够像人类一样，在复杂的图像和视频中准确识别出各种物体的位置和类别。这项技术，就是我们今天将要深度剖析的主题——目标检测（Object Detection）。 从自动驾驶汽车识别行人与车辆，到安防监控系统追踪可疑人员，再到医疗影像分析中的病灶识别，目标检测的身影无处不在，深刻地改变着我们的生活和工作。但它并非一蹴而就，而是历经数十载的迭代与创新，才发展到如今的强大面貌。 今天，我将带领大家踏上一段激动人心的旅程，从历史的起点出发，逐步深入到现代目标检测的核心技术，探寻其背后的数学原理、工程智慧以及未来的发展趋势。准备好了吗？让我们一起揭开这层神秘的面纱！ 一、目标检测：机器视觉的“感知”基石 什么是目标检测？ 目标检测，顾名思义，就是让计算机在图像或视频帧中，准确地识别出特定类别的物体，并同时框选出它们在图像中的精确位置（即边界框，Bounding Box）。它不仅仅是识别“这张图里有只猫”，更是要识别出“这只猫在这里（x1,y1,x...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/07/18/2025-07-18-082519/" title="增强现实与工业维修：一场效率革命"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">增强现实与工业维修：一场效率革命</div></div><div class="info-2"><div class="info-item-1">增强现实 (AR) 技术正以前所未有的速度改变着我们的生活，而其在工业维修领域的应用更是展现出了巨大的潜力。不再局限于科幻电影中的场景，AR 如今已成为提升维修效率、降低维护成本、提高安全性的强大工具。本文将深入探讨 AR 如何与工业维修相结合，并分析其背后的技术和未来发展趋势。 引言：传统工业维修的挑战 传统的工业维修往往面临着诸多挑战：  信息获取困难: 维修人员需要查阅大量的纸质文档、图纸和视频，耗时费力，容易出错。 培训成本高昂:  熟练技工的培养需要漫长的学习过程和大量的实践经验，成本高昂。 安全风险较高:  一些复杂的设备维修存在高风险，例如高压电、高温部件等，容易发生意外事故。 维修效率低下:  由于缺乏实时信息和有效的指导，维修时间往往较长，导致生产停机时间增加，损失巨大。  AR 如何改变工业维修的游戏规则 AR 技术通过将数字信息叠加到现实世界中，为工业维修提供了全新的解决方案： 远程专家指导 通过 AR 眼镜或平板电脑，现场维修人员可以与远程专家实时互动。专家可以通过 AR 系统看到现场设备的实时图像，并利用虚拟标注、3D 模型等工具进行远程指导，大大缩短了...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082652/" title="纳米材料在靶向药物中的革命性应用"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">纳米材料在靶向药物中的革命性应用</div></div><div class="info-2"><div class="info-item-1">近年来，癌症等重大疾病的治疗面临着巨大的挑战，传统的化疗药物往往毒性大、副作用强，难以实现精准治疗。而纳米技术的兴起为解决这一难题提供了新的思路，特别是纳米材料在靶向药物递送系统中的应用，正引发一场医学革命。本文将深入探讨纳米材料如何提升靶向药物的疗效，降低其毒副作用。 纳米材料的特性及其在药物递送中的优势 纳米材料，是指至少在一个维度上尺寸小于100纳米的材料。这种极小的尺寸赋予了它们许多独特的物理和化学性质，使其在药物递送领域具有显著优势： 增强的药物溶解度和稳定性 许多药物具有较低的溶解度，限制了其在体内的吸收和生物利用度。纳米载体，例如脂质体、聚合物纳米颗粒和无机纳米颗粒（如金纳米颗粒、氧化铁纳米颗粒），可以显著提高药物的溶解度和稳定性，延长其在体内的循环时间。例如，将抗癌药物负载在聚合物纳米颗粒中，可以保护药物免受降解，并提高其在肿瘤组织中的积累。 靶向药物递送 纳米材料可以通过表面修饰，例如结合特异性配体（如抗体、肽或小分子），实现对特定细胞或组织的靶向递送。这种靶向递送可以最大限度地减少药物对健康组织的毒性，并提高药物在靶标部位的浓度，从而增强治疗效果。例如，修饰有...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082925/" title="生物化学中的蛋白质折叠问题：一个复杂而迷人的计算挑战"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">生物化学中的蛋白质折叠问题：一个复杂而迷人的计算挑战</div></div><div class="info-2"><div class="info-item-1">生命，这奇妙的现象，其本质很大程度上取决于蛋白质的精确三维结构。蛋白质是由氨基酸链组成的长链分子，但仅仅是氨基酸序列并不能完全决定其功能。蛋白质必须折叠成特定的三维结构（构象），才能发挥其生物学功能，例如催化酶促反应、运输分子或构建细胞结构。  而这个折叠过程，就是著名的“蛋白质折叠问题”。 蛋白质折叠：从线性序列到三维结构 蛋白质的氨基酸序列由基因编码决定，这是一个线性的一维结构。然而，这些氨基酸链并非随机地盘踞在一起，而是会遵循特定的物理和化学原理，自发地折叠成独特的、功能性的三维结构。这个折叠过程涉及到多种相互作用，包括： 疏水相互作用 蛋白质内部的疏水氨基酸残基倾向于聚集在一起，远离水性环境，形成蛋白质的核心区域。而亲水性氨基酸残基则倾向于暴露在蛋白质的表面，与水分子相互作用。 静电相互作用 带电荷的氨基酸残基之间会发生静电吸引或排斥作用，影响蛋白质的折叠。 氢键 氢键在维持蛋白质二级结构（例如α螺旋和β折叠）中起着关键作用。 二硫键 某些氨基酸残基（例如半胱氨酸）之间可以形成二硫键，进一步稳定蛋白质的三维结构。 这些相互作用共同决定了蛋白质的最终构象，这是一个极其复杂的...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-092536/" title="CRISPR基因编辑：技术的奇迹与伦理的挑战"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">CRISPR基因编辑：技术的奇迹与伦理的挑战</div></div><div class="info-2"><div class="info-item-1">大家好！我是你们的技术和数学博主，今天我们要深入探讨一个既令人兴奋又充满争议的话题：CRISPR-Cas9基因编辑技术及其伦理挑战。CRISPR技术以其精准性和效率，为治疗遗传疾病、改良作物等领域带来了革命性的变革，但与此同时，它也引发了诸多伦理难题，需要我们认真思考和谨慎应对。 CRISPR技术：一把双刃剑 CRISPR-Cas9系统，简单来说，就是一种可以精确地“剪切和粘贴”DNA的工具。它源自细菌的天然防御机制，利用向导RNA（gRNA）引导Cas9酶到基因组中的特定位置，从而进行基因的敲除、插入或替换。其操作简便、成本低廉、效率高，使其成为基因编辑领域的“明星”技术。 CRISPR的工作原理 CRISPR系统的工作机制可以概括为以下几个步骤：  设计gRNA:  根据目标基因序列设计相应的gRNA，使其能够特异性地结合目标DNA序列。 Cas9酶的结合: gRNA引导Cas9酶到目标DNA序列。 DNA双链断裂: Cas9酶在目标位点切割DNA双链，形成双链断裂（DSB）。 DNA修复: 细胞利用非同源末端连接（NHEJ）或同源定向修复（HDR）机制修复DSB。NHEJ修...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-094115/" title="免疫学与癌症免疫疗法：一场人体内部的战争与和平"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">免疫学与癌症免疫疗法：一场人体内部的战争与和平</div></div><div class="info-2"><div class="info-item-1">免疫系统，人体精妙的防御机制，日夜不停地抵御着病毒、细菌和其他有害物质的入侵。然而，当这套系统出现故障，对自身细胞发起攻击，或者无法有效清除癌细胞时，疾病便会发生，其中最可怕的莫过于癌症。近年来，癌症免疫疗法异军突起，为癌症治疗带来了新的希望，让我们深入探索这场人体内部的战争与和平。 免疫系统：人体精妙的防御网络 我们的免疫系统由先天免疫和适应性免疫两大支柱组成。 先天免疫：第一道防线 先天免疫是人体抵御病原体的第一道防线，它包含物理屏障（例如皮肤和黏膜）、化学屏障（例如胃酸和酶）以及细胞介导的免疫反应，例如巨噬细胞和自然杀伤细胞（NK细胞）的吞噬和杀伤作用。这些细胞能够识别并清除被感染的细胞或癌细胞，但其特异性较低。 适应性免疫：精准打击 适应性免疫系统则更为精细，它具有特异性和记忆性。T细胞和B细胞是适应性免疫的主角。T细胞负责细胞介导的免疫，其中细胞毒性T细胞（CTL）能够特异性识别并杀死靶细胞，例如被病毒感染的细胞或癌细胞。B细胞则负责体液免疫，产生抗体，中和病原体或标记癌细胞以便清除。  抗原呈递细胞（APC），例如树突状细胞，在将抗原信息呈递给T细胞，启动适应性免疫反...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-094141/" title="生态学中的生物多样性保护：一个复杂系统工程的视角"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">生态学中的生物多样性保护：一个复杂系统工程的视角</div></div><div class="info-2"><div class="info-item-1">大家好！今天我们要深入探讨一个既充满挑战又至关重要的话题：生态学中的生物多样性保护。  这不仅是环境保护的基石，也与我们人类的福祉息息相关。对技术爱好者来说，这更像是一个巨大的、复杂的系统工程，充满了需要解决的优化问题和值得探索的算法。 生物多样性的价值：超越简单的物种数量 我们通常将生物多样性理解为物种数量的多样性。但实际上，它是一个多层次的概念，包括：  遗传多样性 (Genetic Diversity):  同一物种内基因组的差异性，这决定了物种的适应性和进化潜力。  想象一下，一个抗旱基因的缺失可能导致整个小麦品种在干旱年份面临灭绝的风险。 物种多样性 (Species Diversity):  不同物种的数量及其相对丰度。 这通常用Shannon多样性指数 (H=−∑i=1Spilog⁡2piH = -\sum_{i=1}^{S} p_i \log_2 p_iH=−∑i=1S​pi​log2​pi​) 来衡量，其中 pip_ipi​ 是第 iii 个物种的比例，SSS 是物种总数。  更高的Shannon指数表示更高的物种多样性。 生态系统多样性 (Ecosystem ...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qmwneb946</div><div class="author-info-description">一个专注于技术分享的个人博客，涵盖编程、算法、系统设计等内容</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">378</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">382</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qmwneb946"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qmwneb946" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:qmwneb946@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">代码与远方，技术与生活交织的篇章。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89%E5%9F%BA%E7%A1%80%EF%BC%9A%E6%9C%BA%E5%99%A8%E7%90%86%E8%A7%A3%E4%BA%BA%E7%B1%BB%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3"><span class="toc-number">1.</span> <span class="toc-text">第一部分：自然语言处理（NLP）基础：机器理解人类语言的基石</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%9F"><span class="toc-number">1.1.</span> <span class="toc-text">什么是自然语言处理？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#NLP-%E7%9A%84%E6%A0%B8%E5%BF%83%E6%8C%91%E6%88%98"><span class="toc-number">1.2.</span> <span class="toc-text">NLP 的核心挑战</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#NLP-%E7%9A%84%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%EF%BC%9A%E8%A7%84%E5%88%99%E4%B8%8E%E7%BB%9F%E8%AE%A1"><span class="toc-number">1.3.</span> <span class="toc-text">NLP 的传统方法：规则与统计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#NLP-%E7%9A%84%E7%8E%B0%E4%BB%A3%E6%96%B9%E6%B3%95%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B4%9B%E8%B5%B7"><span class="toc-number">1.4.</span> <span class="toc-text">NLP 的现代方法：深度学习的崛起</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%9A%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%EF%BC%88MT%EF%BC%89%E7%AE%80%E5%8F%B2%E4%B8%8E%E6%BC%94%E8%BF%9B"><span class="toc-number">2.</span> <span class="toc-text">第二部分：机器翻译（MT）简史与演进</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%97%A9%E6%9C%9F%E5%B0%9D%E8%AF%95%EF%BC%9A%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91-RBMT"><span class="toc-number">2.1.</span> <span class="toc-text">早期尝试：基于规则的机器翻译 (RBMT)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91-SMT-%E7%9A%84%E9%BB%84%E9%87%91%E6%97%B6%E4%BB%A3"><span class="toc-number">2.2.</span> <span class="toc-text">统计机器翻译 (SMT) 的黄金时代</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91-NMT-%E7%9A%84%E9%9D%A9%E5%91%BD"><span class="toc-number">2.3.</span> <span class="toc-text">神经机器翻译 (NMT) 的革命</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%EF%BC%9A%E6%B7%B1%E5%85%A5%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%EF%BC%9ATransformer-%E7%9A%84%E5%B4%9B%E8%B5%B7"><span class="toc-number">3.</span> <span class="toc-text">第三部分：深入神经机器翻译：Transformer 的崛起</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B-Seq2Seq-%E5%8F%8A%E5%85%B6%E9%99%90%E5%88%B6"><span class="toc-number">3.1.</span> <span class="toc-text">序列到序列模型 (Seq2Seq) 及其限制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Attention-Mechanism-%E7%9A%84%E5%BC%95%E5%85%A5"><span class="toc-number">3.2.</span> <span class="toc-text">注意力机制 (Attention Mechanism) 的引入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Transformer-%E6%9E%B6%E6%9E%84%EF%BC%9ANMT-%E7%9A%84%E9%87%8C%E7%A8%8B%E7%A2%91"><span class="toc-number">3.3.</span> <span class="toc-text">Transformer 架构：NMT 的里程碑</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3"><span class="toc-number">3.4.</span> <span class="toc-text">编码器-解码器结构详解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-Positional-Encoding"><span class="toc-number">3.5.</span> <span class="toc-text">位置编码 (Positional Encoding)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B-Multi-Head-Self-Attention"><span class="toc-number">3.6.</span> <span class="toc-text">多头自注意力 (Multi-Head Self-Attention)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C-Feed-Forward-Network"><span class="toc-number">3.7.</span> <span class="toc-text">前馈网络 (Feed-Forward Network)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96-Residual-Connection-Layer-Normalization"><span class="toc-number">3.8.</span> <span class="toc-text">残差连接与层归一化 (Residual Connection &amp; Layer Normalization)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%EF%BC%9ANMT-%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E4%B8%8E%E6%8C%91%E6%88%98"><span class="toc-number">4.</span> <span class="toc-text">第四部分：NMT 的关键技术与挑战</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9C%80%E6%B1%82%E4%B8%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.1.</span> <span class="toc-text">数据需求与预训练模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-number">4.2.</span> <span class="toc-text">评估指标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%8E%E8%B5%84%E6%BA%90%E8%AF%AD%E8%A8%80%E9%97%AE%E9%A2%98-Low-Resource-Languages"><span class="toc-number">4.3.</span> <span class="toc-text">低资源语言问题 (Low-Resource Languages)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94%E4%B8%8E%E4%B8%AA%E6%80%A7%E5%8C%96%E7%BF%BB%E8%AF%91"><span class="toc-number">4.4.</span> <span class="toc-text">领域适应与个性化翻译</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E4%B8%8E%E9%B2%81%E6%A3%92%E6%80%A7"><span class="toc-number">4.5.</span> <span class="toc-text">模型可解释性与鲁棒性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E7%BF%BB%E8%AF%91%E7%9A%84%E6%9C%AA%E6%9D%A5"><span class="toc-number">4.6.</span> <span class="toc-text">多模态翻译的未来</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%EF%BC%9ANLP-%E4%B8%8E-MT-%E7%9A%84%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B"><span class="toc-number">5.</span> <span class="toc-text">第五部分：NLP 与 MT 的未来展望</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-LLMs-%E5%AF%B9-NMT-%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">5.1.</span> <span class="toc-text">大语言模型 (LLMs) 对 NMT 的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%9A%E7%94%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-AGI-%E7%9A%84%E6%84%BF%E6%99%AF"><span class="toc-number">5.2.</span> <span class="toc-text">通用人工智能 (AGI) 的愿景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%A6%E7%90%86%E3%80%81%E5%81%8F%E8%A7%81%E4%B8%8E%E8%B4%9F%E8%B4%A3%E4%BB%BB%E7%9A%84AI"><span class="toc-number">5.3.</span> <span class="toc-text">伦理、偏见与负责任的AI</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">6.</span> <span class="toc-text">结论</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/22/hello-world/" title="Hello World">Hello World</a><time datetime="2025-07-22T03:54:52.788Z" title="发表于 2025-07-22 11:54:52">2025-07-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/22/%E5%8D%9A%E5%BC%88%E8%AE%BA%E5%9F%BA%E7%A1%80/" title="博弈论基础">博弈论基础</a><time datetime="2025-07-22T03:54:52.788Z" title="发表于 2025-07-22 11:54:52">2025-07-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/21/2025-07-22-035321/" title="范德华异质结的物理特性：一场二维材料的奇幻交响">范德华异质结的物理特性：一场二维材料的奇幻交响</a><time datetime="2025-07-21T19:53:21.000Z" title="发表于 2025-07-22 03:53:21">2025-07-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/21/2025-07-22-035156/" title="引力透镜效应与宇宙学：洞察宇宙奥秘的强大之眼">引力透镜效应与宇宙学：洞察宇宙奥秘的强大之眼</a><time datetime="2025-07-21T19:51:56.000Z" title="发表于 2025-07-22 03:51:56">2025-07-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/21/2025-07-22-035106/" title="宜居带的定义与演化：宇宙生命绿洲的探寻">宜居带的定义与演化：宇宙生命绿洲的探寻</a><time datetime="2025-07-21T19:51:06.000Z" title="发表于 2025-07-22 03:51:06">2025-07-22</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2025 By qmwneb946</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.2</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'qmwneb946/blog',
      'data-repo-id': 'R_kgDOPM6cWw',
      'data-category-id': 'DIC_kwDOPM6cW84CtEzo',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme)
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Giscus' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Giscus' === 'Giscus' || !false) {
    if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>