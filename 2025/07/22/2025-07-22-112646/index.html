<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>随机森林：从原理到实践的决策树集成艺术 | qmwneb946 的博客</title><meta name="author" content="qmwneb946"><meta name="copyright" content="qmwneb946"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="大家好，我是你们的老朋友 qmwneb946。今天，我们要深入探讨一个在机器学习领域大放异彩的算法——随机森林（Random Forest）。它如同森林中的群策群力，将看似简单的个体（决策树）凝聚成一个强大而稳定的整体，无论是处理分类问题还是回归问题，都能展现出卓越的性能。 随机森林由Leo Breiman于2001年提出，它凭借着出色的准确性、对过拟合的鲁棒性以及处理高维数据的能力，迅速成为了数">
<meta property="og:type" content="article">
<meta property="og:title" content="随机森林：从原理到实践的决策树集成艺术">
<meta property="og:url" content="https://qmwneb946.dpdns.org/2025/07/22/2025-07-22-112646/index.html">
<meta property="og:site_name" content="qmwneb946 的博客">
<meta property="og:description" content="大家好，我是你们的老朋友 qmwneb946。今天，我们要深入探讨一个在机器学习领域大放异彩的算法——随机森林（Random Forest）。它如同森林中的群策群力，将看似简单的个体（决策树）凝聚成一个强大而稳定的整体，无论是处理分类问题还是回归问题，都能展现出卓越的性能。 随机森林由Leo Breiman于2001年提出，它凭借着出色的准确性、对过拟合的鲁棒性以及处理高维数据的能力，迅速成为了数">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qmwneb946.dpdns.org/img/icon.png">
<meta property="article:published_time" content="2025-07-22T03:26:46.000Z">
<meta property="article:modified_time" content="2025-07-22T22:25:10.294Z">
<meta property="article:author" content="qmwneb946">
<meta property="article:tag" content="科技前沿">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="随机森林在分类与回归中的应用">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qmwneb946.dpdns.org/img/icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "随机森林：从原理到实践的决策树集成艺术",
  "url": "https://qmwneb946.dpdns.org/2025/07/22/2025-07-22-112646/",
  "image": "https://qmwneb946.dpdns.org/img/icon.png",
  "datePublished": "2025-07-22T03:26:46.000Z",
  "dateModified": "2025-07-22T22:25:10.294Z",
  "author": [
    {
      "@type": "Person",
      "name": "qmwneb946",
      "url": "https://github.com/qmwneb946"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qmwneb946.dpdns.org/2025/07/22/2025-07-22-112646/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '随机森林：从原理到实践的决策树集成艺术',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2845632165165414" crossorigin="anonymous"></script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="qmwneb946 的博客" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qmwneb946 的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">随机森林：从原理到实践的决策树集成艺术</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">随机森林：从原理到实践的决策树集成艺术<a class="post-edit-link" href="https://github.com/qmwneb946/blog/edit/main/source/_posts/2025-07-22-112646.md" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-22T03:26:46.000Z" title="发表于 2025-07-22 11:26:46">2025-07-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-22T22:25:10.294Z" title="更新于 2025-07-23 06:25:10">2025-07-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A7%91%E6%8A%80%E5%89%8D%E6%B2%BF/">科技前沿</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><p>大家好，我是你们的老朋友 qmwneb946。今天，我们要深入探讨一个在机器学习领域大放异彩的算法——随机森林（Random Forest）。它如同森林中的群策群力，将看似简单的个体（决策树）凝聚成一个强大而稳定的整体，无论是处理分类问题还是回归问题，都能展现出卓越的性能。</p>
<p>随机森林由Leo Breiman于2001年提出，它凭借着出色的准确性、对过拟合的鲁棒性以及处理高维数据的能力，迅速成为了数据科学家的宠儿。如果你曾困惑于单一决策树的易过拟合问题，或者好奇集成学习的魔力，那么这篇文章将为你揭开随机森林的神秘面纱，带你从底层原理到实际应用，全面理解这一强大的工具。</p>
<p>我们将从随机森林的基石——决策树开始，逐步过渡到集成学习的核心思想Bagging，然后深入剖析随机森林如何将二者精妙结合，并探讨其在分类与回归任务中的具体应用、参数调优以及如何利用其提取特征重要性。准备好了吗？让我们一起踏上这场探索随机森林的旅程吧！</p>
<h2 id="决策树：随机森林的基石">决策树：随机森林的基石</h2>
<p>要理解随机森林，我们必须先从它的“细胞”——决策树说起。决策树是一种直观且易于解释的非参数监督学习算法，它通过一系列基于特征的判断规则来对数据进行分类或回归预测，最终形成一个树状结构。</p>
<h3 id="什么是决策树？">什么是决策树？</h3>
<p>想象你正在玩一个“猜动物”的游戏。你可能会问：“它有羽毛吗？”如果答案是“是”，你可能接着问：“它会飞吗？”如果答案是“否”，你可能猜是企鹅。这个过程本质上就是一棵决策树。</p>
<p>决策树模型将特征空间划分为若干个矩形区域，每个区域对应一个决策或预测值。对于分类问题，叶子节点表示一个类别标签；对于回归问题，叶子节点表示一个连续值。</p>
<h3 id="决策树的构建">决策树的构建</h3>
<p>决策树的构建过程是一个递归的自上而下、分而治之的过程。在每个节点，算法会选择一个最优特征来分裂数据，使得分裂后的子集尽可能“纯净”。“纯净”意味着子集中的数据点大多属于同一类别（分类）或具有相似的输出值（回归）。</p>
<p>选择最优分裂点通常基于以下指标：</p>
<h4 id="信息增益-Information-Gain">信息增益 (Information Gain)</h4>
<p>信息增益是ID3和C4.5算法中用于选择最佳分裂特征的标准。它衡量了在一个特征上进行分裂后，系统不确定性减少的程度。</p>
<p>首先，我们需要理解“熵”（Entropy），它表示了数据集的混乱程度或不纯度。对于一个包含<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>个类别的分类问题，数据集<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span>的熵定义为：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>p</mi><mi>i</mi></msub><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">H(S) = -\sum_{i=1}^k p_i \log_2 p_i 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.1138em;vertical-align:-1.2777em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8361em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是数据集中类别 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 所占的比例。熵值越高，数据集的混乱程度越大。</p>
<p>信息增益则是父节点的熵与子节点加权平均熵之间的差值。对于特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span>在数据集<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span>上的信息增益定义为：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>I</mi><mi>G</mi><mo stretchy="false">(</mo><mi>S</mi><mo separator="true">,</mo><mi>A</mi><mo stretchy="false">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><mo>−</mo><munder><mo>∑</mo><mrow><mi>v</mi><mo>∈</mo><mtext>Values</mtext><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow></munder><mfrac><mrow><mi mathvariant="normal">∣</mi><msub><mi>S</mi><mi>v</mi></msub><mi mathvariant="normal">∣</mi></mrow><mrow><mi mathvariant="normal">∣</mi><mi>S</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><mi>H</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>v</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">IG(S, A) = H(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} H(S_v) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal">G</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">A</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.943em;vertical-align:-1.516em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.809em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="mrel mtight">∈</span><span class="mord text mtight"><span class="mord mtight">Values</span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">A</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord">∣</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Values</mtext><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Values}(A)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Values</span></span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mclose">)</span></span></span></span> 是特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span>所有可能取值的集合，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">S_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span>取值为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span>时的数据子集，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><msub><mi>S</mi><mi>v</mi></msub><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|S_v|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span></span></span></span> 是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">S_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的样本数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>S</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|S|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord">∣</span></span></span></span> 是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span> 的样本数。信息增益越大，说明使用该特征进行分裂后，数据集的纯度提升越大。</p>
<h4 id="基尼不纯度-Gini-Impurity">基尼不纯度 (Gini Impurity)</h4>
<p>基尼不纯度是CART（Classification and Regression Trees）算法中常用的一个度量，它表示从数据集中随机选取两个样本，它们类别不一致的概率。基尼不纯度越低，数据集的纯度越高。</p>
<p>对于一个包含<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>个类别的分类问题，数据集<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span>的基尼不纯度定义为：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>G</mi><mi>i</mi><mi>n</mi><mi>i</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo>−</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msubsup><mi>p</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">Gini(S) = 1 - \sum_{i=1}^k p_i^2 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">G</span><span class="mord mathnormal">ini</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:3.1138em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8361em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是数据集中类别 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 所占的比例。</p>
<p>在分裂时，算法会选择一个特征和分裂点，使得分裂后子集的加权基尼不纯度最小化。</p>
<h4 id="剪枝-Pruning">剪枝 (Pruning)</h4>
<p>决策树在构建过程中倾向于过度生长，以至于完美地拟合训练数据，但这通常会导致在未见过的数据上表现不佳，即过拟合。剪枝是解决过拟合的一种技术，它分为预剪枝（Pre-pruning）和后剪枝（Post-pruning）。</p>
<ul>
<li><strong>预剪枝：</strong> 在树构建过程中提前停止分裂。例如，当达到最大深度、节点包含的样本数低于某个阈值、或者信息增益（或基尼不纯度减少）低于某个阈值时，就停止生长。</li>
<li><strong>后剪枝：</strong> 先让决策树完全生长，然后从叶子节点开始，自下而上地删除或合并子树，如果这样做能提高在验证集上的性能，则进行剪枝。</li>
</ul>
<h3 id="决策树的优缺点">决策树的优缺点</h3>
<p><strong>优点：</strong></p>
<ul>
<li><strong>直观易懂：</strong> 树状结构与人类决策过程相似，模型结果易于可视化和解释。</li>
<li><strong>无需特征缩放：</strong> 对数值型特征的尺度不敏感，不需要进行归一化或标准化。</li>
<li><strong>能处理混合数据类型：</strong> 可以同时处理数值型和类别型特征。</li>
<li><strong>非参数：</strong> 不对数据分布做任何假设。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>易过拟合：</strong> 单一决策树模型非常容易在训练数据上过拟合，尤其当树的深度很深时。</li>
<li><strong>对训练数据敏感：</strong> 训练数据中微小的变动可能导致生成完全不同的决策树。这导致了高方差。</li>
<li><strong>局部最优：</strong> 决策树的构建过程是贪婪的，每一步都选择局部最优的分裂点，不保证全局最优。</li>
</ul>
<p>这些缺点，尤其是过拟合和高方差，是促使我们转向集成学习，特别是随机森林的重要原因。</p>
<h2 id="集成学习：群体的智慧">集成学习：群体的智慧</h2>
<p>集成学习（Ensemble Learning）是一种机器学习范式，其核心思想是将多个弱学习器（或称为基学习器）的预测结果结合起来，以获得比任何单个学习器都更强大、更鲁棒的预测模型。俗话说“三个臭皮匠，顶个诸葛亮”，集成学习正是这一智慧的体现。</p>
<h3 id="什么是集成学习？">什么是集成学习？</h3>
<p>集成学习通过构建并结合多个学习器来完成学习任务。它通常比单一模型在准确性、稳定性方面表现更优，尤其在处理复杂数据集时。集成学习主要分为两大类：Bagging和Boosting。</p>
<h3 id="Bagging-Bootstrap-Aggregating">Bagging (Bootstrap Aggregating)</h3>
<p>Bagging（自助采样聚合）是随机森林所依赖的核心集成方法。它的基本思想是：</p>
<ol>
<li><strong>自助采样（Bootstrapping）：</strong> 从原始训练集中有放回地随机抽取<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>个样本，生成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>个新的训练子集（每个子集与原始数据集大小相同）。由于是有放回采样，每个子集会包含原始数据集中约63.2%的唯一样本，有些样本可能被重复抽取，有些样本则可能从未被抽取（这些未被抽取的样本称为袋外（Out-Of-Bag, OOB）样本，它们可以用于模型评估）。</li>
<li><strong>独立训练：</strong> 在每个自助采样得到的训练子集上独立训练一个基学习器（例如决策树）。</li>
<li><strong>聚合预测：</strong>
<ul>
<li><strong>分类问题：</strong> 对所有基学习器的预测结果进行“多数投票”（Majority Voting），得票最多的类别作为最终预测结果。</li>
<li><strong>回归问题：</strong> 对所有基学习器的预测结果进行“平均”（Averaging），得到最终的预测结果。</li>
</ul>
</li>
</ol>
<p>Bagging的主要目的是<strong>降低模型的方差</strong>。通过对不同的训练子集进行训练，每个基学习器都会有所不同，它们之间存在一定的独立性。当这些具有差异性的模型进行集成时，它们各自的错误会相互抵消，从而使整体模型的泛化能力更强，对训练数据的随机波动更不敏感。</p>
<h3 id="Boosting-梯度提升等">Boosting (梯度提升等)</h3>
<p>虽然随机森林不属于Boosting家族，但了解其与Bagging的区别有助于更好地理解集成学习。Boosting与Bagging不同，它是一种串行（Sequential）的集成方法。它的核心思想是：</p>
<ol>
<li><strong>迭代训练：</strong> 弱学习器之间存在依赖关系，新的学习器会根据前一个学习器的表现进行调整。</li>
<li><strong>关注错误：</strong> 每一次迭代都会更加关注那些被前一个学习器预测错误的样本，通过调整样本权重或者残差来“纠正”错误。</li>
<li><strong>加权聚合：</strong> 最终的预测结果是所有弱学习器的加权和。</li>
</ol>
<p>Boosting（如AdaBoost、Gradient Boosting）的主要目的是<strong>降低模型的偏差</strong>，同时也能降低方差。它通过不断纠正错误来逐步提升模型的准确性。</p>
<h3 id="为什么要集成学习？偏差-方差权衡">为什么要集成学习？偏差-方差权衡</h3>
<p>理解集成学习的价值，离不开“偏差-方差权衡”（Bias-Variance Trade-off）的概念。</p>
<ul>
<li><strong>偏差（Bias）：</strong> 描述了模型对训练数据的拟合能力。高偏差意味着模型未能充分学习数据中的模式，导致欠拟合（Underfitting）。</li>
<li><strong>方差（Variance）：</strong> 描述了模型对训练数据波动的敏感程度。高方差意味着模型对训练数据的微小变化过于敏感，导致过拟合（Overfitting）。</li>
</ul>
<p>单一的决策树通常具有<strong>低偏差但高方差</strong>的特点。它能够很好地拟合训练数据（低偏差），但对训练数据的随机波动非常敏感（高方差），容易过拟合。</p>
<p>Bagging，特别是随机森林，通过<strong>降低方差</strong>来提高模型的泛化能力。它通过训练多个独立的、具有高方差的决策树，然后将它们的预测结果平均或投票，从而抵消了它们各自的随机错误，大大减少了整体模型的方差，同时保持了较低的偏差。</p>
<p>因此，集成学习提供了一种强大的框架，可以有效地平衡模型的偏差与方差，从而构建出更稳定、更准确的预测模型。</p>
<h2 id="随机森林：Bagging与随机性的完美结合">随机森林：Bagging与随机性的完美结合</h2>
<p>现在，我们来到了本文的核心——随机森林。随机森林巧妙地结合了决策树的易用性与集成学习的强大能力，并通过引入额外的随机性，进一步提升了模型的性能和鲁棒性。</p>
<h3 id="起源与定义">起源与定义</h3>
<p>随机森林由Leo Breiman于2001年提出，其名称来源于它由多个决策树（森林）组成，并且在构建过程中引入了随机性。它是一个包含多个决策树的分类器，其输出的类别是由各个树输出的类别的众数决定的（分类任务），或输出的数值是各个树输出的数值的平均值决定的（回归任务）。</p>
<h3 id="核心思想">核心思想</h3>
<p>随机森林在Bagging的基础上，引入了两个关键的随机性来源，这使其与普通的Bagging决策树集成算法有所不同，并显著提升了性能：</p>
<h4 id="Bagging-自助采样">Bagging (自助采样)</h4>
<p>如前所述，随机森林首先通过自助采样（Bootstrap Aggregating）从原始训练集中生成多个不同的子数据集。对于每个子数据集，都会独立训练一棵决策树。这意味着每棵树看到的训练数据都是原始数据集的一个有放回的随机子集。这种采样方式确保了每棵树的差异性，是降低方差的第一层保证。</p>
<h4 id="特征随机性-Feature-Randomness">特征随机性 (Feature Randomness)</h4>
<p>这是随机森林独有的核心创新点。在构建每棵决策树时，当需要确定一个节点的最佳分裂特征时，随机森林并<strong>不是考虑所有可用特征</strong>，而是从所有特征中随机选择一个<strong>子集</strong>（例如，如果原始数据集有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>个特征，每次只考虑<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mi>M</mi></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{M}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1133em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9267em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span><span style="top:-2.8867em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1133em;"><span></span></span></span></span></span></span></span></span>个特征或<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mi>M</mi></mrow><annotation encoding="application/x-tex">\log_2 M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9386em;vertical-align:-0.2441em;"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>个特征），然后只从这个子集中选择最优特征进行分裂。</p>
<p>这种特征随机性带来了几个重要优势：</p>
<ul>
<li><strong>进一步降低方差：</strong> 如果数据中存在非常强大的特征，Bagging可能会导致所有决策树在顶部节点都选择该特征，从而使树之间高度相关，限制了方差的降低。特征随机性强制每棵树探索不同的特征组合，使得树之间的相关性更低，从而进一步降低了整体模型的方差。</li>
<li><strong>处理高维数据：</strong> 在特征数量很多时，它能有效提高训练效率，避免过度依赖少数几个特征。</li>
<li><strong>对噪声和异常值更鲁棒：</strong> 因为每棵树只看到部分特征和部分样本，所以单个噪声点或异常值对整个森林的影响被削弱。</li>
</ul>
<h3 id="算法流程">算法流程</h3>
<p>随机森林的构建过程可以概括为以下步骤：</p>
<ol>
<li><strong>输入：</strong> 训练数据集 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mi>N</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>N</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">D = \{(x_1, y_1), \dots, (x_N, y_N)\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)}</span></span></span></span>，特征总数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>，要构建的决策树数量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span>。</li>
<li><strong>循环 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span> 到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span>：</strong><br>
a.  <strong>自助采样：</strong> 从原始训练数据集 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span> 中有放回地随机抽取 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 个样本，形成一个自助采样数据集 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">D_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。<br>
b.  <strong>训练决策树 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">T_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：</strong> 在数据集 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">D_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 上训练一棵决策树。在构建每棵树的每个节点时，执行以下操作：<br>
i.  <strong>特征子集选择：</strong> 随机从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> 个特征中选择 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span> 个特征（通常 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><msqrt><mi>M</mi></msqrt></mrow><annotation encoding="application/x-tex">m = \sqrt{M}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1133em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9267em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span><span style="top:-2.8867em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1133em;"><span></span></span></span></span></span></span></span></span> 或 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mi>M</mi></mrow><annotation encoding="application/x-tex">m = \log_2 M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9386em;vertical-align:-0.2441em;"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>）。<br>
ii. <strong>最佳分裂：</strong> 从这 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span> 个随机选择的特征中，找出最佳分裂特征和分裂点（基于信息增益或基尼不纯度）。<br>
iii. <strong>不剪枝：</strong> 决策树 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">T_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 会一直生长，直到满足某些终止条件（如节点样本数低于阈值，或达到最大深度），通常不进行剪枝。不剪枝是随机森林的一个重要特点，因为通过集成大量的树可以抵消单棵树过拟合的影响。</li>
<li><strong>输出：</strong> 得到一个包含 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 棵决策树的森林 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>T</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>T</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>T</mi><mi>K</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{T_1, T_2, \dots, T_K\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>。</li>
<li><strong>预测：</strong> 对于新的未知样本 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></mrow><annotation encoding="application/x-tex">x_{new}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：<br>
a.  <strong>分类：</strong> 每棵决策树 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">T_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 对 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></mrow><annotation encoding="application/x-tex">x_{new}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 进行分类预测，得到一个类别标签 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">c_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。最终的预测结果是所有 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 个类别标签的多数投票结果。<br>
b.  <strong>回归：</strong> 每棵决策树 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">T_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 对 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub></mrow><annotation encoding="application/x-tex">x_{new}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 进行回归预测，得到一个数值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">r_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。最终的预测结果是所有 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 个数值的平均值。</li>
</ol>
<h3 id="随机森林的优势">随机森林的优势</h3>
<p>随机森林之所以如此流行，得益于它拥有许多令人称赞的优势：</p>
<ul>
<li><strong>降低过拟合：</strong> 这是其最重要的优势之一。通过集成多棵基决策树，并引入样本和特征的随机性，随机森林能够显著降低模型的方差，从而有效地抑制过拟合，提高模型的泛化能力。即使单棵决策树过拟合，其错误也会在聚合过程中被其他树的正确预测所“稀释”。</li>
<li><strong>高准确性：</strong> 通常在各种数据集上都能提供较高的预测准确率，甚至与Boosting算法（如GBDT、XGBoost）相媲美。</li>
<li><strong>处理高维数据：</strong> 即使特征数量远大于样本数量，随机森林也能有效地工作。特征随机性使得它在高维数据中表现出色。</li>
<li><strong>处理缺失值：</strong> 某些实现（如Scikit-learn）可以直接处理缺失值，或通过其内部机制（如OOB样本）推断缺失值。</li>
<li><strong>鲁棒性强：</strong> 对数据集中的噪声和异常值不敏感，因为每棵树只看到部分数据和部分特征，个别异常值的影响会被平均化。</li>
<li><strong>特征重要性评估：</strong> 随机森林可以计算出每个特征的重要性分数，这对于特征选择、理解模型以及进行领域知识发现非常有价值。</li>
<li><strong>并行性：</strong> 每棵决策树的训练过程是独立的，因此可以很容易地并行化，大大加快训练速度。</li>
<li><strong>无须特征缩放：</strong> 像单一决策树一样，随机森林对特征的尺度不敏感，无需进行特征归一化或标准化。</li>
</ul>
<h3 id="随机森林的局限性">随机森林的局限性</h3>
<p>尽管随机森林强大，但它并非完美无缺，也存在一些局限性：</p>
<ul>
<li><strong>解释性不如单棵树：</strong> 尽管单棵决策树非常直观，但由于随机森林是多棵树的集合，其内部决策机制变得复杂，难以像单一决策树那样直接可视化和解释。</li>
<li><strong>训练和预测开销：</strong> 相较于单一决策树，随机森林需要训练更多的树，导致训练时间和内存消耗增加。在预测时，也需要计算所有树的预测结果并进行聚合，因此预测速度可能比单一模型慢。对于超大型数据集，这可能成为一个问题。</li>
<li><strong>偏差问题：</strong> 尽管主要降低方差，但在某些特定数据集上，如果基学习器的偏差本身就很高（例如，每个树都对某些模式有很强的偏见），随机森林可能会继承一部分这种偏差，导致预测结果不如Boosting算法。</li>
<li><strong>对文本数据或稀疏数据不够友好：</strong> 对于高维稀疏特征（如文本中的词袋模型），随机森林的表现可能不如线性模型或神经网络。</li>
</ul>
<p>尽管存在这些局限，随机森林在大多数实际应用中仍是一个非常优秀且可靠的选择。</p>
<h2 id="随机森林在分类中的应用">随机森林在分类中的应用</h2>
<p>随机森林在分类任务中表现卓越，是图像识别、医学诊断、金融欺诈检测等领域常用的算法。</p>
<h3 id="原理：多数投票">原理：多数投票</h3>
<p>当用于分类时，随机森林中的每棵决策树都会对新的输入样本进行分类，然后输出一个类别预测。最终，整个森林的预测结果是通过**多数投票（Majority Voting）**来确定的。也就是说，哪个类别获得的票数最多，那个类别就是随机森林的最终预测结果。</p>
<p>例如，如果有100棵树，其中60棵预测为“类别A”，40棵预测为“类别B”，那么随机森林的最终预测就是“类别A”。</p>
<h3 id="评估指标">评估指标</h3>
<p>在分类任务中，我们通常使用以下指标来评估模型的性能：</p>
<ul>
<li><strong>准确率 (Accuracy)：</strong> 正确预测的样本数占总样本数的比例。<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>A</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>y</mi><mo>=</mo><mfrac><mrow><mtext>TP</mtext><mo>+</mo><mtext>TN</mtext></mrow><mrow><mtext>TP</mtext><mo>+</mo><mtext>TN</mtext><mo>+</mo><mtext>FP</mtext><mo>+</mo><mtext>FN</mtext></mrow></mfrac></mrow><annotation encoding="application/x-tex">Accuracy = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">cc</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">cy</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.1297em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">TP</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord text"><span class="mord">TN</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord text"><span class="mord">FP</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord text"><span class="mord">FN</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">TP</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord text"><span class="mord">TN</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
</li>
<li><strong>精确率 (Precision)：</strong> 预测为正类的样本中，真正为正类的比例。衡量模型预测正类的准确性。<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mtext>TP</mtext><mrow><mtext>TP</mtext><mo>+</mo><mtext>FP</mtext></mrow></mfrac></mrow><annotation encoding="application/x-tex">Precision = \frac{\text{TP}}{\text{TP} + \text{FP}} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal">rec</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.1297em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">TP</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord text"><span class="mord">FP</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">TP</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
</li>
<li><strong>召回率 (Recall / Sensitivity)：</strong> 真正为正类的样本中，被模型正确预测为正类的比例。衡量模型识别出所有正类的能力。<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mtext>TP</mtext><mrow><mtext>TP</mtext><mo>+</mo><mtext>FN</mtext></mrow></mfrac></mrow><annotation encoding="application/x-tex">Recall = \frac{\text{TP}}{\text{TP} + \text{FN}} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">ec</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">ll</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.1297em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">TP</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord text"><span class="mord">FN</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">TP</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
</li>
<li><strong>F1分数 (F1-Score)：</strong> 精确率和召回率的调和平均值，综合考虑了两者的表现，尤其适用于类别不平衡的情况。<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>F</mi><mn>1</mn><mo>−</mo><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>=</mo><mn>2</mn><mo>×</mo><mfrac><mrow><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>×</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow><mrow><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">F1-Score = 2 \times \frac{Precision \times Recall}{Precision + Recall} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">core</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.1408em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal">rec</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">ec</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">ll</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal">rec</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">ec</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">ll</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
</li>
<li><strong>混淆矩阵 (Confusion Matrix)：</strong> 详细展示了模型预测结果与真实标签之间的对应关系，包括真阳性（TP）、真阴性（TN）、假阳性（FP）、假阴性（FN）。</li>
<li><strong>ROC曲线 (Receiver Operating Characteristic Curve) 和 AUC (Area Under the Curve)：</strong> ROC曲线以假阳性率（FPR）为横轴，真阳性率（TPR，即召回率）为纵轴，展示了模型在不同分类阈值下的性能。AUC是ROC曲线下的面积，值越接近1，模型性能越好。</li>
</ul>
<h3 id="示例：鸢尾花分类">示例：鸢尾花分类</h3>
<p>让我们使用Python和Scikit-learn库来演示随机森林在经典的鸢尾花分类问题上的应用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, classification_report, confusion_matrix</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line">feature_names = iris.feature_names</span><br><span class="line">target_names = iris.target_names</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集划分为训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>, stratify=y)</span><br><span class="line"><span class="comment"># stratify=y 确保训练集和测试集中每个类别的比例与原始数据集中相同</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--- 随机森林分类示例 ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;训练集样本数: <span class="subst">&#123;<span class="built_in">len</span>(X_train)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;测试集样本数: <span class="subst">&#123;<span class="built_in">len</span>(X_test)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化随机森林分类器</span></span><br><span class="line"><span class="comment"># n_estimators: 森林中树的数量</span></span><br><span class="line"><span class="comment"># random_state: 随机种子，确保结果可复现</span></span><br><span class="line">clf = RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>, n_jobs=-<span class="number">1</span>) <span class="comment"># n_jobs=-1 使用所有可用CPU核心</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;开始训练随机森林分类器...&quot;</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练完成！&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上进行预测</span></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型性能</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型评估报告:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;准确率 (Accuracy): <span class="subst">&#123;accuracy_score(y_test, y_pred):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n分类报告:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred, target_names=target_names))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 混淆矩阵</span></span><br><span class="line">conf_mat = confusion_matrix(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n混淆矩阵:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(conf_mat)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化混淆矩阵</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">sns.heatmap(conf_mat, annot=<span class="literal">True</span>, fmt=<span class="string">&#x27;d&#x27;</span>, cmap=<span class="string">&#x27;Blues&#x27;</span>,</span><br><span class="line">            xticklabels=target_names, yticklabels=target_names)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;预测类别&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;真实类别&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;随机森林分类器混淆矩阵&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看特征重要性</span></span><br><span class="line">feature_importances = pd.Series(clf.feature_importances_, index=feature_names).sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n特征重要性:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(feature_importances)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化特征重要性</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">sns.barplot(x=feature_importances.values, y=feature_importances.index, palette=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;特征重要性 (Gini Importance)&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;重要性得分&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;特征名称&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><strong>代码解释：</strong></p>
<ol>
<li><strong>加载数据：</strong> 使用<code>load_iris</code>加载数据集。</li>
<li><strong>数据划分：</strong> <code>train_test_split</code>将数据分为训练集和测试集，<code>stratify=y</code>确保了训练集和测试集中各类别样本比例一致，这对于分类问题非常重要。</li>
<li><strong>模型初始化：</strong> <code>RandomForestClassifier</code>是Scikit-learn中用于分类的随机森林实现。
<ul>
<li><code>n_estimators=100</code>：指定了构建100棵决策树。通常树的数量越多，模型越稳定，但计算成本也越高。</li>
<li><code>random_state=42</code>：设置随机种子，确保每次运行代码都能得到相同的结果，便于复现和调试。</li>
<li><code>n_jobs=-1</code>：利用所有可用的CPU核心进行并行计算，加速训练过程。</li>
</ul>
</li>
<li><strong>模型训练：</strong> <code>clf.fit(X_train, y_train)</code>使用训练数据训练模型。</li>
<li><strong>模型预测：</strong> <code>clf.predict(X_test)</code>在测试集上进行预测。</li>
<li><strong>模型评估：</strong>
<ul>
<li><code>accuracy_score</code>计算整体准确率。</li>
<li><code>classification_report</code>提供更详细的分类指标（精确率、召回率、F1分数）和每个类别的支持数。</li>
<li><code>confusion_matrix</code>生成混淆矩阵，直观地展示了分类器的性能。</li>
</ul>
</li>
<li><strong>特征重要性：</strong> <code>clf.feature_importances_</code>属性提供了模型中每个特征的重要性得分（基于Gini不纯度减少的平均值）。通过这些得分，我们可以了解哪些特征对模型的预测贡献最大。</li>
</ol>
<p>通过上述代码，我们可以看到随机森林在鸢尾花数据集上通常能达到非常高的准确率，并且能够清晰地展示出对分类贡献最大的特征。</p>
<h2 id="随机森林在回归中的应用">随机森林在回归中的应用</h2>
<p>随机森林不仅适用于分类任务，在回归任务中也同样表现出色，可用于预测连续值，如房价预测、股票价格预测、温度预测等。</p>
<h3 id="原理：平均值">原理：平均值</h3>
<p>当用于回归时，随机森林中的每棵决策树都会对新的输入样本进行回归预测，输出一个连续值。最终，整个森林的预测结果是通过对所有基决策树的预测值进行**平均（Averaging）**来确定的。</p>
<p>例如，如果有100棵树，它们对某个样本的预测值分别是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><mn>100</mn></msub></mrow><annotation encoding="application/x-tex">y_1, y_2, \dots, y_{100}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">100</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，那么随机森林的最终预测值就是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>100</mn></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>100</mn></msubsup><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\frac{1}{100}\sum_{i=1}^{100} y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.299em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">100</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.954em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">100</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</p>
<p>通过平均值的方式，可以有效地平滑掉单棵树的预测噪声，从而提高整体预测的稳定性和准确性。</p>
<h3 id="评估指标-2">评估指标</h3>
<p>在回归任务中，我们通常使用以下指标来评估模型的性能：</p>
<ul>
<li><strong>均方误差 (Mean Squared Error, MSE)：</strong> 最常用的回归损失函数，衡量预测值与真实值之间差的平方的平均值。<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>M</mi><mi>S</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">MSE = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">MSE</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">n</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p>
其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是真实值，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\hat{y}_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是预测值，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> 是样本数。MSE对较大的误差惩罚更大。</li>
<li><strong>平均绝对误差 (Mean Absolute Error, MAE)：</strong> 衡量预测值与真实值之间绝对差的平均值。<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>M</mi><mi>A</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi mathvariant="normal">∣</mi><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">MAE = \frac{1}{n}\sum_{i=1}^n |y_i - \hat{y}_i| 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">n</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span></span></span></span></span></p>
MAE对异常值不如MSE敏感。</li>
<li><strong>R平方 (R-squared / Coefficient of Determination)：</strong> 衡量模型解释因变量方差的比例，值越接近1越好，表示模型拟合效果越好。<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><mi>y</mi><mo>ˉ</mo></mover><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8641em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.4978em;vertical-align:-0.994em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5038em;"><span style="top:-2.3057em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.5678em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">ˉ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7401em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.6897em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7622em;vertical-align:-0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.5678em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">ˉ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span></span></span></span> 是真实值的平均值。</li>
<li><strong>均方根误差 (Root Mean Squared Error, RMSE)：</strong> MSE的平方根，与因变量的单位相同，更具可解释性。<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>R</mi><mi>M</mi><mi>S</mi><mi>E</mi><mo>=</mo><msqrt><mrow><mi>M</mi><mi>S</mi><mi>E</mi></mrow></msqrt></mrow><annotation encoding="application/x-tex">RMSE = \sqrt{MSE} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">RMSE</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.0645em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9755em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal" style="margin-right:0.05764em;">MSE</span></span></span><span style="top:-2.9355em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.0645em;"><span></span></span></span></span></span></span></span></span></span></p>
</li>
</ul>
<h3 id="示例：房价预测-简化版">示例：房价预测 (简化版)</h3>
<p>为了演示随机森林在回归中的应用，我们使用一个简化版的房价数据集（Scikit-learn自带的波士顿房价数据集，虽然其使用已被弃用，但作为示例仍然合适）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="comment"># 更推荐使用加州房价数据集作为回归示例</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error, r2_score, mean_absolute_error</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载加州房价数据集</span></span><br><span class="line">housing = fetch_california_housing()</span><br><span class="line">X = housing.data</span><br><span class="line">y = housing.target</span><br><span class="line">feature_names = housing.feature_names</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集划分为训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--- 随机森林回归示例 ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;训练集样本数: <span class="subst">&#123;<span class="built_in">len</span>(X_train)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;测试集样本数: <span class="subst">&#123;<span class="built_in">len</span>(X_test)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化随机森林回归器</span></span><br><span class="line"><span class="comment"># n_estimators: 森林中树的数量</span></span><br><span class="line"><span class="comment"># random_state: 随机种子，确保结果可复现</span></span><br><span class="line">regressor = RandomForestRegressor(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>, n_jobs=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;开始训练随机森林回归器...&quot;</span>)</span><br><span class="line">regressor.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练完成！&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上进行预测</span></span><br><span class="line">y_pred = regressor.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型性能</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型评估报告:&quot;</span>)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line">rmse = np.sqrt(mse)</span><br><span class="line">mae = mean_absolute_error(y_test, y_pred)</span><br><span class="line">r2 = r2_score(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;均方误差 (MSE): <span class="subst">&#123;mse:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;均方根误差 (RMSE): <span class="subst">&#123;rmse:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;平均绝对误差 (MAE): <span class="subst">&#123;mae:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;R平方 (R-squared): <span class="subst">&#123;r2:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化预测结果与真实值的对比</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.scatter(y_test, y_pred, alpha=<span class="number">0.3</span>)</span><br><span class="line">plt.plot([y_test.<span class="built_in">min</span>(), y_test.<span class="built_in">max</span>()], [y_test.<span class="built_in">min</span>(), y_test.<span class="built_in">max</span>()], <span class="string">&#x27;r--&#x27;</span>, lw=<span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;真实房价 (千美元)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;预测房价 (千美元)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;随机森林回归：真实值 vs. 预测值&#x27;</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看特征重要性</span></span><br><span class="line">feature_importances_reg = pd.Series(regressor.feature_importances_, index=feature_names).sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n特征重要性 (回归):&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(feature_importances_reg)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化特征重要性</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">sns.barplot(x=feature_importances_reg.values, y=feature_importances_reg.index, palette=<span class="string">&#x27;magma&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;特征重要性 (回归)&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;重要性得分&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;特征名称&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><strong>代码解释：</strong></p>
<ol>
<li><strong>加载数据：</strong> 使用<code>fetch_california_housing</code>加载加州房价数据集，这是Scikit-learn官方推荐的替代波士顿房价数据集的回归示例。</li>
<li><strong>数据划分：</strong> 同样进行训练集和测试集的划分。</li>
<li><strong>模型初始化与训练：</strong> <code>RandomForestRegressor</code>是Scikit-learn中用于回归的随机森林实现，参数设置与分类器类似。</li>
<li><strong>模型预测：</strong> 进行预测并获取连续值。</li>
<li><strong>模型评估：</strong> 使用MSE、RMSE、MAE和R-squared来评估回归模型的性能。</li>
<li><strong>可视化：</strong> 散点图展示了真实值与预测值的关系，红色虚线表示理想的完美预测（y=x）。点越接近这条线，模型性能越好。</li>
<li><strong>特征重要性：</strong> 同样可以通过<code>regressor.feature_importances_</code>获取特征重要性。</li>
</ol>
<p>通过这个回归示例，我们可以看到随机森林能够有效地捕捉数据中的非线性关系，并对连续值进行准确预测。</p>
<h2 id="随机森林的参数调优">随机森林的参数调优</h2>
<p>随机森林虽然默认参数通常表现良好，但通过对超参数进行精细调优，可以进一步提升模型性能，防止过拟合或欠拟合，并优化计算效率。以下是一些随机森林中最重要的超参数及其含义：</p>
<h3 id="核心参数">核心参数</h3>
<ul>
<li>
<p><code>n_estimators</code> (整数，默认值=100)：</p>
<ul>
<li><strong>含义：</strong> 森林中决策树的数量。</li>
<li><strong>影响：</strong> 树越多，模型的方差越小，泛化能力越强，通常准确率也越高。但计算时间也会随之增加，且达到一定数量后，性能提升会趋于平缓。</li>
<li><strong>调优建议：</strong> 从100开始，逐步增加，直到模型性能不再显著提升或计算成本过高。</li>
</ul>
</li>
<li>
<p><code>max_features</code> (整数、浮点数、{“auto”, “sqrt”, “log2”}或None，默认值=“sqrt”)：</p>
<ul>
<li><strong>含义：</strong> 在每个节点分裂时，随机考虑的特征子集的大小。
<ul>
<li><code>&quot;auto&quot;</code>/<code>&quot;sqrt&quot;</code>：分类时使用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mi>M</mi></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{M}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1133em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9267em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span><span style="top:-2.8867em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1133em;"><span></span></span></span></span></span></span></span></span>，回归时使用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> （所有特征）。但Scikit-learn 1.1版本后，<code>&quot;auto&quot;</code>已被弃用，推荐直接使用<code>&quot;sqrt&quot;</code>或<code>&quot;log2&quot;</code>。</li>
<li><code>&quot;sqrt&quot;</code>：每次分裂考虑 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mi>M</mi></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{M}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1133em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9267em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span><span style="top:-2.8867em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1133em;"><span></span></span></span></span></span></span></span></span> 个特征（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> 是总特征数）。这是分类的常用默认值。</li>
<li><code>&quot;log2&quot;</code>：每次分裂考虑 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mi>M</mi></mrow><annotation encoding="application/x-tex">\log_2 M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9386em;vertical-align:-0.2441em;"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> 个特征。</li>
<li>整数：直接指定考虑的特征数量。</li>
<li>浮点数：指定考虑的特征比例（例如0.5表示考虑一半特征）。</li>
<li><code>None</code>：每次分裂考虑所有特征（即退化为Bagging Trees）。</li>
</ul>
</li>
<li><strong>影响：</strong> <code>max_features</code>越小，树之间的相关性越低，从而可以降低方差，但可能会增加偏差。选择过大会使树之间相似度高，失去随机森林的优势。</li>
<li><strong>调优建议：</strong> 从默认值开始，尝试<code>&quot;log2&quot;</code>、较小的整数或比例值。这是调优随机森林的关键参数之一。</li>
</ul>
</li>
<li>
<p><code>max_depth</code> (整数或None，默认值=None)：</p>
<ul>
<li><strong>含义：</strong> 每棵决策树的最大深度。如果为None，树会一直生长到叶子节点纯净或包含的样本数少于<code>min_samples_split</code>。</li>
<li><strong>影响：</strong> 限制树的深度有助于防止过拟合（尤其对单棵树而言），但可能导致欠拟合。在随机森林中，由于有<code>n_estimators</code>的存在，通常可以允许树生长得更深。</li>
<li><strong>调优建议：</strong> 如果发现模型过拟合，可以尝试减小此参数。</li>
</ul>
</li>
</ul>
<h3 id="其他重要参数">其他重要参数</h3>
<ul>
<li>
<p><code>min_samples_split</code> (整数或浮点数，默认值=2)：</p>
<ul>
<li><strong>含义：</strong> 节点分裂所需的最小样本数。如果一个节点包含的样本数少于这个阈值，它将不会被分裂。</li>
<li><strong>影响：</strong> 限制树的生长，防止过拟合。</li>
<li><strong>调优建议：</strong> 增加此值会使模型更平滑，但可能导致欠拟合。</li>
</ul>
</li>
<li>
<p><code>min_samples_leaf</code> (整数或浮点数，默认值=1)：</p>
<ul>
<li><strong>含义：</strong> 一个叶子节点所需的最小样本数。</li>
<li><strong>影响：</strong> 限制树的生长，防止过拟合。</li>
<li><strong>调优建议：</strong> 增加此值会使模型更平滑，但可能导致欠拟合。</li>
</ul>
</li>
<li>
<p><code>bootstrap</code> (布尔值，默认值=True)：</p>
<ul>
<li><strong>含义：</strong> 是否在构建树时使用自助采样。</li>
<li><strong>影响：</strong> 如果设置为False，则使用整个数据集训练每棵树，这会增加树之间的相关性，通常会增加方差，但可以用于某些特殊情况。对于随机森林，通常保持为True。</li>
</ul>
</li>
<li>
<p><code>oob_score</code> (布尔值，默认值=False)：</p>
<ul>
<li><strong>含义：</strong> 是否使用袋外（Out-Of-Bag, OOB）样本来估计模型的泛化准确率。</li>
<li><strong>影响：</strong> OOB分数可以作为交叉验证的替代，无需额外的数据集划分，节省计算资源。</li>
<li><strong>调优建议：</strong> 设置为True可以在训练完成后直接获取模型的OOB分数。</li>
</ul>
</li>
<li>
<p><code>criterion</code> (字符串，默认值=“gini” for classifier, “mse” for regressor)：</p>
<ul>
<li><strong>分类器：</strong> <code>&#123;&quot;gini&quot;, &quot;entropy&quot;&#125;</code>，衡量分裂质量的函数。</li>
<li><strong>回归器：</strong> <code>&#123;&quot;squared_error&quot;, &quot;absolute_error&quot;, &quot;friedman_mse&quot;, &quot;poisson&quot;&#125;</code>。</li>
<li><strong>影响：</strong> 不同标准可能会导致不同的树结构和性能，但通常&quot;gini&quot;和&quot;squared_error&quot;是很好的起点。</li>
<li><strong>调优建议：</strong> 可以尝试不同标准，看是否带来性能提升。</li>
</ul>
</li>
</ul>
<h3 id="参数调优策略">参数调优策略</h3>
<p>常用的参数调优方法包括：</p>
<h4 id="交叉验证-Cross-validation">交叉验证 (Cross-validation)</h4>
<p>交叉验证是一种评估模型泛化能力的常用技术。它将训练数据划分为多个折（folds），轮流用其中一部分作为验证集，其余作为训练集。这有助于更可靠地评估模型在不同超参数组合下的性能，避免过拟合验证集。</p>
<h4 id="网格搜索-Grid-Search">网格搜索 (Grid Search)</h4>
<p>网格搜索是一种穷举搜索方法，它会在所有指定参数值组合中进行遍历，为每种组合训练模型并使用交叉验证评估性能。最终选择性能最佳的参数组合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义要搜索的参数网格</span></span><br><span class="line">param_grid = &#123;</span><br><span class="line">    <span class="string">&#x27;n_estimators&#x27;</span>: [<span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>],</span><br><span class="line">    <span class="string">&#x27;max_features&#x27;</span>: [<span class="string">&#x27;sqrt&#x27;</span>, <span class="string">&#x27;log2&#x27;</span>, <span class="number">0.5</span>],</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: [<span class="number">5</span>, <span class="number">10</span>, <span class="literal">None</span>],</span><br><span class="line">    <span class="string">&#x27;min_samples_split&#x27;</span>: [<span class="number">2</span>, <span class="number">5</span>],</span><br><span class="line">    <span class="string">&#x27;min_samples_leaf&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化GridSearchCV</span></span><br><span class="line"><span class="comment"># estimator: 要优化的模型</span></span><br><span class="line"><span class="comment"># param_grid: 参数网格</span></span><br><span class="line"><span class="comment"># cv: 交叉验证折数</span></span><br><span class="line"><span class="comment"># scoring: 评估指标（如&#x27;accuracy&#x27; for classification, &#x27;neg_mean_squared_error&#x27; for regression）</span></span><br><span class="line"><span class="comment"># n_jobs: 并行计算核心数</span></span><br><span class="line">grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=<span class="number">42</span>),</span><br><span class="line">                           param_grid=param_grid,</span><br><span class="line">                           cv=<span class="number">5</span>,</span><br><span class="line">                           scoring=<span class="string">&#x27;accuracy&#x27;</span>,</span><br><span class="line">                           n_jobs=-<span class="number">1</span>,</span><br><span class="line">                           verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行网格搜索</span></span><br><span class="line">grid_search.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印最佳参数和最佳分数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n网格搜索最佳参数:&quot;</span>, grid_search.best_params_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;网格搜索最佳分数:&quot;</span>, grid_search.best_score_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用最佳参数的模型进行预测</span></span><br><span class="line">best_clf = grid_search.best_estimator_</span><br><span class="line">y_pred_tuned = best_clf.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;调优后模型在测试集上的准确率: <span class="subst">&#123;accuracy_score(y_test, y_pred_tuned):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="随机搜索-Random-Search">随机搜索 (Random Search)</h4>
<p>网格搜索在参数空间很大时效率较低。随机搜索则在指定的参数范围内随机抽取固定数量的参数组合进行训练和评估。实践证明，在许多情况下，随机搜索在相同计算资源下能找到比网格搜索更好的结果，因为它更可能探索到参数空间中更广阔的区域。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> randint <span class="keyword">as</span> sp_randint</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义要搜索的参数分布（不再是固定的网格，可以是范围或分布）</span></span><br><span class="line">param_dist = &#123;</span><br><span class="line">    <span class="string">&#x27;n_estimators&#x27;</span>: sp_randint(<span class="number">50</span>, <span class="number">200</span>), <span class="comment"># 从50到200之间随机整数</span></span><br><span class="line">    <span class="string">&#x27;max_features&#x27;</span>: [<span class="string">&#x27;sqrt&#x27;</span>, <span class="string">&#x27;log2&#x27;</span>, <span class="number">0.5</span>, <span class="number">0.7</span>],</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: sp_randint(<span class="number">5</span>, <span class="number">20</span>), <span class="comment"># 从5到20之间随机整数</span></span><br><span class="line">    <span class="string">&#x27;min_samples_split&#x27;</span>: sp_randint(<span class="number">2</span>, <span class="number">11</span>),</span><br><span class="line">    <span class="string">&#x27;min_samples_leaf&#x27;</span>: sp_randint(<span class="number">1</span>, <span class="number">11</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化RandomizedSearchCV</span></span><br><span class="line"><span class="comment"># n_iter: 随机采样的组合数量</span></span><br><span class="line">random_search = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=<span class="number">42</span>),</span><br><span class="line">                                   param_distributions=param_dist,</span><br><span class="line">                                   n_iter=<span class="number">50</span>, <span class="comment"># 随机尝试50种不同的参数组合</span></span><br><span class="line">                                   cv=<span class="number">5</span>,</span><br><span class="line">                                   scoring=<span class="string">&#x27;accuracy&#x27;</span>,</span><br><span class="line">                                   n_jobs=-<span class="number">1</span>,</span><br><span class="line">                                   random_state=<span class="number">42</span>,</span><br><span class="line">                                   verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行随机搜索</span></span><br><span class="line">random_search.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印最佳参数和最佳分数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n随机搜索最佳参数:&quot;</span>, random_search.best_params_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;随机搜索最佳分数:&quot;</span>, random_search.best_score_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用最佳参数的模型进行预测</span></span><br><span class="line">best_clf_random = random_search.best_estimator_</span><br><span class="line">y_pred_tuned_random = best_clf_random.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;调优后模型在测试集上的准确率: <span class="subst">&#123;accuracy_score(y_test, y_pred_tuned_random):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>在实际应用中，通常会先用随机搜索在大范围参数空间中找到一个较优的区域，然后再用网格搜索在该区域进行更精细的调优。此外，还有更高级的优化算法如贝叶斯优化（Bayesian Optimization）可以更高效地进行超参数搜索。</p>
<h2 id="特征重要性">特征重要性</h2>
<p>随机森林的一个非常实用的特性是它能够计算出数据集中每个特征的重要性。这对于理解模型、进行特征选择以及发现数据中的潜在模式非常有价值。</p>
<h3 id="如何计算？">如何计算？</h3>
<p>Scikit-learn中的随机森林通过两种主要方式计算特征重要性：</p>
<ol>
<li>
<p><strong>基于基尼不纯度减少的平均值 (Mean Decrease in Impurity, MDI / Gini Importance)：</strong></p>
<ul>
<li>这是Scikit-learn默认的特征重要性计算方法，通过<code>feature_importances_</code>属性获取。</li>
<li>在构建每棵决策树时，当一个特征用于分裂节点时，它会导致该节点的不纯度（如基尼不纯度或熵）减少。这个减少量就是该特征的重要性贡献。</li>
<li>对于森林中的所有树，某个特征在所有节点上的不纯度减少量的总和，经过标准化后，就得到了该特征的重要性分数。</li>
<li><strong>优点：</strong> 计算速度快，在训练模型时同时得到。</li>
<li><strong>缺点：</strong> 倾向于偏向于具有更多类别或数值特征的特征（因为它们有更多机会被选择），并且可能会高估共线性特征的重要性。</li>
</ul>
</li>
<li>
<p><strong>置换重要性 (Permutation Importance)：</strong></p>
<ul>
<li>这是一种模型无关的特征重要性计算方法，通过<code>eli5</code>或Scikit-learn的<code>sklearn.inspection.permutation_importance</code>模块实现。</li>
<li><strong>原理：</strong> 对于一个训练好的模型，计算其在验证集上的基准性能（例如准确率或R平方）。然后，对于每个特征，随机打乱（置换）该特征在验证集中的值，并重新计算模型的性能。性能下降的幅度越大，说明该特征越重要。</li>
<li><strong>优点：</strong> 更可靠地反映了特征对模型预测能力的贡献，不会偏向于特定类型的特征，并且能处理共线性问题。</li>
<li><strong>缺点：</strong> 计算成本较高，需要多次预测。</li>
</ul>
</li>
</ol>
<h3 id="实际应用">实际应用</h3>
<p>特征重要性在实际项目中扮演着关键角色：</p>
<ul>
<li><strong>特征选择：</strong> 可以去除那些重要性得分非常低的特征，从而简化模型、减少训练时间、提高模型的可解释性，甚至有时能提升性能（通过消除噪声特征）。</li>
<li><strong>模型解释：</strong> 帮助我们理解哪些特征对模型的决策影响最大，从而获得对业务问题的洞察。例如，在房价预测中，哪个因素（面积、位置、卧室数）对房价影响最大。</li>
<li><strong>数据清洗和特征工程：</strong> 如果某个重要的特征缺失值过多或存在质量问题，我们可以优先处理它。同时，重要特征的组合或变换可能产生新的有效特征。</li>
</ul>
<h3 id="代码示例-置换重要性">代码示例 (置换重要性)</h3>
<p>虽然上面分类和回归示例中已经展示了基于Gini Importance的特征重要性，这里我们补充一个使用Scikit-learn的置换重要性的例子，这通常被认为更可靠。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.inspection <span class="keyword">import</span> permutation_importance</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们已经训练好了分类器 clf 和回归器 regressor</span></span><br><span class="line"><span class="comment"># 使用之前鸢尾花分类的 clf</span></span><br><span class="line"><span class="comment"># 使用之前加州房价回归的 regressor</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对分类模型计算置换重要性</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- 分类模型置换重要性 (鸢尾花) ---&quot;</span>)</span><br><span class="line">result_clf = permutation_importance(clf, X_test, y_test, n_repeats=<span class="number">10</span>, random_state=<span class="number">42</span>, n_jobs=-<span class="number">1</span>)</span><br><span class="line">sorted_idx_clf = result_clf.importances_mean.argsort()[::-<span class="number">1</span>] <span class="comment"># 降序排列</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征重要性（均值）:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> sorted_idx_clf:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;feature_names[i]&#125;</span>: <span class="subst">&#123;result_clf.importances_mean[i]:<span class="number">.4</span>f&#125;</span> +/- <span class="subst">&#123;result_clf.importances_std[i]:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化置换重要性</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.boxplot(result_clf.importances[sorted_idx_clf].T,</span><br><span class="line">            vert=<span class="literal">False</span>, labels=np.array(feature_names)[sorted_idx_clf])</span><br><span class="line">plt.title(<span class="string">&quot;分类模型置换特征重要性&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;重要性减少 (准确率)&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对回归模型计算置换重要性</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- 回归模型置换重要性 (加州房价) ---&quot;</span>)</span><br><span class="line">result_reg = permutation_importance(regressor, X_test, y_test, n_repeats=<span class="number">10</span>, random_state=<span class="number">42</span>, n_jobs=-<span class="number">1</span>)</span><br><span class="line">sorted_idx_reg = result_reg.importances_mean.argsort()[::-<span class="number">1</span>] <span class="comment"># 降序排列</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征重要性（均值）:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> sorted_idx_reg:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;housing.feature_names[i]&#125;</span>: <span class="subst">&#123;result_reg.importances_mean[i]:<span class="number">.4</span>f&#125;</span> +/- <span class="subst">&#123;result_reg.importances_std[i]:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化置换重要性</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.boxplot(result_reg.importances[sorted_idx_reg].T,</span><br><span class="line">            vert=<span class="literal">False</span>, labels=np.array(housing.feature_names)[sorted_idx_reg])</span><br><span class="line">plt.title(<span class="string">&quot;回归模型置换特征重要性&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;重要性减少 (MSE)&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>置换重要性通常能提供更稳定、更具说服力的特征重要性排名，尤其是在特征之间存在共线性时。通过观察重要性均值和标准差，我们可以更好地理解特征的稳定性。</p>
<h2 id="与其他算法的比较">与其他算法的比较</h2>
<p>随机森林在机器学习领域占有重要地位，但在选择模型时，了解其与其他流行算法的异同至关重要。</p>
<h3 id="与梯度提升决策树-GBDT">与梯度提升决策树 (GBDT)</h3>
<ul>
<li><strong>集成方式：</strong>
<ul>
<li><strong>随机森林 (RF)：</strong> 属于Bagging类集成，基学习器（决策树）是独立并行训练的，通过样本和特征随机性来降低方差。</li>
<li><strong>GBDT (Gradient Boosting Decision Trees)：</strong> 属于Boosting类集成，基学习器是串行训练的，每棵树都试图纠正前一棵树的错误（通过拟合残差），主要目的是降低偏差。</li>
</ul>
</li>
<li><strong>树的特点：</strong>
<ul>
<li><strong>RF：</strong> 每棵树通常会生长得很深，甚至不剪枝，因为最终通过集成可以抵消过拟合。</li>
<li><strong>GBDT：</strong> 基学习器通常是浅层决策树（弱学习器），因为它需要更关注错误而不是完全拟合数据。</li>
</ul>
</li>
<li><strong>训练速度：</strong>
<ul>
<li><strong>RF：</strong> 由于并行训练，速度相对较快。</li>
<li><strong>GBDT：</strong> 串行训练，速度相对较慢。</li>
</ul>
</li>
<li><strong>鲁棒性：</strong>
<ul>
<li><strong>RF：</strong> 对噪声和异常值更鲁棒，因为每棵树只看到部分数据。</li>
<li><strong>GBDT：</strong> 对噪声和异常值更敏感，因为后续的树会努力纠正被错误预测的异常值，可能导致过拟合。</li>
</ul>
</li>
<li><strong>性能：</strong> GBDT（及其变体如XGBoost、LightGBM、CatBoost）在许多竞赛和实际任务中表现出比随机森林更高的准确率，尤其是在处理结构化数据时。然而，GBDT通常更难调参，且更容易过拟合。</li>
</ul>
<p><strong>总结：</strong> 随机森林是一个“开箱即用”的强大模型，鲁棒性好，不易过拟合，训练速度快。GBDT及其变体通常能达到更高的准确率，但更复杂，对参数和数据更敏感。</p>
<h3 id="与支持向量机-SVM">与支持向量机 (SVM)</h3>
<ul>
<li><strong>原理：</strong>
<ul>
<li><strong>RF：</strong> 基于决策树的集成。</li>
<li><strong>SVM：</strong> 基于最大间隔分类器，在高维空间中找到一个最优超平面来分离数据。</li>
</ul>
</li>
<li><strong>数据类型：</strong>
<ul>
<li><strong>RF：</strong> 可以直接处理混合数据类型，无需特征缩放。</li>
<li><strong>SVM：</strong> 对特征缩放敏感，通常需要进行归一化。对非线性关系需要使用核函数。</li>
</ul>
</li>
<li><strong>解释性：</strong>
<ul>
<li><strong>RF：</strong> 可以提供特征重要性，但整体决策过程不如单棵树直观。</li>
<li><strong>SVM：</strong> 线性SVM可解释性较强，核SVM解释性较差。</li>
</ul>
</li>
<li><strong>性能：</strong> 在许多分类任务中，两者都能达到很高的准确率。SVM在小样本、高维度数据上表现可能优异，而随机森林在大型、复杂数据集上往往更占优势。</li>
</ul>
<h3 id="与逻辑回归-Logistic-Regression">与逻辑回归 (Logistic Regression)</h3>
<ul>
<li><strong>原理：</strong>
<ul>
<li><strong>RF：</strong> 非线性、非参数模型。</li>
<li><strong>逻辑回归：</strong> 线性、参数模型，基于广义线性模型的分类算法。</li>
</ul>
</li>
<li><strong>数据关系：</strong>
<ul>
<li><strong>RF：</strong> 能捕捉复杂的非线性关系。</li>
<li><strong>逻辑回归：</strong> 假设特征与对数几率之间存在线性关系。</li>
</ul>
</li>
<li><strong>解释性：</strong>
<ul>
<li><strong>RF：</strong> 整体解释性相对较低。</li>
<li><strong>逻辑回归：</strong> 模型参数可以直接解释特征对概率的影响，解释性非常好。</li>
</ul>
</li>
<li><strong>性能：</strong> 在线性可分或近似线性可分的数据上，逻辑回归可能表现良好且高效。但当数据具有复杂非线性模式时，随机森林通常会显著优于逻辑回归。</li>
</ul>
<p><strong>总结：</strong> 随机森林是一个非常通用的“黑盒”模型，能够处理各种数据类型和复杂的非线性关系，通常是首选的基线模型之一。而线性模型（如逻辑回归、线性回归）在解释性、速度和简单线性关系建模方面有优势。</p>
<h2 id="结论">结论</h2>
<p>随机森林，作为Bagging集成学习策略的杰出代表，无疑是机器学习工具箱中一颗璀璨的明星。它通过集成大量的独立且随机化的决策树，巧妙地结合了决策树的易用性和集成学习的强大能力，有效地解决了单一决策树容易过拟合的顽疾，并显著提升了模型的泛化能力和鲁棒性。</p>
<p>我们探讨了随机森林的基石——决策树的构建原理及其优缺点，理解了集成学习中Bagging与Boosting的区别以及偏差-方差权衡的重要性。随后，我们深入剖析了随机森林的核心机制：自助采样和特征随机性，正是这两层随机性的巧妙结合，赋予了随机森林强大的性能和对各种数据的适应能力。</p>
<p>无论是分类任务（通过多数投票）还是回归任务（通过平均值），随机森林都能提供高准确率的预测，并且能够处理缺失值、高维数据，对噪声具有天然的抵抗力。此外，其内置的特征重要性评估机制，为我们理解数据和进行特征选择提供了宝贵的洞察。通过参数调优，如<code>n_estimators</code>、<code>max_features</code>等，我们可以进一步优化模型的性能。</p>
<p>当然，随机森林也并非万能，它的黑盒特性使其解释性不如单一决策树直观，且在计算资源和预测速度上相较于一些轻量级模型可能存在劣势。然而，在大多数实际应用场景中，随机森林都是一个非常可靠、性能优异的首选算法。</p>
<p>希望这篇文章能帮助你对随机森林有了一个全面而深入的理解。理论的学习固然重要，但实践才是检验真理的唯一标准。我鼓励你下载一些真实数据集，亲手运用随机森林解决分类或回归问题，感受它带来的强大力量。从参数调优到特征重要性分析，每一步实践都会让你对这个算法的理解更上一层楼。</p>
<p>数据科学的旅途充满挑战，也充满乐趣。保持好奇，不断探索，我们下次再见！</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://github.com/qmwneb946">qmwneb946</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://qmwneb946.dpdns.org/2025/07/22/2025-07-22-112646/">https://qmwneb946.dpdns.org/2025/07/22/2025-07-22-112646/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://opensource.org/licenses/MIT" target="_blank">MIT License</a> 许可协议。转载请注明来源 <a href="https://qmwneb946.dpdns.org" target="_blank">qmwneb946 的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A7%91%E6%8A%80%E5%89%8D%E6%B2%BF/">科技前沿</a><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%9C%A8%E5%88%86%E7%B1%BB%E4%B8%8E%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/">随机森林在分类与回归中的应用</a></div><div class="post-share"><div class="social-share" data-image="/img/icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/07/22/2025-07-22-113954/" title="金融科技中的监管科技（RegTech）：构建安全、高效的数字金融未来"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">金融科技中的监管科技（RegTech）：构建安全、高效的数字金融未来</div></div><div class="info-2"><div class="info-item-1">嘿，各位技术与数学爱好者们！我是你们的老朋友 qmwneb946，今天我们来聊一个既充满技术挑战又蕴含巨大社会价值的领域——金融科技（FinTech）中的监管科技（RegTech）。 在过去的十年里，金融行业经历了一场前所未有的数字化革命。从移动支付到数字货币，从P2P借贷到智能投顾，金融科技以其颠覆性的力量，极大地提升了金融服务的效率、可及性，并降低了成本。然而，硬币的另一面是，这些创新也带来了前所未有的风险和监管挑战。洗钱、欺诈、数据泄露、市场操纵……这些问题如影随形，对金融稳定、消费者保护乃至国家安全构成了严重威胁。 正是在这样的背景下，监管科技（RegTech）应运而生，并迅速发展成为一个炙手可热的领域。简单来说，RegTech就是利用现代科技，特别是大数据、人工智能、区块链、云计算等前沿技术，来优化、自动化和简化合规与监管流程。它不是简单地将传统合规流程数字化，而是旨在从根本上改变金融机构应对监管要求的方式，使其变得更高效、更精准、更具前瞻性。 今天，我们将深入剖析RegTech的本质、其背后的技术驱动力、在金融领域的具体应用，以及它所面临的挑战和光明前景。这不仅是一场...</div></div></div></a><a class="pagination-related" href="/2025/07/22/2025-07-22-112531/" title="探索组合优化的迷宫：启发式算法的智慧与实践"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">探索组合优化的迷宫：启发式算法的智慧与实践</div></div><div class="info-2"><div class="info-item-1">你好，各位技术同仁与数学爱好者！我是你们的老朋友 qmwneb946。 在我们的数字世界中，有无数的问题需要我们做出“最佳”决策：物流公司如何规划最短路径以节省燃料？生产线如何安排任务以最大化吞吐量？投资组合经理如何选择资产以平衡风险和收益？这些看似简单的问题背后，往往隐藏着一个巨大的挑战——组合优化。 引言：在复杂性与效率之间寻找平衡 组合优化是运筹学和计算机科学中的一个核心领域，它研究如何在给定约束条件下，从一个有限（但通常巨大）的离散选择集合中找到一个最优解。这些问题通常涉及对离散变量进行选择、排序或分配，以优化某个目标函数。例如，旅行商问题 (TSP) 寻找访问一系列城市并返回起点的最短路径；背包问题 (Knapsack Problem) 试图在容量限制下最大化装入物品的总价值；调度问题则旨在高效分配资源以完成任务。 然而，组合优化问题的“迷人”之处，也正是其“棘手”所在。许多实际的组合优化问题属于 NP-hard 范畴。这意味着，在最坏情况下，找到它们的精确最优解所需的计算时间会随问题规模的增长呈指数级爆炸式增长。即使是现代最强大的计算机，也可能在处理中等规模的 NP-...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/07/18/2025-07-18-082408/" title="人工智能在医疗诊断中的应用：机遇与挑战"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">人工智能在医疗诊断中的应用：机遇与挑战</div></div><div class="info-2"><div class="info-item-1">大家好，我是你们的技术和数学博主！今天，我们来深入探讨一个激动人心的领域：人工智能 (AI) 在医疗诊断中的应用。AI 的快速发展正在彻底改变医疗行业，为更精准、高效的诊断提供了前所未有的可能性。但同时，我们也需要审慎地看待其挑战和局限性。 引言：AI 赋能医疗诊断 医疗诊断是一个复杂的过程，需要医生具备丰富的知识、经验和判断力。然而，人类医生可能会受到主观偏差、疲劳以及信息过载的影响。AI 的介入，则为提高诊断准确性和效率提供了新的途径。通过分析大量的医学影像数据、病历记录和基因组信息，AI 算法可以学习识别疾病模式，辅助医生进行诊断，甚至在某些情况下独立完成初步诊断。 AI 在医疗诊断中的核心技术 深度学习在医学影像分析中的应用 深度学习，特别是卷积神经网络 (CNN)，在医学影像分析中取得了显著的成功。CNN 可以从大量的医学影像数据（例如 X 光片、CT 扫描、MRI 图像）中学习特征，并识别出细微的病变，例如肺癌结节、脑瘤或心血管疾病。 例如，一个训练良好的 CNN 模型可以比人类放射科医生更早地检测出肺癌，从而提高早期诊断率和治疗成功率。  这其中的关键在于大量的标注...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082643/" title="高分子化学与可降解塑料：迈向可持续未来的关键"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">高分子化学与可降解塑料：迈向可持续未来的关键</div></div><div class="info-2"><div class="info-item-1">近年来，塑料污染已成为全球性环境问题。传统塑料由于其难以降解的特性，对环境造成了巨大的压力。而可降解塑料的出现，为解决这一问题提供了一条可行的途径。本文将深入探讨高分子化学在可降解塑料研发中的关键作用，并介绍几种主要的降解机制和材料。 高分子化学：可降解塑料的基础 可降解塑料并非简单的“可被分解的塑料”，其核心在于高分子材料的分子结构设计。高分子化学为我们提供了理解和操纵聚合物结构的工具，从而设计出具有特定降解性能的材料。传统塑料通常由难以断裂的强共价键连接而成，而可降解塑料则通过引入特定的化学键或结构单元，使其在特定条件下能够断裂，从而实现降解。  这需要对聚合物的合成方法、分子量分布、链结构以及结晶度等进行精细的控制。 常见的可降解塑料聚合物 目前，市场上常见的可降解塑料主要包括以下几种：   聚乳酸 (PLA):  PLA 是一种生物基聚合物，由可再生资源（例如玉米淀粉）制成。其降解过程主要依靠水解反应，在特定条件下（例如堆肥环境）可以被微生物降解。PLA 的机械性能较好，但耐热性相对较差。   聚羟基脂肪酸酯 (PHAs): PHAs 是一类由微生物合成的聚酯。它们具有良...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082805/" title="电化学储能技术的新进展：迈向更清洁、更持久的能源未来"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">电化学储能技术的新进展：迈向更清洁、更持久的能源未来</div></div><div class="info-2"><div class="info-item-1">电化学储能技术作为解决可再生能源间歇性问题的关键技术，近年来取得了显著进展。从电动汽车到智能电网，电化学储能系统正深刻地改变着我们的生活。本文将深入探讨电化学储能技术的最新突破，涵盖不同类型的储能技术及其面临的挑战与机遇。 电化学储能技术的类型 目前，市场上主要的电化学储能技术包括： 锂离子电池 锂离子电池凭借其高能量密度、长循环寿命和相对较低的成本，占据了当前电化学储能市场的主导地位。然而，锂资源的有限性和安全性问题仍然是制约其发展的瓶颈。  近年来，研究者们致力于开发高能量密度锂离子电池，例如：  固态锂电池:  固态电解质的采用可以显著提高电池的安全性，并有望实现更高的能量密度。然而，固态电解质的离子电导率和界面接触仍然是需要克服的挑战。 锂硫电池:  锂硫电池具有极高的理论能量密度，但其循环寿命和硫的穿梭效应仍然是需要解决的关键问题。  研究者们正在探索各种改性策略来提高锂硫电池的性能。 锂空气电池:  锂空气电池拥有理论上最高的能量密度，但其反应动力学缓慢，副反应多，循环寿命短等问题限制了其商业化应用。  钠离子电池 作为锂离子的潜在替代品，钠离子电池具有成本低、资源丰...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-092352/" title="材料科学与新型半导体材料：摩尔定律的未来"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">材料科学与新型半导体材料：摩尔定律的未来</div></div><div class="info-2"><div class="info-item-1">引言 摩尔定律，即集成电路上的晶体管数量每隔两年翻一番，几十年来一直驱动着信息技术产业的飞速发展。然而，随着晶体管尺寸逼近物理极限，摩尔定律的持续性受到了挑战。为了维持这种指数级增长，我们需要探索新型半导体材料，突破硅基技术的瓶颈。本文将深入探讨材料科学在新型半导体材料研发中的关键作用，并介绍一些具有前景的候选材料。 新型半导体材料的需求 硅作为半导体材料的主力，其优势在于成本低、工艺成熟。但其固有的物理特性限制了其在更高频率、更高功率和更低功耗方面的性能提升。例如，硅的载流子迁移率相对较低，导致能量损耗增加，尤其是在高频应用中。因此，我们需要寻找具有更高载流子迁移率、更宽禁带宽度、更高饱和电子漂移速度等优异特性的材料。 性能瓶颈及解决方案 硅基技术的性能瓶颈主要体现在以下几个方面：  漏电流:  随着晶体管尺寸的缩小，漏电流问题日益严重，导致功耗增加和性能下降。 热耗散: 高频运行会导致晶体管产生大量热量，影响器件稳定性和可靠性。 开关速度: 硅的载流子迁移率限制了晶体管的开关速度，限制了处理器的运行频率。  为了解决这些问题，研究人员正在积极探索各种新型半导体材料，例如：  ...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-092411/" title="弦理论中的额外维度探索：超越我们感知的宇宙"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">弦理论中的额外维度探索：超越我们感知的宇宙</div></div><div class="info-2"><div class="info-item-1">引言 我们生活在一个看似三维的空间中，加上时间构成四维时空。然而，弦理论，这个试图统一所有基本力的优雅理论，却预言了额外维度的存在。这些额外维度并非我们日常经验所能感知，它们蜷缩在比原子尺度还要小得多的空间里。本文将深入探讨弦理论中额外维度的概念，并解释科学家们如何尝试探测这些隐藏的宇宙维度。 弦理论与额外维度：一个必要的假设 弦理论的核心思想是将基本粒子视为微小的振动弦，不同振动模式对应不同的粒子。为了使理论自洽，并消除量子场论中的一些困扰，弦理论需要引入额外空间维度。最初的弦理论版本需要 26 个维度，而超弦理论则将维度数量缩减到 10 个（或 11 个，在 M 理论中）。这多出来的 6 个（或 7 个）维度是如何隐藏起来的呢？ 卡拉比-丘空间：卷曲的维度 弦理论提出，额外维度并非不存在，而是以紧致化的形式存在，就像一根细细的管子卷曲得非常紧密，以至于在宏观尺度上无法被察觉。这些紧致化的额外维度通常被描述为卡拉比-丘空间，这是一类复杂的六维流形，具有独特的几何性质。卡拉比-丘空间的形状和大小直接影响了我们观察到的粒子物理学特性，例如粒子质量和相互作用强度。 R6R^6R6 表...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-092451/" title="粒子物理学的标准模型之外：探索宇宙未解之谜"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">粒子物理学的标准模型之外：探索宇宙未解之谜</div></div><div class="info-2"><div class="info-item-1">我们生活在一个由基本粒子及其相互作用组成的宇宙中。粒子物理学的标准模型，如同一个精妙的乐章，成功地描述了已知的基本粒子及其三种基本作用力（电磁力、弱力和强力），并准确预测了许多实验结果。然而，这个模型并非完美无缺，它留下了许多未解之谜，指引着我们向标准模型之外的更广阔领域探索。 标准模型的局限性 标准模型尽管取得了巨大的成功，但它并不能解释宇宙中的一切现象。一些关键的不足之处包括： 暗物质与暗能量 宇宙学观测表明，宇宙中存在大量的暗物质和暗能量，它们构成了宇宙质量能量的大部分，但标准模型中却无法解释它们的本质。暗物质不参与电磁相互作用，因此我们无法直接观测到它，只能通过其引力效应间接探测。暗能量则是一种神秘的能量形式，导致宇宙加速膨胀。它们的发现暗示着标准模型之外存在着新的物理学。 中微子质量 标准模型最初假设中微子是无质量的。然而，实验观测表明中微子具有微小的质量，这与标准模型的预言相矛盾。中微子的质量之谜需要新的物理机制来解释，例如 seesaw 机制。 质子衰变 标准模型预言质子是稳定的，然而，一些大统一理论（GUTs）预测质子会发生极其缓慢的衰变。虽然到目前为止还没有观测...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qmwneb946</div><div class="author-info-description">一个专注于技术分享的个人博客，涵盖编程、算法、系统设计等内容</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">588</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">592</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qmwneb946"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qmwneb946" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:qmwneb946@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">代码与远方，技术与生活交织的篇章。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%9A%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%9A%84%E5%9F%BA%E7%9F%B3"><span class="toc-number">1.</span> <span class="toc-text">决策树：随机森林的基石</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%9F"><span class="toc-number">1.1.</span> <span class="toc-text">什么是决策树？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E6%9E%84%E5%BB%BA"><span class="toc-number">1.2.</span> <span class="toc-text">决策树的构建</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A-Information-Gain"><span class="toc-number">1.2.1.</span> <span class="toc-text">信息增益 (Information Gain)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E5%B0%BC%E4%B8%8D%E7%BA%AF%E5%BA%A6-Gini-Impurity"><span class="toc-number">1.2.2.</span> <span class="toc-text">基尼不纯度 (Gini Impurity)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%AA%E6%9E%9D-Pruning"><span class="toc-number">1.2.3.</span> <span class="toc-text">剪枝 (Pruning)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">1.3.</span> <span class="toc-text">决策树的优缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%9A%E7%BE%A4%E4%BD%93%E7%9A%84%E6%99%BA%E6%85%A7"><span class="toc-number">2.</span> <span class="toc-text">集成学习：群体的智慧</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%9F"><span class="toc-number">2.1.</span> <span class="toc-text">什么是集成学习？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bagging-Bootstrap-Aggregating"><span class="toc-number">2.2.</span> <span class="toc-text">Bagging (Bootstrap Aggregating)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Boosting-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E7%AD%89"><span class="toc-number">2.3.</span> <span class="toc-text">Boosting (梯度提升等)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%9F%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E6%9D%83%E8%A1%A1"><span class="toc-number">2.4.</span> <span class="toc-text">为什么要集成学习？偏差-方差权衡</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%EF%BC%9ABagging%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%80%A7%E7%9A%84%E5%AE%8C%E7%BE%8E%E7%BB%93%E5%90%88"><span class="toc-number">3.</span> <span class="toc-text">随机森林：Bagging与随机性的完美结合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B5%B7%E6%BA%90%E4%B8%8E%E5%AE%9A%E4%B9%89"><span class="toc-number">3.1.</span> <span class="toc-text">起源与定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-number">3.2.</span> <span class="toc-text">核心思想</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Bagging-%E8%87%AA%E5%8A%A9%E9%87%87%E6%A0%B7"><span class="toc-number">3.2.1.</span> <span class="toc-text">Bagging (自助采样)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E9%9A%8F%E6%9C%BA%E6%80%A7-Feature-Randomness"><span class="toc-number">3.2.2.</span> <span class="toc-text">特征随机性 (Feature Randomness)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="toc-number">3.3.</span> <span class="toc-text">算法流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="toc-number">3.4.</span> <span class="toc-text">随机森林的优势</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">3.5.</span> <span class="toc-text">随机森林的局限性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%9C%A8%E5%88%86%E7%B1%BB%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">4.</span> <span class="toc-text">随机森林在分类中的应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86%EF%BC%9A%E5%A4%9A%E6%95%B0%E6%8A%95%E7%A5%A8"><span class="toc-number">4.1.</span> <span class="toc-text">原理：多数投票</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-number">4.2.</span> <span class="toc-text">评估指标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%9A%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB"><span class="toc-number">4.3.</span> <span class="toc-text">示例：鸢尾花分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%9C%A8%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">5.</span> <span class="toc-text">随机森林在回归中的应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86%EF%BC%9A%E5%B9%B3%E5%9D%87%E5%80%BC"><span class="toc-number">5.1.</span> <span class="toc-text">原理：平均值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87-2"><span class="toc-number">5.2.</span> <span class="toc-text">评估指标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%9A%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B-%E7%AE%80%E5%8C%96%E7%89%88"><span class="toc-number">5.3.</span> <span class="toc-text">示例：房价预测 (简化版)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%9A%84%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98"><span class="toc-number">6.</span> <span class="toc-text">随机森林的参数调优</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%8F%82%E6%95%B0"><span class="toc-number">6.1.</span> <span class="toc-text">核心参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0"><span class="toc-number">6.2.</span> <span class="toc-text">其他重要参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%E7%AD%96%E7%95%A5"><span class="toc-number">6.3.</span> <span class="toc-text">参数调优策略</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81-Cross-validation"><span class="toc-number">6.3.1.</span> <span class="toc-text">交叉验证 (Cross-validation)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2-Grid-Search"><span class="toc-number">6.3.2.</span> <span class="toc-text">网格搜索 (Grid Search)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%90%9C%E7%B4%A2-Random-Search"><span class="toc-number">6.3.3.</span> <span class="toc-text">随机搜索 (Random Search)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-number">7.</span> <span class="toc-text">特征重要性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%EF%BC%9F"><span class="toc-number">7.1.</span> <span class="toc-text">如何计算？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8"><span class="toc-number">7.2.</span> <span class="toc-text">实际应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-%E7%BD%AE%E6%8D%A2%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-number">7.3.</span> <span class="toc-text">代码示例 (置换重要性)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8E%E5%85%B6%E4%BB%96%E7%AE%97%E6%B3%95%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">8.</span> <span class="toc-text">与其他算法的比较</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%86%B3%E7%AD%96%E6%A0%91-GBDT"><span class="toc-number">8.1.</span> <span class="toc-text">与梯度提升决策树 (GBDT)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-SVM"><span class="toc-number">8.2.</span> <span class="toc-text">与支持向量机 (SVM)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-Logistic-Regression"><span class="toc-number">8.3.</span> <span class="toc-text">与逻辑回归 (Logistic Regression)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">9.</span> <span class="toc-text">结论</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/22/hello-world/" title="Hello World">Hello World</a><time datetime="2025-07-22T22:25:10.320Z" title="发表于 2025-07-23 06:25:10">2025-07-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/22/%E5%8D%9A%E5%BC%88%E8%AE%BA%E5%9F%BA%E7%A1%80/" title="博弈论基础">博弈论基础</a><time datetime="2025-07-22T22:25:10.320Z" title="发表于 2025-07-23 06:25:10">2025-07-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/22/2025-07-22-222303/" title="深度剖析：文本风格迁移技术的奥秘与前沿">深度剖析：文本风格迁移技术的奥秘与前沿</a><time datetime="2025-07-22T14:23:03.000Z" title="发表于 2025-07-22 22:23:03">2025-07-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/22/2025-07-22-222136/" title="永不止步的探索：持续学习与灾难性遗忘的挑战与机遇">永不止步的探索：持续学习与灾难性遗忘的挑战与机遇</a><time datetime="2025-07-22T14:21:36.000Z" title="发表于 2025-07-22 22:21:36">2025-07-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/22/2025-07-22-222027/" title="微内核与宏内核之争：深入理解操作系统架构的权衡与选择">微内核与宏内核之争：深入理解操作系统架构的权衡与选择</a><time datetime="2025-07-22T14:20:27.000Z" title="发表于 2025-07-22 22:20:27">2025-07-22</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2025 By qmwneb946</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.2</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'qmwneb946/blog',
      'data-repo-id': 'R_kgDOPM6cWw',
      'data-category-id': 'DIC_kwDOPM6cW84CtEzo',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme)
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Giscus' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Giscus' === 'Giscus' || !false) {
    if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>