<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>自然语言处理中的常识推理：从符号到大模型的演进与挑战 | qmwneb946 的博客</title><meta name="author" content="qmwneb946"><meta name="copyright" content="qmwneb946"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="亲爱的技术爱好者们，大家好！我是你们的老朋友 qmwneb946。 在人工智能，特别是自然语言处理 (NLP) 的浩瀚星辰中，我们不断追求让机器不仅能“听懂”人类语言，更能“理解”其深层含义。然而，这种理解的最高境界，往往并非复杂算法或海量数据所能完全企及——它藏在一个看似简单却又异常难以捉摸的概念里：常识 (Common Sense)。 想象一下，你对一个朋友说：“我把书放在了包里，它很重。”">
<meta property="og:type" content="article">
<meta property="og:title" content="自然语言处理中的常识推理：从符号到大模型的演进与挑战">
<meta property="og:url" content="https://qmwneb946.dpdns.org/2025/07/24/2025-07-25-065009/index.html">
<meta property="og:site_name" content="qmwneb946 的博客">
<meta property="og:description" content="亲爱的技术爱好者们，大家好！我是你们的老朋友 qmwneb946。 在人工智能，特别是自然语言处理 (NLP) 的浩瀚星辰中，我们不断追求让机器不仅能“听懂”人类语言，更能“理解”其深层含义。然而，这种理解的最高境界，往往并非复杂算法或海量数据所能完全企及——它藏在一个看似简单却又异常难以捉摸的概念里：常识 (Common Sense)。 想象一下，你对一个朋友说：“我把书放在了包里，它很重。”">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qmwneb946.dpdns.org/img/icon.png">
<meta property="article:published_time" content="2025-07-24T22:50:09.000Z">
<meta property="article:modified_time" content="2025-07-26T08:21:24.357Z">
<meta property="article:author" content="qmwneb946">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="计算机科学">
<meta property="article:tag" content="自然语言处理中的常识推理">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qmwneb946.dpdns.org/img/icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "自然语言处理中的常识推理：从符号到大模型的演进与挑战",
  "url": "https://qmwneb946.dpdns.org/2025/07/24/2025-07-25-065009/",
  "image": "https://qmwneb946.dpdns.org/img/icon.png",
  "datePublished": "2025-07-24T22:50:09.000Z",
  "dateModified": "2025-07-26T08:21:24.357Z",
  "author": [
    {
      "@type": "Person",
      "name": "qmwneb946",
      "url": "https://github.com/qmwneb946"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qmwneb946.dpdns.org/2025/07/24/2025-07-25-065009/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '自然语言处理中的常识推理：从符号到大模型的演进与挑战',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2845632165165414" crossorigin="anonymous"></script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="qmwneb946 的博客" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qmwneb946 的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">自然语言处理中的常识推理：从符号到大模型的演进与挑战</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">自然语言处理中的常识推理：从符号到大模型的演进与挑战<a class="post-edit-link" href="https://github.com/qmwneb946/blog/edit/main/source/_posts/2025-07-25-065009.md" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-24T22:50:09.000Z" title="发表于 2025-07-25 06:50:09">2025-07-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-26T08:21:24.357Z" title="更新于 2025-07-26 16:21:24">2025-07-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/">计算机科学</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><p>亲爱的技术爱好者们，大家好！我是你们的老朋友 qmwneb946。</p>
<p>在人工智能，特别是自然语言处理 (NLP) 的浩瀚星辰中，我们不断追求让机器不仅能“听懂”人类语言，更能“理解”其深层含义。然而，这种理解的最高境界，往往并非复杂算法或海量数据所能完全企及——它藏在一个看似简单却又异常难以捉摸的概念里：<strong>常识 (Common Sense)</strong>。</p>
<p>想象一下，你对一个朋友说：“我把书放在了包里，它很重。” 你朋友立刻明白，“它”指的是“书”，而不是“包”。如果我说：“我把包放在了书上，它很重。” 你的朋友也能迅速判断，“它”更可能指的是“包”。这种不假思索的、基于日常经验和世界知识的判断，就是常识推理。对于人类来说，这是我们认知系统的基石，是理解世界、做出决策、甚至进行幽默和讽刺的基础。但对于机器而言，这却是一道长期存在的、极其严峻的鸿沟。</p>
<p>今天，我将带大家深入探索自然语言处理中的常识推理这一迷人而充满挑战的领域。我们将一同审视这一难题的本质，追溯从早期符号主义尝试到如今大语言模型时代的技术演进，剖析主流的基准任务与模型方法，并展望未来的无限可能。准备好了吗？让我们一起踏上这场充满智慧与挑战的旅程！</p>
<h2 id="第一章：常识推理的本质与挑战">第一章：常识推理的本质与挑战</h2>
<h3 id="什么是常识？">什么是常识？</h3>
<p>在深入探讨机器如何获得常识之前，我们首先需要明确“常识”究竟是什么。常识并非一套严格的、形式化的逻辑规则，而是一种关于世界如何运作的广泛的、非形式化的、通常是隐含的知识体系。它包括：</p>
<ol>
<li><strong>物理常识 (Physical Common Sense)</strong>：关于物体、空间、时间、因果关系等物理世界的知识。例如：水往低处流；固体不能穿过固体；火能烧毁纸张。</li>
<li><strong>社会常识 (Social Common Sense)</strong>：关于人类行为、意图、情感、社会规范、伦理道德等社会层面的知识。例如：去医院是因为生病；排队是为了公平；送礼物表示友好。</li>
<li><strong>语言常识 (Linguistic Common Sense)</strong>：关于语言使用习惯、词语多义性、指代消解、隐含意义等语言层面的知识。例如：在“他拿起笔写字”中，“笔”是写字的工具；“感冒了就多喝热水”中的“热水”是安慰和建议。</li>
<li><strong>心理常识 (Psychological Common Sense)</strong>：关于人的信念、欲望、意图、计划等心智状态的知识。例如：一个人哭泣是因为悲伤或疼痛；一个人的行为有其动机。</li>
</ol>
<p>常识的这些特征决定了它的复杂性：</p>
<ul>
<li><strong>隐含性 (Implicit)</strong>：它通常不被明确表达，而是存在于我们的潜意识中，通过经验积累。</li>
<li><strong>海量性 (Vast)</strong>：常识知识的数量是无限的，无法通过简单的枚举来穷尽。</li>
<li><strong>动态性 (Dynamic)</strong>：常识会随着时间和文化背景的变化而演进。</li>
<li><strong>不确定性 (Uncertain)</strong>：常识并非总是绝对真理，它常常是基于概率或一般性规则的推断。</li>
</ul>
<h3 id="为什么机器难以掌握常识？">为什么机器难以掌握常识？</h3>
<p>机器，尤其是传统的基于规则或统计模型的机器，在处理常识方面面临着巨大的挑战。</p>
<ol>
<li><strong>知识获取的瓶颈</strong>：人类通过一生与世界的互动来学习常识。这包括感知、行动、社会互动等。机器缺乏这种具身性 (embodiment) 和真实世界的互动经验。试图人工编码所有常识是不可行的“AI完成度问题” (AI-completeness problem)，因为其规模过于庞大。</li>
<li><strong>符号接地问题 (Symbol Grounding Problem)</strong>：符号主义AI面临的核心问题之一。机器可以操作符号（例如“杯子”），但它们不理解这些符号在现实世界中对应的含义和属性（例如“杯子”可以用来喝水，通常是玻璃或陶瓷做的，如果掉在地上会碎）。它们没有与“杯子”相关的感官、物理和功能经验。</li>
<li><strong>情境依赖性 (Context Dependency)</strong>：常识推理往往高度依赖于特定的语境和情境。同一个词或概念在不同情境下可能有不同的含义或推论。例如，“打破”在“打破花瓶”和“打破记录”中含义截然不同。</li>
<li><strong>开放世界挑战 (Open-World Problem)</strong>：现实世界是开放的，机器在面对未曾见过的、新颖的情境时，需要能够灵活地运用常识进行推理，而不是简单地匹配预设的模式。</li>
<li><strong>推理的复杂性</strong>：常识推理常常是非单调的 (non-monotonic)，即新的信息可能会推翻先前的结论。例如，我们通常认为鸟会飞，但如果得知这是一只企鹅或一只受伤的鸟，我们的结论就会改变。这种灵活的、可撤销的推理是机器难以模拟的。</li>
</ol>
<h3 id="常识在NLP中的重要性">常识在NLP中的重要性</h3>
<p>尽管困难重重，常识推理却是实现真正通用人工智能 (AGI) 的关键一环，对NLP领域更是至关重要：</p>
<ol>
<li><strong>提高自然语言理解 (NLU) 的深度</strong>：没有常识，机器对语言的理解只能停留在表面，无法捕捉深层语义、隐含意图和言外之意。例如，情感分析、讽刺检测、指代消解等任务都严重依赖常识。</li>
<li><strong>增强自然语言生成 (NLG) 的合理性</strong>：一个有常识的生成模型可以生成更连贯、更符合逻辑、更贴近人类表达习惯的文本。避免生成荒谬、矛盾或不合常理的内容。例如，聊天机器人、故事生成、摘要生成等。</li>
<li><strong>实现更高级的推理任务</strong>：问答系统、知识图谱补全、事件预测、计划生成等需要对世界有深刻理解的任务，都离不开常识推理的支持。</li>
<li><strong>提高AI系统的鲁棒性和可信度</strong>：具备常识的AI系统在面对不确定或歧义信息时，能做出更合理、更符合预期的判断，从而提升其在现实世界应用中的可靠性。</li>
</ol>
<p>可以说，常识是赋予机器“智慧”的灵魂。它让机器从一个冰冷的计算器，逐渐向一个能够理解并适应复杂世界的智能体迈进。</p>
<h2 id="第二章：早期符号主义方法与知识库构建">第二章：早期符号主义方法与知识库构建</h2>
<p>在深度学习浪潮席卷全球之前，人工智能领域的主流范式是符号主义 (Symbolic AI)。符号主义认为智能的核心是基于符号表示和逻辑推理。在常识推理领域，早期的研究者们主要致力于通过人工编码或半自动构建大规模的常识知识库，并设计相应的推理机制。</p>
<h3 id="符号主义的理论基础">符号主义的理论基础</h3>
<p>符号主义的核心思想是将世界知识表示为离散的符号（如概念、属性、关系），并利用逻辑规则（如谓词逻辑、产生式规则）对这些符号进行操作，以实现推理。</p>
<ul>
<li><strong>知识表示 (Knowledge Representation)</strong>：通过语义网络 (Semantic Networks)、框架 (Frames)、逻辑公式 (Logical Formulas) 等形式来表示常识知识。例如，“鸟”是一个概念，“有翅膀”是鸟的一个属性，“会飞”是鸟的一个行为。</li>
<li><strong>推理机制 (Inference Mechanisms)</strong>：利用归纳、演绎、溯因等逻辑推理方法，从已知知识中推导出新知识。例如，如果“所有鸟都会飞”且“麻雀是鸟”，那么可以演绎出“麻雀会飞”。</li>
</ul>
<h3 id="人工构建的常识知识库">人工构建的常识知识库</h3>
<p>这些知识库是早期常识推理研究的基石，它们旨在捕获人类的常识知识。</p>
<h4 id="Cyc">Cyc</h4>
<p>Cyc 项目由道格拉斯·莱纳特 (Douglas Lenat) 于1984年启动，是人工智能领域最宏大、最具野心的项目之一。其目标是构建一个庞大的、通用的人类常识知识库，通过数百万条人工编码的断言 (assertions) 和规则来表示常识。</p>
<ul>
<li><strong>特点</strong>：Cyc 使用一种名为 CycL 的一阶逻辑变体作为知识表示语言。它包含实体、概念、关系、规则、情境 (contexts) 等，旨在捕获日常生活中最基本的概念和事实。例如，“当一个人在下雨天外出时，通常会打伞”这样的规则。</li>
<li><strong>挑战</strong>：Cyc 项目耗时极长，投入巨大，但其知识获取的“瓶颈”问题始终难以解决。人工编码知识效率低下，且难以覆盖常识的无限多样性。此外，它的推理过程是基于逻辑的，在处理不确定性、非单调性和模糊性常识时存在局限。</li>
</ul>
<h4 id="WordNet">WordNet</h4>
<p>WordNet 是一个大型的英语词汇数据库，由普林斯顿大学的认知科学实验室创建。它并非专门为常识推理设计，但其结构中包含了丰富的词汇语义关系，间接反映了部分常识。</p>
<ul>
<li><strong>特点</strong>：WordNet 将名词、动词、形容词和副词组织成同义词集 (synsets)，每个同义词集代表一个基本概念。这些同义词集之间通过词汇关系连接，如：
<ul>
<li><strong>上位词/下位词 (Hypernymy/Hyponymy)</strong>：is-a 关系，例如“狗”是“哺乳动物”的下位词。</li>
<li><strong>部分/整体 (Meronymy/Holonymy)</strong>：part-of 关系，例如“轮子”是“汽车”的一部分。</li>
<li><strong>反义词 (Antonymy)</strong>：例如“热”与“冷”。</li>
</ul>
</li>
<li><strong>应用</strong>：WordNet 在信息检索、词义消歧、文本分类等NLP任务中发挥了重要作用。通过遍历WordNet的层级结构，可以进行一定程度的常识推理，例如判断两个概念的相似性或包含关系。</li>
<li><strong>局限性</strong>：WordNet 主要关注词汇语义，缺乏对实体、事件和复杂因果关系的常识知识。它的结构是静态的，难以捕获动态的或情境依赖的常识。</li>
</ul>
<h4 id="ConceptNet">ConceptNet</h4>
<p>ConceptNet 是一个自由开放的语义网络，旨在捕获日常常识知识。它融合了来自各种来源（如众包、词典、特定领域知识库）的数据，并以图的形式表示知识。</p>
<ul>
<li><strong>特点</strong>：ConceptNet 使用边来表示概念之间的关系，节点表示概念。这些关系包括：
<ul>
<li><strong>IsA</strong>：是A kind of B，例如“(猫) IsA (动物)”。</li>
<li><strong>CapableOf</strong>：能够做什么，例如“(刀) CapableOf (切割)”。</li>
<li><strong>UsedFor</strong>：用于做什么，例如“(椅子) UsedFor (坐)”。</li>
<li><strong>HasProperty</strong>：有什么属性，例如“(冰) HasProperty (冷)”。</li>
<li><strong>AtLocation</strong>：在哪里，例如“(鱼) AtLocation (水)”。</li>
</ul>
</li>
<li><strong>构建方式</strong>：ConceptNet 的数据主要来源于众包项目（如 Games With A Purpose 的 Verbosity 游戏），以及其他结构化资源。</li>
<li><strong>优势</strong>：相较于Cyc，ConceptNet 的构建成本更低，更新更灵活。其图结构更易于与图神经网络 (GNNs) 等现代模型结合。</li>
<li><strong>局限性</strong>：尽管比Cyc更灵活，但其知识的粒度、覆盖范围和一致性仍有待提高。众包数据可能包含噪音和不准确的信息。</li>
</ul>
<h4 id="Freebase-Wikidata">Freebase / Wikidata</h4>
<p>Freebase (现已归并到 Google 的 Knowledge Graph 中) 和 Wikidata 是大规模的通用知识图谱，虽然它们主要存储事实性知识（例如，“埃菲尔铁塔位于巴黎”，“某某是某某的导演”），但这些事实性知识构成了理解世界的基础，间接为常识推理提供了支持。它们以 (实体，关系，实体) 的三元组形式表示知识。</p>
<h3 id="基于规则和逻辑的推理系统">基于规则和逻辑的推理系统</h3>
<p>在拥有了知识库之后，研究者们也开发了各种基于规则和逻辑的推理系统来利用这些知识进行常识推理。</p>
<ul>
<li><strong>产生式系统 (Production Systems)</strong>：由一组“如果-那么”规则组成，例如“如果 (天气下雨) 那么 (带伞)”。</li>
<li><strong>描述逻辑 (Description Logics)</strong>：用于表示概念和角色（关系）的逻辑形式，支持分类、推理实例等。</li>
<li><strong>常识推理引擎</strong>：利用这些逻辑形式进行形式化推理，如判断一致性、推导隐含关系等。</li>
</ul>
<h3 id="符号方法的局限性">符号方法的局限性</h3>
<p>尽管符号主义方法在AI早期取得了重要进展，但在常识推理方面，它们面临着根本性的局限：</p>
<ol>
<li><strong>知识获取瓶颈</strong>：这是最核心的问题。无论人工编码还是半自动抽取，都难以穷尽并维护海量、动态的常识知识。</li>
<li><strong>符号接地问题</strong>：如前所述，符号本身没有内在含义，它们与现实世界的联系薄弱。</li>
<li><strong>脆弱性 (Brittleness)</strong>：基于规则的系统在面对未曾预见的、边缘的情况时，往往表现出极度的脆弱性，无法泛化。</li>
<li><strong>不确定性与模糊性</strong>：常识并非都是确定性的，常常包含不确定和模糊的成分。符号逻辑在处理这些方面表现不佳。</li>
<li><strong>缺乏学习能力</strong>：传统的符号系统缺乏从经验中学习和改进的能力，需要人工不断调整和维护。</li>
</ol>
<p>这些局限性促使AI研究者们转向了数据驱动、学习范式的新方法，尤其是深度学习的崛起，为常识推理带来了新的曙光。</p>
<h2 id="第三章：数据驱动的深度学习范式">第三章：数据驱动的深度学习范式</h2>
<p>进入21世纪，特别是2010年以后，随着计算能力的提升、大数据集的涌现以及新算法（如深度神经网络）的突破，人工智能领域的研究范式发生了根本性转变：从传统的符号主义转向了数据驱动的深度学习。在NLP领域，这一转变尤为显著。</p>
<h3 id="从特征工程到端到端学习">从特征工程到端到端学习</h3>
<p>在深度学习之前，NLP模型通常依赖于人工设计的特征（如词性标注、句法分析树、N-gram等）和浅层机器学习模型（如支持向量机、条件随机场）。这种方法费时费力，且难以捕获语言的深层语义。</p>
<p>深度学习则实现了端到端 (End-to-End) 的学习：直接从原始文本数据中学习特征表示和任务映射，大大简化了开发流程，并显著提升了模型性能。</p>
<h3 id="预训练语言模型（PLMs）的崛起">预训练语言模型（PLMs）的崛起</h3>
<p>2018年，随着BERT、GPT等预训练语言模型 (Pre-trained Language Models, PLMs) 的横空出世，NLP领域迎来了“预训练+微调” (Pre-train &amp; Fine-tune) 的新范式。</p>
<ul>
<li>
<p><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>：由Google提出，采用 Transformer 编码器架构。通过两个任务进行预训练：</p>
<ol>
<li><strong>掩码语言模型 (Masked Language Model, MLM)</strong>：随机遮蔽输入序列中的一部分词，然后预测这些被遮蔽的词。这使得模型能够学习上下文相关的双向语言表示。例如，在“我爱 [MASK] 国”中，模型需要根据上下文预测“中”。</li>
<li><strong>下一句预测 (Next Sentence Prediction, NSP)</strong>：判断两个句子是否在原始文本中是连续的。这有助于模型理解句子之间的关系。<br>
通过这两个任务，BERT 在海量无标注文本数据上进行了预训练，学习了丰富的语言知识。</li>
</ol>
</li>
<li>
<p><strong>GPT (Generative Pre-trained Transformer) 系列</strong>：由OpenAI提出，采用 Transformer 解码器架构。</p>
<ol>
<li><strong>自回归生成 (Auto-regressive Generation)</strong>：模型根据前文预测下一个词，这种单向的预测方式使其特别适合生成任务。</li>
<li>早期的GPT模型（如GPT-1、GPT-2）展示了强大的文本生成能力。随着模型规模的不断扩大，特别是GPT-3、GPT-4，其在零样本 (Zero-shot) 和少样本 (Few-shot) 学习上的能力令人惊叹。</li>
</ol>
</li>
</ul>
<p><strong>核心思想：通过大规模语料库学习“世界知识”</strong><br>
PLMs 的成功在于它们在天文数字般的文本数据（如Common Crawl、Wikipedia、BooksCorpus等）上进行预训练。这些语料库包含了人类社会积累的海量知识和语言使用模式。PLMs 通过优化预训练任务的目标函数，被迫“内化”了这些知识，包括：</p>
<ol>
<li><strong>句法结构</strong>：学习了不同语言的语法规则和句子结构。</li>
<li><strong>语义信息</strong>：学习了词语的含义、词与词之间的关系（同义、反义、上下位等）。</li>
<li><strong>事实性知识</strong>：通过反复出现的事实陈述，模型能够记住大量事实性信息。</li>
<li><strong>隐式常识</strong>：这是最关键的一点。PLMs 并没有被明确地告知“水往低处流”或“猫有四条腿”，但它们通过分析海量文本中关于“水”、“猫”的描述、行为和相互关系，逐渐构建起这些概念的表征，并在一定程度上学习了与之相关的常识。当模型看到“猫”这个词时，其内部表征会激活与“毛茸茸”、“喵喵叫”、“喜欢抓老鼠”等相关的信息。</li>
</ol>
<p><strong>表征学习中的隐含常识</strong><br>
PLMs 生成的词嵌入 (Word Embeddings) 和上下文嵌入 (Contextual Embeddings) 本身就蕴含了丰富的语义和常识信息。例如：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>vec</mtext><mo stretchy="false">(</mo><mtext>king</mtext><mo stretchy="false">)</mo><mo>−</mo><mtext>vec</mtext><mo stretchy="false">(</mo><mtext>man</mtext><mo stretchy="false">)</mo><mo>+</mo><mtext>vec</mtext><mo stretchy="false">(</mo><mtext>woman</mtext><mo stretchy="false">)</mo><mo>≈</mo><mtext>vec</mtext><mo stretchy="false">(</mo><mtext>queen</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{vec}(\text{king}) - \text{vec}(\text{man}) + \text{vec}(\text{woman}) \approx \text{vec}(\text{queen})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">vec</span></span><span class="mopen">(</span><span class="mord text"><span class="mord">king</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">vec</span></span><span class="mopen">(</span><span class="mord text"><span class="mord">man</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">vec</span></span><span class="mopen">(</span><span class="mord text"><span class="mord">woman</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">vec</span></span><span class="mopen">(</span><span class="mord text"><span class="mord">queen</span></span><span class="mclose">)</span></span></span></span></span></p>
<p>这个著名的例子表明，模型能够学习到词汇之间复杂的语义关系，这种关系可以被视为一种基本常识的数学化表示。更进一步，上下文嵌入能够根据语境调整词的含义，例如在“银行的河岸”和“银行的存钱处”中，“银行”的嵌入会有显著差异。这种对多义词的理解也是常识的一部分。</p>
<p><strong>通过微调提升常识推理能力</strong><br>
尽管PLMs在预训练阶段内化了大量知识，但为了在特定常识推理任务上表现出色，通常还需要进行下游任务的微调。通过在专门设计的常识推理数据集上进行监督学习，模型能够进一步提升其在该任务上的性能。这种“预训练+微调”的范式，极大地推动了NLP在常识推理方面的进展。</p>
<p>然而，我们必须清醒地认识到，PLMs 所学习到的常识，更多的是一种<strong>统计关联</strong>和<strong>模式匹配</strong>，而非真正意义上的因果理解或世界模型。它们善于模仿人类的语言行为，但其深层理解能力仍然是一个开放的科学问题。这引出了我们下一章的内容：如何量化和评估这些模型所具备的常识能力？</p>
<h2 id="第四章：常识推理的基准数据集与任务">第四章：常识推理的基准数据集与任务</h2>
<p>为了评估机器在常识推理方面的能力，研究者们设计了一系列富有挑战性的基准数据集和任务。这些任务迫使模型不仅要理解语言，更要结合外部世界知识和推理能力才能给出正确答案。</p>
<h3 id="Winograd-Schema-Challenge-WSC">Winograd Schema Challenge (WSC)</h3>
<p>Winograd Schema Challenge (WSC) 是由 Hector Levesque 在2011年提出的一项旨在超越图灵测试的常识推理挑战。它是一个二选一的指代消解任务，其特点是只需改变一个词，就能改变代词的指代对象，而这种改变依赖于常识。</p>
<ul>
<li>
<p><strong>背景与挑战</strong>：</p>
<ul>
<li><strong>例1</strong>：The city councilmen refused the demonstrators a permit because <strong>they</strong> advocated violence. (市议员拒绝给示威者许可，因为<strong>他们</strong>主张暴力。)
<ul>
<li>这里“they”很明显指代“demonstrators”（示威者），因为示威者更可能主张暴力。</li>
</ul>
</li>
<li><strong>例2</strong>：The city councilmen refused the demonstrators a permit because <strong>they</strong> feared violence. (市议员拒绝给示威者许可，因为<strong>他们</strong>害怕暴力。)
<ul>
<li>这里“they”很明显指代“city councilmen”（市议员），因为议员更可能害怕暴力并因此拒绝许可。<br>
WSC 的巧妙之处在于，表面上看只是简单的指代消解，但其正确答案的判断，需要模型具备关于社会行为、意图、原因等方面的常识。传统的NLP系统（如基于句法解析的指代消解）很难正确处理这些例子。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>WinoGrande</strong>：</p>
<ul>
<li>WSC 的数据集规模很小，为了解决模型可能通过“作弊”记住答案而非真正推理的问题，Ali Emami 等人在2020年发布了 WinoGrande 数据集。WinoGrande 包含了4.4万个 Winograd Schema 风格的问题，规模更大，并通过对抗性过滤机制来确保模型不能仅仅通过统计偏差来解决问题，从而更准确地评估其常识推理能力。</li>
</ul>
</li>
</ul>
<h3 id="CommonsenseQA-CSQA">CommonsenseQA (CSQA)</h3>
<p>CommonsenseQA (CSQA) 是2019年由 Alon Talmor 等人提出的一个多项选择式的常识问答数据集。它要求模型回答关于实体之间常识关系的问题。</p>
<ul>
<li>
<p><strong>特点</strong>：</p>
<ul>
<li>问题和答案都来源于 ConceptNet 知识图谱，但问题本身并非简单的知识查询，而是需要进行多跳推理或隐含常识判断。</li>
<li><strong>例</strong>：“为了去海边，你会需要什么？”
<ul>
<li>选项：A) 防晒霜，B) 咖啡，C) 滑雪板，D) 字典</li>
<li>正确答案：A) 防晒霜。这需要模型知道海边与阳光、防晒霜的关系。</li>
</ul>
</li>
<li>数据集的设计旨在挑战模型整合语言理解和常识知识的能力。</li>
</ul>
</li>
<li>
<p><strong>与知识图谱的结合</strong>：CSQA 的问题通常可以追溯到 ConceptNet 中的路径。例如，回答“哪里能找到一台洗衣机？”需要知道“洗衣机”通常在“洗衣房”或“厨房”，而这些地点常在“家里”。</p>
</li>
</ul>
<h3 id="HellaSwag">HellaSwag</h3>
<p>HellaSwag 是由 Rowee Zhang 等人在2019年推出的一个用于评估常识推理的任务，其独特之处在于它关注的是<strong>连贯性推理</strong>和<strong>合理行动序列的预测</strong>。</p>
<ul>
<li><strong>特点</strong>：
<ul>
<li>数据集中的每个样本都是一个视频剪辑的开始部分（或文本描述），模型需要从多个选项中选择最合理的下一个事件或动作序列。</li>
<li><strong>例</strong>：Given a context, choose the most likely ending:
<ul>
<li>Context: “一个小女孩在厨房里打碎了一个玻璃杯。”</li>
<li>Options: A) 她开始清理碎片。 B) 她去学校。 C) 她把玻璃杯放回橱柜。 D) 她开始演奏小提琴。</li>
<li>正确答案：A) 她开始清理碎片。这需要模型理解事件的因果关系和常见的行为序列。</li>
</ul>
</li>
<li>HellaSwag 的难点在于，其错误选项通常也看起来合理，但相较于正确选项，它们在常识上显得不那么连贯或可能性较低。数据集的构建利用了 Adversarial Filtering (对抗性过滤) 和 human-in-the-loop 的方法，以确保问题的挑战性。</li>
</ul>
</li>
</ul>
<h3 id="PIQA-Physical-Interaction-Question-Answering">PIQA (Physical Interaction Question Answering)</h3>
<p>PIQA 是由 Jonathan Bras 等人在2020年提出的，专注于物理常识推理。</p>
<ul>
<li><strong>特点</strong>：
<ul>
<li>数据集包含有关日常物理世界中物体和动作的常识问题，通常涉及“如何做某事”以及“为什么这样做”的问题。</li>
<li>每个问题都有两个候选答案，模型需要选择更合理、更安全、更高效或更符合物理定律的方案。</li>
<li><strong>例</strong>：“如何把木头劈开？”
<ul>
<li>选项1：用斧子砍。</li>
<li>选项2：用手掰。</li>
<li>正确答案：选项1。这需要模型知道斧子的功能和木头的物理特性。</li>
</ul>
</li>
<li>PIQA 强调对物理世界基本属性和因果关系的理解。</li>
</ul>
</li>
</ul>
<h3 id="Social-IQa-Social-Interaction-Question-Answering">Social IQa (Social Interaction Question Answering)</h3>
<p>Social IQa 是由 Maarten Sap 等人在2019年提出的，专注于社会常识推理。</p>
<ul>
<li><strong>特点</strong>：
<ul>
<li>数据集中的问题围绕人类互动、情感、意图和社交规范。</li>
<li>模型需要理解人们在特定社交情境下的动机、感受和行为。</li>
<li><strong>例</strong>：“Alice 拒绝了 Bob 的邀请。”
<ul>
<li>动机：A) 她很忙。 B) 她很喜欢 Bob。 C) 她想让他开心。</li>
<li>感受：A) 尴尬。 B) 兴奋。 C) 感激。</li>
<li>正确答案通常是 A) 她很忙 (动机) 和 A) 尴尬 (感受)，这取决于具体的语境。</li>
</ul>
</li>
<li>这项任务旨在推动模型超越简单的文本理解，进入对人类心理和社交行为的深层理解。</li>
</ul>
</li>
</ul>
<h3 id="其他相关任务">其他相关任务</h3>
<p>除了上述典型任务，还有许多其他任务也间接或直接地评估模型的常识推理能力：</p>
<ul>
<li><strong>故事补全 (Story Completion)</strong>：给定故事前文，预测合理的结尾或下一句话。</li>
<li><strong>事件预测 (Event Prediction)</strong>：给定一系列事件，预测下一个可能发生的事件。</li>
<li><strong>因果推理 (Causal Reasoning)</strong>：判断事件之间的因果关系。</li>
<li><strong>蕴含识别 (Recognizing Textual Entailment, RTE)</strong>：判断一个句子是否蕴含另一个句子的含义。</li>
</ul>
<p>这些基准数据集和任务的出现，为常识推理的研究提供了明确的方向和可量化的评估标准。它们促使研究者们开发出更强大的模型，能够更好地捕捉和利用隐含在海量数据中的常识知识。</p>
<h2 id="第五章：面向常识推理的模型架构与技术">第五章：面向常识推理的模型架构与技术</h2>
<p>随着深度学习，特别是预训练语言模型 (PLMs) 的兴起，常识推理领域涌现出许多新颖的模型架构和技术。这些方法旨在更好地利用PLMs的强大表示能力，并结合其他知识源或机制来提升常识推理表现。</p>
<h3 id="基于知识图谱的神经网络（KG-GNNs）">基于知识图谱的神经网络（KG-GNNs）</h3>
<p>尽管PLMs通过海量文本学习了隐含常识，但它们仍然难以完全掌握结构化的、明确的知识。知识图谱 (Knowledge Graphs, KGs) 提供了这种结构化知识。将KGs与神经网络结合，成为了一种强大的常识推理方法。</p>
<ul>
<li>
<p><strong>核心思想</strong>：将知识图谱中的实体和关系嵌入 (embedding) 到低维向量空间中，然后利用图神经网络 (Graph Neural Networks, GNNs) 在图结构上进行信息传播和聚合，从而捕获实体之间的复杂关系和推理路径。</p>
</li>
<li>
<p><strong>知识图谱嵌入 (Knowledge Graph Embeddings)</strong>：</p>
<ul>
<li><strong>TransE (Translating Embeddings for Knowledge Graph Embedding)</strong>：一种经典的KG嵌入模型，它假设如果 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>h</mi><mo separator="true">,</mo><mi>r</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(h, r, t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span> 是一个事实三元组（头实体、关系、尾实体），那么在嵌入空间中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>+</mo><mi>r</mi><mo>≈</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">h + r \approx t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4831em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span>。即关系 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span> 可以被视为从头实体 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span></span></span></span> 到尾实体 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 的翻译向量。</li>
<li>其他模型如 TransH, TransR, RotatE, ComplEx 等，旨在解决不同类型的关系复杂性。</li>
</ul>
</li>
<li>
<p><strong>图神经网络 (Graph Neural Networks, GNNs)</strong>：</p>
<ul>
<li>GNNs 能够直接在图结构上进行学习，通过在节点及其邻居之间传递和聚合信息来更新节点的表示。</li>
<li><strong>图卷积网络 (Graph Convolutional Networks, GCNs)</strong>：通过聚合邻居节点的特征来更新中心节点的表示。在常识推理中，可以用来在知识图谱上进行多跳推理。<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>H</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mover accent="true"><mi>D</mi><mo>~</mo></mover><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><mover accent="true"><mi>A</mi><mo>~</mo></mover><msup><mover accent="true"><mi>D</mi><mo>~</mo></mover><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><msup><mi>H</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup><msup><mi>W</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.938em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.254em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9202em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span><span style="top:-3.6023em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">~</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.004em;"><span style="top:-3.413em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8443em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.2255em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9202em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">A</span></span><span style="top:-3.6023em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1111em;"><span class="mord">~</span></span></span></span></span></span></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9202em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span><span style="top:-3.6023em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">~</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.004em;"><span style="top:-3.413em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8443em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.2255em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>A</mi><mo>~</mo></mover></mrow><annotation encoding="application/x-tex">\tilde{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9202em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9202em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">A</span></span><span style="top:-3.6023em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1111em;"><span class="mord">~</span></span></span></span></span></span></span></span></span></span> 是带有自连接的邻接矩阵，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>D</mi><mo>~</mo></mover></mrow><annotation encoding="application/x-tex">\tilde{D}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9202em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9202em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span><span style="top:-3.6023em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">~</span></span></span></span></span></span></span></span></span></span> 是其度矩阵，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>H</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">H^{(l)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span> 是第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span></span></span></span> 层的节点特征矩阵，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">W^{(l)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span> 是权重矩阵，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span> 是激活函数。</li>
<li><strong>图注意力网络 (Graph Attention Networks, GATs)</strong>：引入了注意力机制，允许模型为不同邻居分配不同的权重，从而更灵活地聚合信息。</li>
<li><strong>应用</strong>：KG-GNNs 可以用于在常识问答中寻找知识图谱中的推理路径，或增强PLMs的知识表示。例如，在 CommonsenseQA 任务中，模型可以利用 ConceptNet 的图结构来寻找连接问题实体和答案实体的路径。</li>
</ul>
</li>
</ul>
<h3 id="多模态常识推理">多模态常识推理</h3>
<p>人类的常识不仅仅来源于文本，更来源于对现实世界多模态信息（视觉、听觉、触觉等）的感知和互动。因此，将多模态信息引入常识推理，是提升机器理解能力的重要方向。</p>
<ul>
<li><strong>结合图像与文本</strong>：
<ul>
<li>例如，HellaSwag 任务虽然是文本形式，但其数据来源是视频字幕，隐含着丰富的视觉常识。</li>
<li>研究者们正在开发能够同时处理图像和文本输入的模型，例如 Visual Question Answering (VQA) 和 Captioning 任务，这些任务往往也需要常识来解决视觉歧义或进行合理推断。</li>
<li>多模态PLMs（如 CLIP, DALL-E, Flamingo 等）能够学习跨模态的统一表示，这为多模态常识推理提供了新的路径。</li>
</ul>
</li>
</ul>
<h3 id="对抗性训练与数据增强">对抗性训练与数据增强</h3>
<p>为了提高模型的鲁棒性和泛化能力，使其不仅仅记住训练数据中的模式，还能应对更复杂、更具挑战性的常识推理问题，研究者们采用了对抗性训练和数据增强技术。</p>
<ul>
<li>
<p><strong>对抗性训练 (Adversarial Training)</strong>：</p>
<ul>
<li>生成对抗样本 (Adversarial Examples)，即对输入进行微小但精心设计的扰动，使模型给出错误预测。然后将这些对抗样本加入训练集，使模型学习如何抵御这些扰动，从而提高其鲁棒性。</li>
<li>在常识推理中，可以生成难以区分的负样本，迫使模型更深入地理解常识，而非仅仅依赖表面特征。</li>
</ul>
</li>
<li>
<p><strong>数据增强 (Data Augmentation)</strong>：</p>
<ul>
<li>通过对现有数据进行变换（如同义词替换、句子重构、噪声注入等）来生成新的训练样本，从而扩大数据集规模并提高模型的泛化能力。</li>
<li>在常识推理任务中，可以基于现有的常识知识或规则来生成新的、具有挑战性的问答对。</li>
</ul>
</li>
</ul>
<h3 id="解释性与可信赖AI">解释性与可信赖AI</h3>
<p>随着模型复杂度的提升，理解其决策过程变得越来越困难。在常识推理这种需要“理解”的任务中，模型的解释性 (Explainability) 和可信赖性 (Trustworthiness) 变得尤为重要。</p>
<ul>
<li>
<p><strong>可解释性AI (XAI)</strong>：</p>
<ul>
<li><strong>注意力机制的可视化</strong>：分析 Transformer 模型中的注意力权重，可以发现模型在做出决策时关注了输入序列的哪些部分。</li>
<li><strong>梯度分析</strong>：通过分析输入对输出的影响（如 LIME, SHAP 等方法），可以识别模型决策中最关键的输入特征。</li>
<li><strong>生成解释</strong>：一些模型可以不仅给出答案，还能生成解释其推理过程的文本。</li>
<li><strong>挑战</strong>：尽管有这些方法，但对于深度模型而言，提供真正符合人类认知逻辑的解释仍然是一个难题。模型的“黑箱”特性使得我们很难确定它是否真的掌握了常识，还是仅仅通过高维统计关联“蒙对”了答案。</li>
</ul>
</li>
<li>
<p><strong>可信赖AI</strong>：</p>
<ul>
<li>确保模型在不同情境下都能稳定、可靠地进行常识推理，避免产生“幻觉”或不合理的答案。</li>
<li>对模型的常识边界进行探索，明确其在哪些领域表现良好，哪些领域仍有欠缺。</li>
</ul>
</li>
</ul>
<p>这些技术的发展，使得我们能够构建更强大、更鲁棒的常识推理系统。然而，要真正让机器拥有人类般的常识，仍然面临巨大的挑战。这正引出了当前研究的前沿：大语言模型（LLMs）如何改变常识推理的格局。</p>
<h2 id="第六章：大语言模型（LLMs）的常识能力与未来展望">第六章：大语言模型（LLMs）的常识能力与未来展望</h2>
<p>毫无疑问，近年来大语言模型（LLMs）的爆发式发展，彻底改变了NLP的版图，也为常识推理领域带来了前所未有的机遇与挑战。</p>
<h3 id="LLMs的常识“涌现”能力">LLMs的常识“涌现”能力</h3>
<p>以GPT-3、PaLM、Llama 等为代表的LLMs，其参数量动辄千亿甚至万亿，在海量无标注数据上进行超大规模预训练。令人惊讶的是，这些模型在没有显式常识知识注入的情况下，展现出了一定程度的常识“涌现”能力 (Emergent Abilities)。</p>
<ul>
<li><strong>上下文学习 (In-context Learning)</strong>：LLMs 能够通过少量示例（通常在提示中给出）来学习新的任务，而无需参数更新。在常识推理任务中，这意味着模型可以通过在提示中提供几个常识问答对，然后对新问题进行推理。</li>
<li><strong>指令遵循 (Instruction Following)</strong>：经过指令微调 (Instruction Tuning) 的LLMs，能够理解并遵循人类的自然语言指令，执行各种复杂的任务，包括常识推理。例如，你可以直接问它：“如果我把手机掉进了马桶里，我应该怎么做？” LLM 通常能给出非常合理的常识性建议。</li>
<li><strong>多模态融合 (Multimodal Fusion)</strong>：最新的多模态LLMs（如 GPT-4V, Gemini）进一步扩展了这种能力，它们不仅能处理文本，还能理解图像和视频内容，从而进行更全面的常识推理。例如，给它一张图片，问“这个人下一步可能会做什么？”，它能结合视觉信息和常识给出答案。</li>
</ul>
<p>LLMs 的这种能力可以归因于：</p>
<ol>
<li><strong>规模效应</strong>：极大的模型规模和训练数据量，使得模型能够捕获到数据中极其复杂和微妙的统计模式，这些模式在一定程度上反映了现实世界的运作规律和常识。</li>
<li><strong>Transformer 架构</strong>：Transformer 的自注意力机制使其能够捕捉长距离依赖，并有效地整合全局信息。</li>
<li><strong>多任务预训练的隐含效益</strong>：虽然LLMs的预训练目标是简单的语言建模（预测下一个词），但为了达到这一目标，模型需要隐含地学习世界的知识、句法、语义，甚至因果关系，这其中就包含了常识。</li>
</ol>
<h3 id="LLMs的局限性与“幻觉”问题">LLMs的局限性与“幻觉”问题</h3>
<p>尽管LLMs表现出惊人的常识能力，但它们并非完美无缺，仍存在显著的局限性：</p>
<ol>
<li><strong>“幻觉” (Hallucination)</strong>：LLMs 可能会生成听起来合理但实际上是错误或虚构的信息。这表明它们可能只是在模仿语言模式，而非真正理解其背后的事实或常识。例如，它可能会自信地告诉你“鱼是哺乳动物”，或者给出不合逻辑的行动建议。</li>
<li><strong>缺乏真正的世界模型</strong>：LLMs 本质上是概率分布模型，它们基于文本的统计关联进行预测。它们没有具身的经验，没有内在的世界模型，无法像人类那样进行物理模拟或因果推断。它们能说出“苹果从树上掉下来”，但可能不真正理解重力。</li>
<li><strong>推理的浅层性</strong>：尽管可以进行多步推理，但LLMs的推理往往是基于文本模式匹配的启发式，而非严谨的逻辑推导。在面对需要深层因果链或逻辑一致性的复杂问题时，它们可能出现错误。</li>
<li><strong>数据偏见与有害内容</strong>：训练数据中的偏见和错误信息也会被LLMs习得，导致它们在常识推理中产生歧视性或有害的输出。</li>
<li><strong>不确定性表达不足</strong>：LLMs 往往缺乏对自身知识边界的感知，对于不确定或模棱两可的问题，它们倾向于生成一个看似合理的答案，而不是承认其不确定性。</li>
</ol>
<h3 id="提示工程-Prompt-Engineering-在常识推理中的应用">提示工程 (Prompt Engineering) 在常识推理中的应用</h3>
<p>提示工程已成为与LLMs交互的关键技术。通过精心设计的提示，我们可以引导LLMs更好地利用其内部的常识知识。</p>
<ul>
<li><strong>少样本学习 (Few-shot Learning)</strong>：在提示中提供少量输入-输出示例，让模型学习任务模式。
<ul>
<li>例如：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">问：我把钥匙锁在车里了，怎么办？</span><br><span class="line">答：你应该尝试联系开锁匠或使用备用钥匙。</span><br><span class="line"></span><br><span class="line">问：我正在做饭，发现食谱上说需要盐，但是我没有盐。我应该怎么办？</span><br><span class="line">答：你可以尝试使用酱油、海带或盐焗花生等替代品，或者问邻居借一点。</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><strong>思维链提示 (Chain-of-Thought Prompting)</strong>：通过在提示中展示逐步推理的过程，引导模型进行更复杂的推理。
<ul>
<li>例如：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">问：如果我要把一个非常大的衣柜搬到楼上，我应该考虑什么？</span><br><span class="line">推理步骤：</span><br><span class="line">1. 考虑衣柜的大小和重量，以及楼梯的宽度和弯曲程度。</span><br><span class="line">2. 确保有足够的人手来搬运，并考虑使用搬运工具，如滑轮或搬运带。</span><br><span class="line">3. 检查门框和楼梯拐角是否能通过。</span><br><span class="line">4. 提前清理搬运路线上的障碍物。</span><br><span class="line">5. 考虑在地面和楼梯上铺设保护垫，防止刮伤或损坏。</span><br><span class="line">答：你需要考虑衣柜的尺寸和重量、楼梯的宽度和形状、所需的人手和工具、以及潜在的路径障碍和保护措施。</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<p>这种方法显著提升了LLMs在复杂常识推理任务上的表现。</p>
<h3 id="混合方法：符号与神经的融合">混合方法：符号与神经的融合</h3>
<p>鉴于符号主义和深度学习各自的优缺点，将两者结合的混合方法 (Hybrid Approaches) 被认为是未来常识推理的重要方向。</p>
<ol>
<li>
<p><strong>知识注入 (Knowledge Injection)</strong>：</p>
<ul>
<li>将结构化的知识图谱（如 ConceptNet）信息注入到PLMs中，例如通过修改预训练目标、在模型架构中引入知识编码层，或在微调阶段利用知识图谱进行正则化。</li>
<li>目标是让PLMs 不仅学习文本中的隐含知识，还能直接利用明确编码的常识。</li>
</ul>
</li>
<li>
<p><strong>知识蒸馏 (Knowledge Distillation)</strong>：</p>
<ul>
<li>将一个“教师”模型（可能是基于知识图谱或传统推理系统）的知识，通过软目标或硬目标的形式，转移给一个“学生”的深度学习模型。</li>
</ul>
</li>
<li>
<p><strong>神经符号系统 (Neuro-Symbolic Systems)</strong>：</p>
<ul>
<li>这是融合两种范式的更深层次尝试。模型可能包含一个神经网络部分，用于处理感知、模式识别和模糊匹配；同时包含一个符号推理部分，用于进行逻辑推理、规划和解释。</li>
<li>例如，神经网络识别出图像中的物体，然后将这些符号化的物体传递给一个逻辑推理引擎，由其利用常识规则进行进一步推理。</li>
<li>这种方法旨在结合神经网络的强大表征学习能力和符号系统的透明性、可控性及严谨性。</li>
</ul>
</li>
</ol>
<h3 id="常识推理的未来方向">常识推理的未来方向</h3>
<p>常识推理依然是AI领域最根本、最长期的挑战之一。未来研究可能关注以下几个方面：</p>
<ol>
<li><strong>更深层次的理解与推理</strong>：超越模式匹配，让机器能够真正理解因果关系、意图、计划和物理世界的基本原理。这可能需要模型构建更接近人类认知的“世界模型”。</li>
<li><strong>动态常识学习</strong>：让AI系统能够从不断变化的现实世界中持续学习新的常识，并适应新的情境。这涉及到无监督、自监督或弱监督学习范式的进一步发展。</li>
<li><strong>个性化与情境化常识</strong>：常识并非完全普适，它有时也依赖于个体经验、文化背景和特定情境。未来的系统可能需要根据用户或特定场景，灵活地运用和调整常识。</li>
<li><strong>具身智能与常识</strong>：让机器人或其他具身AI系统通过与物理世界的真实互动来学习常识，而非仅仅通过文本数据。这是实现真正通用常识理解的终极路径。</li>
<li><strong>可解释性与可信度</strong>：在模型能力提升的同时，确保其常识推理过程是透明的、可解释的，并且在关键任务中是可靠和安全的。</li>
</ol>
<h2 id="结论">结论</h2>
<p>在自然语言处理的广阔天地中，常识推理无疑是一座难以逾越但又必须征服的高峰。从早期的符号主义尝试，研究者们付出了巨大努力构建庞大而严谨的知识库，却最终受限于知识获取的瓶颈和符号接地的难题。</p>
<p>随着深度学习，特别是预训练语言模型（PLMs）的崛起，我们见证了机器在从海量文本数据中“学习”隐含常识方面的惊人进展。BERT、GPT等模型以其强大的语言建模能力，在Winograd Schema Challenge、CommonsenseQA等一系列基准任务上取得了突破性成就，使得机器对语言的理解进入了一个前所未有的深度。最新的大语言模型（LLMs）更是展现出了令人震撼的常识“涌现”能力，它们能够通过上下文学习和指令遵循，在许多常识任务上表现出超越以往的水平。</p>
<p>然而，我们也必须清醒地认识到，LLMs所展现的常识能力，更多是基于统计关联和模式匹配，而非真正的人类般的世界模型或因果理解。它们的“幻觉”问题、推理的浅层性以及对数据偏见的继承，都提醒着我们，距离实现真正具有人类常识的AI，仍有漫长的道路要走。</p>
<p>未来的研究将很可能聚焦于混合方法，即如何有机地融合符号主义的严谨逻辑和深度学习的强大表示能力。同时，探索具身智能、动态学习、多模态融合以及提升模型的可解释性和可信赖性，也将是常识推理领域持续进化的关键方向。</p>
<p>常识，这个人类与生俱来的能力，正成为人工智能研究中最迷人也最具挑战的谜题。我们相信，随着技术的不断演进和跨学科的深入合作，机器终将能够更好地理解我们所共享的这个世界，并真正成为具备智慧的智能伙伴。</p>
<p>感谢大家的阅读，我是 qmwneb946，期待与您在AI的未来旅程中再次相遇！</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://github.com/qmwneb946">qmwneb946</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://qmwneb946.dpdns.org/2025/07/24/2025-07-25-065009/">https://qmwneb946.dpdns.org/2025/07/24/2025-07-25-065009/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://opensource.org/licenses/MIT" target="_blank">MIT License</a> 许可协议。转载请注明来源 <a href="https://qmwneb946.dpdns.org" target="_blank">qmwneb946 的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/">计算机科学</a><a class="post-meta__tags" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E5%B8%B8%E8%AF%86%E6%8E%A8%E7%90%86/">自然语言处理中的常识推理</a></div><div class="post-share"><div class="social-share" data-image="/img/icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/07/24/2025-07-25-065124/" title="揭秘“光子”与“电子”的交汇点：深入探索太赫兹成像技术"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">揭秘“光子”与“电子”的交汇点：深入探索太赫兹成像技术</div></div><div class="info-2"><div class="info-item-1">引言 大家好，我是你们的老朋友qmwneb946，今天我们要聊一个非常酷炫且充满潜力的技术领域——太赫兹成像。想象一下，有一种“光”，它既不像可见光那样容易被不透明物体阻挡，又不像X射线那样具有电离辐射风险；它能够“看透”衣物、塑料、纸张、陶瓷甚至干木材，同时还能识别出物质独特的“指纹”。没错，这就是太赫兹波（Terahertz wave），而太赫兹成像技术正是利用了它的这些神奇特性，为我们的世界打开了一扇全新的窗户。 在电磁波谱中，太赫兹波（通常指频率在0.1 THz到10 THz之间，对应波长在3毫米到30微米之间）像一个被遗忘的角落，横跨了微波与红外线之间。长期以来，由于缺乏高效的太赫兹波源和探测器，这一频段被称为“太赫兹鸿沟”（Terahertz Gap）。然而，随着超快激光技术、半导体材料科学以及微纳加工技术的飞速发展，我们不仅成功跨越了这道鸿沟，更是开启了太赫兹技术在安全检查、无损检测、生物医学、食品安全、通信等众多领域的革命性应用。 太赫兹成像技术究竟有何魔力？它能给我们带来怎样的惊喜？在这篇博客中，我将带领大家从太赫兹波的基本特性出发，深入探讨其如何从“鸿沟”中崛...</div></div></div></a><a class="pagination-related" href="/2025/07/24/2025-07-25-064850/" title="窥探小样本目标检测的奥秘：数据稀缺下的视觉智能难题"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">窥探小样本目标检测的奥秘：数据稀缺下的视觉智能难题</div></div><div class="info-2"><div class="info-item-1">大家好，我是你们的博主 qmwneb946。 在这个 AI 爆炸的时代，深度学习在图像识别、自然语言处理等领域取得了举世瞩目的成就。我们惊叹于模型在海量数据喂养下展现出的强大能力。然而，在真实世界的许多应用场景中，我们往往无法获得充足的、高质量的标注数据。想象一下，一个罕见的疾病样本、一种新发现的物种、或者在极端环境下出现的新目标，这些情况下，传统深度学习模型的“数据饥渴症”便暴露无遗。 这正是“小样本学习”（Few-Shot Learning）的魅力与挑战所在。而当小样本学习与计算机视觉中最核心、最具挑战性的任务之一——目标检测——结合时，便产生了我们今天要深入探讨的主题：小样本目标检测（Few-Shot Object Detection, FSOD）。 小样本目标检测，顾名思思义，旨在让模型在仅有极少量标注样本的情况下，识别并定位图像中的目标。这不仅仅是对数据效率的追求，更是对机器智能更深层次能力的探索：能否像人类一样，通过极少的示例就能快速学习和泛化新概念？ 这篇文章将带领大家，从定义、核心挑战到未来展望，全面剖析小样本目标检测这个前沿而又充满潜力的领域。 什么是小样本目标...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/07/18/2025-07-18-082429/" title="区块链技术与数字版权保护：一场技术与法律的博弈"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">区块链技术与数字版权保护：一场技术与法律的博弈</div></div><div class="info-2"><div class="info-item-1">大家好，我是你们的技术博主X，今天我们来聊一个非常热门的话题：区块链技术如何应用于数字版权保护。在数字内容飞速发展的时代，版权侵权问题日益严峻，传统的版权保护机制显得力不从心。而区块链技术，凭借其去中心化、不可篡改、透明等特性，为解决这一难题提供了新的思路。 区块链技术概述 首先，让我们简单回顾一下区块链技术的基本原理。区块链是一个由多个区块组成的链式数据库，每个区块包含一系列经过加密验证的交易记录。这些交易记录一旦被写入区块链，就无法被篡改或删除，保证了数据的完整性和安全性。  其核心技术包括：  密码学:  确保数据的安全性和完整性，例如哈希算法和数字签名。 共识机制:  例如工作量证明（PoW）和权益证明（PoS），用于维护区块链的统一性和安全性，防止恶意攻击。 分布式账本: 数据分布在多个节点上，提高了系统的容错性和安全性。  区块链如何保护数字版权 区块链技术可以为数字版权保护提供多种方案，主要体现在以下几个方面： 版权登记与确权 传统的版权登记流程繁琐且耗时，而区块链可以提供一个快速、透明的版权登记平台。创作者可以将作品的哈希值（作品的数字指纹）记录到区块链上，以此证...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082418/" title="机器学习算法的公平性问题：技术挑战与伦理困境"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">机器学习算法的公平性问题：技术挑战与伦理困境</div></div><div class="info-2"><div class="info-item-1">引言 机器学习 (ML) 正在迅速改变我们的世界，从医疗保健到金融，再到刑事司法系统，它的应用几乎无处不在。然而，随着 ML 系统的广泛部署，一个越来越令人担忧的问题浮出水面：公平性。  算法的输出可能反映并放大现有的社会偏见，导致对某些群体的不公平待遇。本文将深入探讨机器学习算法中的公平性问题，分析其技术根源和伦理困境，并探讨一些可能的解决方案。 偏见是如何进入机器学习模型的？ 机器学习模型的公平性问题并非源于算法本身的恶意，而是源于其训练数据的偏见。  这些偏见可能来自多种来源： 数据收集与标注  样本选择偏差 (Sampling Bias):  如果训练数据未能充分代表所有群体，模型就会学习到一个有偏的表示。例如，如果一个用于预测贷款偿还能力的模型主要基于白人申请人的数据，它可能会对少数族裔申请人产生不公平的负面预测。 测量偏差 (Measurement Bias):  数据收集过程中的错误或不一致也会引入偏见。例如，在犯罪预测模型中，如果某些社区的执法力度更大，导致该社区的犯罪数据被过度记录，模型就会对该社区产生负面偏见。 标注偏差 (Label Bias):  人工标注...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082438/" title="云计算中的数据安全与隐私：挑战与应对"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">云计算中的数据安全与隐私：挑战与应对</div></div><div class="info-2"><div class="info-item-1">云计算为企业和个人提供了强大的计算资源和数据存储能力，但也带来了新的安全与隐私挑战。本文将深入探讨云计算环境下的数据安全与隐私问题，分析其背后的技术机制，并提出一些有效的应对策略。 云计算安全风险剖析 云计算环境中，数据安全与隐私面临着多种威胁，主要包括： 数据泄露与丢失 这是最常见的风险之一。  数据可能由于云提供商的内部安全漏洞、恶意攻击（例如SQL注入、DDoS攻击）、员工失误或意外事件（例如硬件故障）而泄露或丢失。  对于敏感数据，例如医疗记录、金融信息和个人身份信息，这种风险尤为严重。 数据违规 数据违规是指未经授权访问或使用数据的情况。这可能导致数据被篡改、删除或用于非法目的。  法规遵从性（例如 GDPR, CCPA）的压力也使得数据违规的代价越来越高。 权限管理不足 缺乏细粒度的访问控制机制可能导致数据被未授权的个人或应用程序访问。  复杂的云环境中，权限的管理和审核是一个极大的挑战。 数据完整性问题 云环境中的数据完整性需要得到保障，确保数据没有被未经授权的修改或破坏。  这需要使用诸如哈希算法和数字签名等技术来验证数据的完整性。 数据合规性 不同国家和地区对数...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082500/" title="物联网设备的网络安全协议：挑战与解决方案"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">物联网设备的网络安全协议：挑战与解决方案</div></div><div class="info-2"><div class="info-item-1">物联网 (IoT) 设备正以前所未有的速度渗透到我们生活的方方面面，从智能家居到工业自动化，再到医疗保健。然而，这种广泛的连接也带来了巨大的安全风险。由于物联网设备通常资源受限，安全性设计常常被忽视，导致它们成为网络攻击的理想目标。本文将深入探讨物联网设备面临的网络安全挑战，以及用于增强其安全性的各种协议和技术。 物联网安全面临的挑战 物联网设备的安全挑战与传统IT系统大相径庭，主要体现在以下几个方面： 资源受限 许多物联网设备具有有限的处理能力、内存和存储空间。这使得部署复杂的加密算法和安全协议变得困难，同时也增加了运行时开销。  运行资源消耗较大的安全软件可能会影响设备的性能甚至导致其崩溃。 设备异构性 物联网生态系统由各种各样的设备组成，这些设备运行不同的操作系统，使用不同的编程语言，并具有不同的安全特性。这种异构性使得实施统一的安全策略变得极其复杂。  很难找到一个适用于所有设备的通用安全解决方案。 数据隐私与安全 物联网设备通常会收集大量敏感数据，例如个人健康信息、位置数据和财务信息。保护这些数据的隐私和安全至关重要，但由于设备自身的安全缺陷和数据传输过程中的漏洞，这成...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082528/" title="量子计算对现代密码学的威胁：后量子密码学的挑战与机遇"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">量子计算对现代密码学的威胁：后量子密码学的挑战与机遇</div></div><div class="info-2"><div class="info-item-1">量子计算的飞速发展为许多领域带来了革命性的变革，但也对现有的密码体系构成了前所未有的挑战。本文将深入探讨量子计算如何威胁现代密码学，以及我们如何应对这一挑战。 量子计算的优势与密码学的困境 经典计算机基于比特，其值只能是 0 或 1。而量子计算机利用量子比特，可以同时表示 0 和 1 的叠加态，这使得它们能够进行并行计算，处理能力远超经典计算机。  这种巨大的计算能力为解决某些目前被认为是“不可解”的问题提供了可能性，其中就包括许多现代密码学的基石。 例如，RSA 算法，广泛应用于电子商务和安全通信，其安全性依赖于大数分解的困难性。经典计算机分解一个很大的数需要指数级的时间，因此被认为是安全的。然而，Shor 算法，一个在量子计算机上运行的算法，能够以多项式时间分解大数。这意味着，一台足够强大的量子计算机能够轻易破解 RSA 加密，从而威胁到大量的在线交易、数据安全以及国家安全。 同样，椭圆曲线密码学 (ECC)，另一种广泛使用的密码算法，其安全性也依赖于某些数学问题的复杂性。然而，量子计算机也能够有效地解决这些问题，例如离散对数问题。 Shor 算法与 Grover 算法：量子...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082537/" title="图论算法在社交网络分析中的应用"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">图论算法在社交网络分析中的应用</div></div><div class="info-2"><div class="info-item-1">社交网络已经成为我们生活中不可或缺的一部分。从Facebook和Twitter到微信和微博，这些平台连接着数十亿用户，产生着海量的数据。而理解这些数据，挖掘其背后的规律和价值，就需要借助强大的数学工具——图论。本文将深入探讨图论算法在社交网络分析中的多种应用。 社交网络的图表示 在图论中，社交网络可以被自然地表示为图 G=(V,E)G = (V, E)G=(V,E)，其中 VVV 代表用户集合（节点），EEE 代表用户之间的关系集合（边）。例如，在Facebook中，每个用户是一个节点，如果两个用户是朋友，则在他们之间存在一条无向边；在Twitter中，如果用户A关注用户B，则存在一条从A指向B的有向边。边的权重可以表示关系的强度（例如，朋友关系的亲密度，或者互动频率）。  这种图表示为我们分析社交网络提供了坚实的基础。 核心图论算法及其应用 社区发现 社区发现旨在将社交网络划分成多个紧密连接的社区（也称为集群）。这对于理解用户群体、推荐系统以及病毒式营销等都至关重要。常用的算法包括：  Louvain算法:  一种贪婪的启发式算法，通过迭代优化模块度来寻找最佳社区结构。模块度 ...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qmwneb946</div><div class="author-info-description">一个专注于技术分享的个人博客，涵盖编程、算法、系统设计等内容</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">1357</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1361</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qmwneb946"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qmwneb946" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:qmwneb946@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">代码与远方，技术与生活交织的篇章。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%AF%86%E6%8E%A8%E7%90%86%E7%9A%84%E6%9C%AC%E8%B4%A8%E4%B8%8E%E6%8C%91%E6%88%98"><span class="toc-number">1.</span> <span class="toc-text">第一章：常识推理的本质与挑战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%B8%B8%E8%AF%86%EF%BC%9F"><span class="toc-number">1.1.</span> <span class="toc-text">什么是常识？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%BA%E5%99%A8%E9%9A%BE%E4%BB%A5%E6%8E%8C%E6%8F%A1%E5%B8%B8%E8%AF%86%EF%BC%9F"><span class="toc-number">1.2.</span> <span class="toc-text">为什么机器难以掌握常识？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%AF%86%E5%9C%A8NLP%E4%B8%AD%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-number">1.3.</span> <span class="toc-text">常识在NLP中的重要性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E6%97%A9%E6%9C%9F%E7%AC%A6%E5%8F%B7%E4%B8%BB%E4%B9%89%E6%96%B9%E6%B3%95%E4%B8%8E%E7%9F%A5%E8%AF%86%E5%BA%93%E6%9E%84%E5%BB%BA"><span class="toc-number">2.</span> <span class="toc-text">第二章：早期符号主义方法与知识库构建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%A6%E5%8F%B7%E4%B8%BB%E4%B9%89%E7%9A%84%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80"><span class="toc-number">2.1.</span> <span class="toc-text">符号主义的理论基础</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%9E%84%E5%BB%BA%E7%9A%84%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E5%BA%93"><span class="toc-number">2.2.</span> <span class="toc-text">人工构建的常识知识库</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Cyc"><span class="toc-number">2.2.1.</span> <span class="toc-text">Cyc</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#WordNet"><span class="toc-number">2.2.2.</span> <span class="toc-text">WordNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ConceptNet"><span class="toc-number">2.2.3.</span> <span class="toc-text">ConceptNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Freebase-Wikidata"><span class="toc-number">2.2.4.</span> <span class="toc-text">Freebase &#x2F; Wikidata</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E5%92%8C%E9%80%BB%E8%BE%91%E7%9A%84%E6%8E%A8%E7%90%86%E7%B3%BB%E7%BB%9F"><span class="toc-number">2.3.</span> <span class="toc-text">基于规则和逻辑的推理系统</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%A6%E5%8F%B7%E6%96%B9%E6%B3%95%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">2.4.</span> <span class="toc-text">符号方法的局限性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%8C%83%E5%BC%8F"><span class="toc-number">3.</span> <span class="toc-text">第三章：数据驱动的深度学习范式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E5%88%B0%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.1.</span> <span class="toc-text">从特征工程到端到端学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88PLMs%EF%BC%89%E7%9A%84%E5%B4%9B%E8%B5%B7"><span class="toc-number">3.2.</span> <span class="toc-text">预训练语言模型（PLMs）的崛起</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9A%E5%B8%B8%E8%AF%86%E6%8E%A8%E7%90%86%E7%9A%84%E5%9F%BA%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8E%E4%BB%BB%E5%8A%A1"><span class="toc-number">4.</span> <span class="toc-text">第四章：常识推理的基准数据集与任务</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Winograd-Schema-Challenge-WSC"><span class="toc-number">4.1.</span> <span class="toc-text">Winograd Schema Challenge (WSC)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CommonsenseQA-CSQA"><span class="toc-number">4.2.</span> <span class="toc-text">CommonsenseQA (CSQA)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HellaSwag"><span class="toc-number">4.3.</span> <span class="toc-text">HellaSwag</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PIQA-Physical-Interaction-Question-Answering"><span class="toc-number">4.4.</span> <span class="toc-text">PIQA (Physical Interaction Question Answering)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Social-IQa-Social-Interaction-Question-Answering"><span class="toc-number">4.5.</span> <span class="toc-text">Social IQa (Social Interaction Question Answering)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E7%9B%B8%E5%85%B3%E4%BB%BB%E5%8A%A1"><span class="toc-number">4.6.</span> <span class="toc-text">其他相关任务</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E9%9D%A2%E5%90%91%E5%B8%B8%E8%AF%86%E6%8E%A8%E7%90%86%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%8A%80%E6%9C%AF"><span class="toc-number">5.</span> <span class="toc-text">第五章：面向常识推理的模型架构与技术</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88KG-GNNs%EF%BC%89"><span class="toc-number">5.1.</span> <span class="toc-text">基于知识图谱的神经网络（KG-GNNs）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E5%B8%B8%E8%AF%86%E6%8E%A8%E7%90%86"><span class="toc-number">5.2.</span> <span class="toc-text">多模态常识推理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E6%8A%97%E6%80%A7%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">5.3.</span> <span class="toc-text">对抗性训练与数据增强</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E6%80%A7%E4%B8%8E%E5%8F%AF%E4%BF%A1%E8%B5%96AI"><span class="toc-number">5.4.</span> <span class="toc-text">解释性与可信赖AI</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLMs%EF%BC%89%E7%9A%84%E5%B8%B8%E8%AF%86%E8%83%BD%E5%8A%9B%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B"><span class="toc-number">6.</span> <span class="toc-text">第六章：大语言模型（LLMs）的常识能力与未来展望</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LLMs%E7%9A%84%E5%B8%B8%E8%AF%86%E2%80%9C%E6%B6%8C%E7%8E%B0%E2%80%9D%E8%83%BD%E5%8A%9B"><span class="toc-number">6.1.</span> <span class="toc-text">LLMs的常识“涌现”能力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LLMs%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7%E4%B8%8E%E2%80%9C%E5%B9%BB%E8%A7%89%E2%80%9D%E9%97%AE%E9%A2%98"><span class="toc-number">6.2.</span> <span class="toc-text">LLMs的局限性与“幻觉”问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B-Prompt-Engineering-%E5%9C%A8%E5%B8%B8%E8%AF%86%E6%8E%A8%E7%90%86%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">6.3.</span> <span class="toc-text">提示工程 (Prompt Engineering) 在常识推理中的应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B7%E5%90%88%E6%96%B9%E6%B3%95%EF%BC%9A%E7%AC%A6%E5%8F%B7%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%9A%84%E8%9E%8D%E5%90%88"><span class="toc-number">6.4.</span> <span class="toc-text">混合方法：符号与神经的融合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%AF%86%E6%8E%A8%E7%90%86%E7%9A%84%E6%9C%AA%E6%9D%A5%E6%96%B9%E5%90%91"><span class="toc-number">6.5.</span> <span class="toc-text">常识推理的未来方向</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">7.</span> <span class="toc-text">结论</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/26/hello-world/" title="Hello World">Hello World</a><time datetime="2025-07-26T08:21:24.408Z" title="发表于 2025-07-26 16:21:24">2025-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/26/%E5%8D%9A%E5%BC%88%E8%AE%BA%E5%9F%BA%E7%A1%80/" title="博弈论基础">博弈论基础</a><time datetime="2025-07-26T08:21:24.408Z" title="发表于 2025-07-26 16:21:24">2025-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/26/2025-07-26-081818/" title="深入解析量子信息处理的物理实现：从原理到前沿">深入解析量子信息处理的物理实现：从原理到前沿</a><time datetime="2025-07-26T00:18:18.000Z" title="发表于 2025-07-26 08:18:18">2025-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/26/2025-07-26-081652/" title="金融风险的传染模型：洞悉系统性危机的数学之美与工程实践">金融风险的传染模型：洞悉系统性危机的数学之美与工程实践</a><time datetime="2025-07-26T00:16:52.000Z" title="发表于 2025-07-26 08:16:52">2025-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/26/2025-07-26-081535/" title="动力系统中的分形吸引子：混沌之美与秩序">动力系统中的分形吸引子：混沌之美与秩序</a><time datetime="2025-07-26T00:15:35.000Z" title="发表于 2025-07-26 08:15:35">2025-07-26</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2025 By qmwneb946</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.2</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'qmwneb946/blog',
      'data-repo-id': 'R_kgDOPM6cWw',
      'data-category-id': 'DIC_kwDOPM6cW84CtEzo',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme)
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Giscus' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Giscus' === 'Giscus' || !false) {
    if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>