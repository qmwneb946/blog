<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>深入探讨神经网络：从原理到实践的探索 | qmwneb946 的博客</title><meta name="author" content="qmwneb946"><meta name="copyright" content="qmwneb946"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="引言 在当今科技浪潮中，“人工智能”无疑是最激动人心的词汇之一。从智能推荐系统到自动驾驶汽车，从疾病诊断到自然语言处理，AI正以前所未有的速度改变着我们的世界。而在这场变革的核心，隐藏着一个精妙且强大的计算模型——神经网络。 对于许多技术爱好者而言，神经网络似乎是一个神秘的“黑箱”。它如何学习？为何能做出如此复杂的决策？本文旨在揭开神经网络的神秘面纱，带您从最基本的神经元开始，逐步深入理解其内部机">
<meta property="og:type" content="article">
<meta property="og:title" content="深入探讨神经网络：从原理到实践的探索">
<meta property="og:url" content="https://blog.qmwneb946.dpdns.org/2025/07/17/2025-07-17-221929/index.html">
<meta property="og:site_name" content="qmwneb946 的博客">
<meta property="og:description" content="引言 在当今科技浪潮中，“人工智能”无疑是最激动人心的词汇之一。从智能推荐系统到自动驾驶汽车，从疾病诊断到自然语言处理，AI正以前所未有的速度改变着我们的世界。而在这场变革的核心，隐藏着一个精妙且强大的计算模型——神经网络。 对于许多技术爱好者而言，神经网络似乎是一个神秘的“黑箱”。它如何学习？为何能做出如此复杂的决策？本文旨在揭开神经网络的神秘面纱，带您从最基本的神经元开始，逐步深入理解其内部机">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.qmwneb946.dpdns.org/img/icon.png">
<meta property="article:published_time" content="2025-07-17T14:19:29.000Z">
<meta property="article:modified_time" content="2025-07-18T05:32:14.813Z">
<meta property="article:author" content="qmwneb946">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="数学">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.qmwneb946.dpdns.org/img/icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "深入探讨神经网络：从原理到实践的探索",
  "url": "https://blog.qmwneb946.dpdns.org/2025/07/17/2025-07-17-221929/",
  "image": "https://blog.qmwneb946.dpdns.org/img/icon.png",
  "datePublished": "2025-07-17T14:19:29.000Z",
  "dateModified": "2025-07-18T05:32:14.813Z",
  "author": [
    {
      "@type": "Person",
      "name": "qmwneb946",
      "url": "https://github.com/qmwneb946"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://blog.qmwneb946.dpdns.org/2025/07/17/2025-07-17-221929/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深入探讨神经网络：从原理到实践的探索',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="qmwneb946 的博客" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qmwneb946 的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">深入探讨神经网络：从原理到实践的探索</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">深入探讨神经网络：从原理到实践的探索<a class="post-edit-link" href="https://github.com/qmwneb946/blog/edit/main/source/_posts/2025-07-17-221929.md" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-17T14:19:29.000Z" title="发表于 2025-07-17 22:19:29">2025-07-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-18T05:32:14.813Z" title="更新于 2025-07-18 13:32:14">2025-07-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%95%B0%E5%AD%A6/">数学</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h2 id="引言">引言</h2>
<p>在当今科技浪潮中，“人工智能”无疑是最激动人心的词汇之一。从智能推荐系统到自动驾驶汽车，从疾病诊断到自然语言处理，AI正以前所未有的速度改变着我们的世界。而在这场变革的核心，隐藏着一个精妙且强大的计算模型——神经网络。</p>
<p>对于许多技术爱好者而言，神经网络似乎是一个神秘的“黑箱”。它如何学习？为何能做出如此复杂的决策？本文旨在揭开神经网络的神秘面纱，带您从最基本的神经元开始，逐步深入理解其内部机制、训练过程以及面临的挑战，最终展望其未来的发展。无论您是初学者还是有一定基础的开发者，都将从这次深度探索中获益。</p>
<h2 id="1-神经网络的基石：神经元">1. 神经网络的基石：神经元</h2>
<p>要理解神经网络，我们必须从其最基本的组成单位——<strong>神经元（Neuron）<strong>或称为</strong>感知机（Perceptron）</strong>——开始。它受到生物神经元的启发，尽管其数学模型远比生物神经元简单，但已足够强大。</p>
<p>一个人工神经元接收来自其他神经元的输入信号，每个信号都有一个<strong>权重（Weight）</strong>，表示该输入的重要性。所有加权输入会被求和，并加上一个**偏置（Bias）<strong>项。最后，这个和会通过一个</strong>激活函数（Activation Function）**来产生神经元的输出。</p>
<p><strong>数学表达：</strong><br>
假设一个神经元接收 ( n ) 个输入 ( x_1, x_2, \dots, x_n )，对应的权重为 ( w_1, w_2, \dots, w_n )，偏置为 ( b )。<br>
首先，计算加权和：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>z</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">z = \sum_{i=1}^{n} w_i x_i + b 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span></span></p>
<p>然后，通过激活函数 ( f ) 产生输出 ( a )：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>a</mi><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a = f(z) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span></span></p>
<h3 id="1-1-激活函数">1.1 激活函数</h3>
<p>激活函数是神经网络中至关重要的一环，它引入了<strong>非线性</strong>。如果没有激活函数（或者使用线性激活函数），无论网络有多少层，它都只能学习线性关系，这大大限制了其表达能力。非线性使得神经网络能够逼近任意复杂的函数。</p>
<p>以下是一些常见的激活函数：</p>
<ul>
<li>
<p><strong>Sigmoid 函数：</strong></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>z</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\sigma(z) = \frac{1}{1 + e^{-z}} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0908em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6973em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>将输入压缩到 ( (0, 1) ) 区间。常用于二分类任务的输出层。<br>
<strong>优点：</strong> 输出平滑，易于求导。<br>
<strong>缺点：</strong> 容易出现**梯度消失（Vanishing Gradient）**问题，即当 ( z ) 值非常大或非常小时，梯度趋近于0，导致网络训练缓慢或停滞。</p>
</li>
<li>
<p><strong>ReLU (Rectified Linear Unit) 函数：</strong></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>ReLU</mtext><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{ReLU}(z) = \max(0, z) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">ReLU</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span></span></p>
<p>当输入大于0时，直接输出输入值；当输入小于等于0时，输出0。<br>
<strong>优点：</strong> 解决了Sigmoid和Tanh的梯度消失问题（在正区间），计算效率高。是目前隐藏层最常用的激活函数。<br>
<strong>缺点：</strong> “死亡ReLU”问题，即当输入永远为负时，神经元将不再激活。</p>
</li>
<li>
<p><strong>Tanh (Hyperbolic Tangent) 函数：</strong></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mi>z</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>z</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>z</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>z</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2177em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4483em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.5904em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6973em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>将输入压缩到 ( (-1, 1) ) 区间。<br>
<strong>优点：</strong> 相对于Sigmoid，输出以0为中心，有助于梯度下降。<br>
<strong>缺点：</strong> 同样存在梯度消失问题。</p>
</li>
<li>
<p><strong>Softmax 函数：</strong></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Softmax</mtext><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mi>i</mi></msub></msup><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.6484em;vertical-align:-1.307em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3414em;"><span style="top:-2.1288em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6065em;"><span style="top:-3.0051em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.044em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.044em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.307em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>常用于多分类任务的输出层，将一组数值转换成概率分布，所有输出值之和为1。</p>
</li>
</ul>
<p><strong>Python 伪代码实现一个简单神经元：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Neuron</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, activation_function</span>):</span><br><span class="line">        <span class="comment"># 随机初始化权重和偏置</span></span><br><span class="line">        <span class="variable language_">self</span>.weights = np.random.randn(num_inputs) * <span class="number">0.01</span> </span><br><span class="line">        <span class="variable language_">self</span>.bias = np.random.randn() * <span class="number">0.01</span></span><br><span class="line">        <span class="variable language_">self</span>.activation_function = activation_function</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="comment"># 计算加权和 z</span></span><br><span class="line">        z = np.dot(inputs, <span class="variable language_">self</span>.weights) + <span class="variable language_">self</span>.bias</span><br><span class="line">        <span class="comment"># 通过激活函数</span></span><br><span class="line">        output = <span class="variable language_">self</span>.activation_function(z)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的ReLU激活函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例使用</span></span><br><span class="line"><span class="comment"># my_neuron = Neuron(num_inputs=3, activation_function=relu)</span></span><br><span class="line"><span class="comment"># inputs = np.array([0.5, 0.2, 0.8])</span></span><br><span class="line"><span class="comment"># output = my_neuron.forward(inputs)</span></span><br><span class="line"><span class="comment"># print(f&quot;神经元输出: &#123;output&#125;&quot;)</span></span><br></pre></td></tr></table></figure>
<h2 id="2-网络的结构：层与连接">2. 网络的结构：层与连接</h2>
<p>单个神经元的能力有限，真正的智能源于神经元的互联。当神经元堆叠成<strong>层（Layer）</strong>，并将这些层连接起来时，就形成了<strong>神经网络（Neural Network）</strong>。</p>
<p>一个典型的<strong>前馈神经网络（Feedforward Neural Network）</strong>，也称为<strong>多层感知机（Multi-Layer Perceptron, MLP）</strong>，通常包含以下几种层：</p>
<ul>
<li><strong>输入层（Input Layer）：</strong> 接收原始数据，例如图像的像素值、文本的词向量等。输入层神经元的数量由数据的特征维度决定。</li>
<li><strong>隐藏层（Hidden Layers）：</strong> 位于输入层和输出层之间，是神经网络进行特征提取和学习的核心。一个网络可以有一个或多个隐藏层，层数越多，网络的深度越深，理论上可以学习更复杂的模式。</li>
<li><strong>输出层（Output Layer）：</strong> 产生网络的最终预测结果。输出层神经元的数量和激活函数取决于任务类型（如分类、回归）。</li>
</ul>
<p>在全连接（Dense）神经网络中，每一层中的每个神经元都与前一层的所有神经元相连接，这意味着前一层的所有输出都将作为当前层每个神经元的输入。信息从输入层单向传播，经过隐藏层，最终到达输出层，这个过程称为<strong>前向传播（Forward Propagation）</strong>。</p>
<h2 id="3-神经网络的训练：学习的艺术">3. 神经网络的训练：学习的艺术</h2>
<p>神经网络的“智能”并非与生俱来，而是通过**训练（Training）**从大量数据中学习而来。训练的目标是调整网络内部的权重和偏置，使得网络对给定输入的预测结果尽可能接近真实值。</p>
<h3 id="3-1-损失函数-Loss-Function">3.1 损失函数 (Loss Function)</h3>
<p>训练的第一步是量化“预测错误”的程度。<strong>损失函数（Loss Function）</strong>，也称为<strong>代价函数（Cost Function）<strong>或</strong>目标函数（Objective Function）</strong>，用于计算模型预测值与真实值之间的差异。损失值越小，表示模型的预测越准确。</p>
<ul>
<li>
<p><strong>均方误差（Mean Squared Error, MSE）：</strong><br>
常用于<strong>回归</strong>问题。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi mathvariant="bold">w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>m</mi></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false">(</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>−</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">J(\mathbf{w}, b) = \frac{1}{2m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathbf" style="margin-right:0.01597em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord mathnormal">m</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中，( m ) 是样本数量，( y^{(i)} ) 是第 ( i ) 个样本的真实值，( \hat{y}^{(i)} ) 是模型的预测值。前面乘以 ( \frac{1}{2} ) 是为了求导时方便。</p>
</li>
<li>
<p><strong>交叉熵（Cross-Entropy）：</strong><br>
常用于<strong>分类</strong>问题，特别是多分类问题。<br>
对于二分类：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo>=</mo><mo>−</mo><mo stretchy="false">(</mo><mi>y</mi><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L = -(y \log(\hat{y}) + (1-y) \log(1-\hat{y})) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mclose">))</span></span></span></span></span></p>
<p>其中，( y ) 是真实标签（0或1），( \hat{y} ) 是模型预测为1的概率。<br>
对于多分类（Softmax + Categorical Cross-Entropy）：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>y</mi><mi>k</mi></msub><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L = -\sum_{k=1}^{K} y_k \log(\hat{y}_k) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.1304em;vertical-align:-1.3021em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8479em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3021em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>其中，( K ) 是类别数量，( y_k ) 是真实标签的one-hot编码（如果为该类别则为1，否则为0），( \hat{y}_k ) 是模型预测为该类别的概率。</p>
</li>
</ul>
<h3 id="3-2-优化算法：梯度下降-Gradient-Descent">3.2 优化算法：梯度下降 (Gradient Descent)</h3>
<p>训练神经网络的本质就是找到一组最优的权重和偏置，使得损失函数的值最小。这个过程通常通过<strong>优化算法（Optimization Algorithm）<strong>实现，其中最基础且最核心的就是</strong>梯度下降（Gradient Descent）</strong>。</p>
<p>可以把损失函数想象成一个崎岖的山谷，我们的目标是找到山谷的最低点。梯度下降法就像一个登山者，每一步都沿着当前位置最陡峭的方向（负梯度方向）下山，直到达到谷底。</p>
<p>**梯度（Gradient）**是损失函数相对于每个权重和偏置的偏导数向量，它指向函数值增加最快的方向。因此，我们沿着梯度的反方向调整参数。</p>
<p>参数更新规则：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>w</mi><mo>:</mo><mo>=</mo><mi>w</mi><mo>−</mo><mi>α</mi><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>J</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">w := w - \alpha \frac{\partial J}{\partial w} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.0574em;vertical-align:-0.686em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>b</mi><mo>:</mo><mo>=</mo><mi>b</mi><mo>−</mo><mi>α</mi><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>J</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>b</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">b := b - \alpha \frac{\partial J}{\partial b} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.0574em;vertical-align:-0.686em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">b</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>其中，( w ) 代表网络中的所有权重，( b ) 代表所有偏置，( J ) 是损失函数，( \alpha ) 是<strong>学习率（Learning Rate）</strong>，它决定了每一步参数更新的步长。学习率过大可能导致跳过最优解，过小则可能导致收敛速度过慢。</p>
<p>根据每次更新使用的样本数量，梯度下降有几种变体：</p>
<ul>
<li><strong>批量梯度下降（Batch Gradient Descent）：</strong> 使用所有训练样本计算梯度，更新一次参数。计算量大，但方向准确。</li>
<li><strong>随机梯度下降（Stochastic Gradient Descent, SGD）：</strong> 每次只使用一个样本计算梯度并更新参数。更新频繁，可能导致路径震荡，但收敛速度快。</li>
<li><strong>小批量梯度下降（Mini-batch Gradient Descent）：</strong> 介于两者之间，每次使用一小批样本（通常是几十到几百个）计算梯度。这是实践中最常用的方法，兼顾了稳定性和效率。</li>
</ul>
<h3 id="3-3-反向传播-Backpropagation">3.3 反向传播 (Backpropagation)</h3>
<p>梯度下降需要计算损失函数对每个权重和偏置的偏导数。对于一个多层神经网络来说，手动计算这些偏导数是非常复杂的。<strong>反向传播（Backpropagation）算法</strong>就是为了高效计算这些梯度而发明的。</p>
<p>反向传播基于<strong>链式法则（Chain Rule）</strong>，其核心思想是将输出层的误差（损失函数的值）反向传播回网络，逐层计算每个神经元对误差的贡献，从而得到每个权重和偏置的梯度。</p>
<p><strong>大致流程：</strong></p>
<ol>
<li><strong>前向传播：</strong> 输入数据通过网络，逐层计算输出，直到得到最终预测值和损失。</li>
<li><strong>反向传播：</strong>
<ul>
<li>首先计算输出层神经元的误差。</li>
<li>利用链式法则，将误差信号从输出层向输入层逐层传播。</li>
<li>在传播过程中，计算每一层中每个权重和偏置对总误差的贡献（即它们的梯度）。</li>
</ul>
</li>
<li><strong>参数更新：</strong> 根据计算出的梯度和学习率，使用梯度下降法更新所有权重和偏置。</li>
</ol>
<p>这个前向传播和反向传播的循环，在训练数据集上重复进行多次（称为** эпоха，Epoch**），直到模型的性能达到满意水平或收敛。</p>
<h2 id="4-挑战与解决方案">4. 挑战与解决方案</h2>
<p>神经网络的训练并非一帆风顺，会遇到一些常见挑战：</p>
<h3 id="4-1-过拟合-Overfitting">4.1 过拟合 (Overfitting)</h3>
<p><strong>过拟合</strong>是指模型在训练数据上表现非常好，但在未见过的新数据（测试数据）上表现很差的现象。这通常是由于模型过于复杂，过度学习了训练数据中的噪声和特有模式，而不是泛化规律。</p>
<p><strong>解决方案：</strong></p>
<ul>
<li><strong>增加训练数据：</strong> 最直接有效的方法。</li>
<li><strong>正则化（Regularization）：</strong>
<ul>
<li><strong>L1/L2 正则化：</strong> 在损失函数中添加惩罚项，限制权重的大小，鼓励模型使用更简单的参数。</li>
<li><strong>Dropout：</strong> 在训练过程中随机“关闭”（置零）一部分神经元及其连接，强迫网络学习更鲁棒的特征表示。</li>
</ul>
</li>
<li><strong>早停（Early Stopping）：</strong> 在训练过程中监控模型在验证集上的性能，当验证集性能不再提升甚至开始下降时，提前停止训练。</li>
<li><strong>简化模型：</strong> 减少网络的层数或每层的神经元数量。</li>
</ul>
<h3 id="4-2-梯度消失与梯度爆炸-Vanishing-and-Exploding-Gradients">4.2 梯度消失与梯度爆炸 (Vanishing and Exploding Gradients)</h3>
<ul>
<li><strong>梯度消失：</strong> 在深度网络中，反向传播时梯度在传播过程中变得越来越小，导致浅层网络的权重几乎无法更新，网络学习停滞。Sigmoid和Tanh激活函数是主要原因之一。</li>
<li><strong>梯度爆炸：</strong> 梯度在传播过程中变得非常大，导致权重更新过大，模型参数发散，训练不稳定。</li>
</ul>
<p><strong>解决方案：</strong></p>
<ul>
<li><strong>使用ReLU及其变体：</strong> ReLU在正区间梯度为常数1，有效缓解梯度消失。</li>
<li><strong>批量归一化（Batch Normalization）：</strong> 在网络的每一层输入激活函数之前对数据进行归一化，使其均值为0，方差为1。这有助于稳定梯度，加速训练，并对学习率不那么敏感。</li>
<li><strong>残差连接（Residual Connections）：</strong> 在深度学习（如ResNet）中，允许信息“跳过”某些层直接传递，有助于缓解梯度消失，使得训练更深的网络成为可能。</li>
<li><strong>权重初始化：</strong> 采用更合适的权重初始化策略（如He初始化、Xavier初始化），避免初始梯度过小或过大。</li>
<li><strong>梯度裁剪（Gradient Clipping）：</strong> 当梯度值超过某个阈值时，将其截断。主要用于处理梯度爆炸。</li>
</ul>
<h2 id="5-展望：未来的神经网络">5. 展望：未来的神经网络</h2>
<p>神经网络的研究和应用日新月异。除了我们探讨的基础多层感知机外，还有许多先进的架构和概念：</p>
<ul>
<li><strong>卷积神经网络（Convolutional Neural Networks, CNNs）：</strong> 专门用于处理图像数据，通过卷积层、池化层等提取空间特征，在计算机视觉领域取得了巨大成功。</li>
<li><strong>循环神经网络（Recurrent Neural Networks, RNNs）及其变体（LSTM, GRU）：</strong> 擅长处理序列数据，如文本、语音，因为它们具有处理时间依赖性的能力。</li>
<li><strong>Transformer：</strong> 基于自注意力机制的模型，彻底改变了自然语言处理领域，并开始在计算机视觉等其他领域展现出强大潜力。</li>
<li><strong>生成对抗网络（Generative Adversarial Networks, GANs）：</strong> 由一个生成器和一个判别器组成，通过对抗学习生成逼真的新数据。</li>
<li><strong>强化学习（Reinforcement Learning）：</strong> 将神经网络与决策过程结合，使智能体通过与环境的互动学习最优策略。</li>
</ul>
<p>未来的神经网络将更加注重：</p>
<ul>
<li><strong>可解释性（Explainable AI, XAI）：</strong> 尝试理解模型决策的原因，而非仅仅得到结果。</li>
<li><strong>鲁棒性与安全性：</strong> 提高模型抵御对抗性攻击的能力。</li>
<li><strong>小数据学习与迁移学习：</strong> 在数据量有限的情况下依然能高效学习。</li>
<li><strong>神经符号AI：</strong> 结合神经网络的感知能力和符号AI的推理能力。</li>
<li><strong>能效与边缘计算：</strong> 开发更小、更快的模型，使其能在资源受限的设备上运行。</li>
</ul>
<h2 id="结论">结论</h2>
<p>从最基本的神经元到复杂的深度学习网络，我们已经深入探讨了神经网络的运作原理、学习机制以及面对的挑战。神经网络的强大之处在于其从数据中学习复杂模式的能力，而反向传播和梯度下降则是其学习的引擎。</p>
<p>理解这些基本原理，不仅能帮助我们更好地使用现有的AI工具，更能激发我们探索创新解决方案的灵感。神经网络领域仍在高速发展，每一次技术突破都可能带来新的范式变革。希望本文能为您打开一扇门，邀请您进一步探索这个充满无限可能的智能世界。现在，是时候拿起您的Python编辑器，亲自动手构建第一个神经网络了！</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://github.com/qmwneb946">qmwneb946</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.qmwneb946.dpdns.org/2025/07/17/2025-07-17-221929/">https://blog.qmwneb946.dpdns.org/2025/07/17/2025-07-17-221929/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://opensource.org/licenses/MIT" target="_blank">MIT License</a> 许可协议。转载请注明来源 <a href="https://blog.qmwneb946.dpdns.org" target="_blank">qmwneb946 的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/%E6%95%B0%E5%AD%A6/">数学</a></div><div class="post-share"><div class="social-share" data-image="/img/icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/07/17/2025-07-17-231957/" title="P vs NP 问题：计算世界的终极谜团"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">P vs NP 问题：计算世界的终极谜团</div></div><div class="info-2"><div class="info-item-1"> 引言：百万美元的计算之谜 在计算机科学和数学的殿堂中，P vs NP 问题无疑是最耀眼、最深刻的未解之谜之一。它被克莱数学研究所列为七个“千禧年大奖问题”之一，悬赏一百万美元征求任何一个正确的解答。但这不仅仅是金钱的诱惑，这个问题的答案将彻底改变我们对计算能力的理解，甚至颠覆我们世界的运作方式。 P vs NP 问题，简而言之，就是在问一个直观的问题：如果一个问题的解决方案可以被快速验证（即，如果你被提供一个答案，你能很快确认它是否正确），那么这个问题的解决方案是否也能被快速找到？这个问题触及了计算的本质，它的答案将对密码学、人工智能、优化理论、药物发现乃至哲学产生深远影响。 本文将深入探讨 P 类问题和 NP 类问题的定义，剖析 P vs NP 问题的核心，介绍 NP-完全性这一关键概念，并展望如果 P=NP 或 P!=NP，世界将发生怎样的变化。 什么是P类问题？ P，代表“多项式时间”（Polynomial Time）。P 类问题指的是那些可以在多项式时间内被确定性图灵机解决的问题。 定义： 一个问题属于 P 类，意味着存在一个算法，其运行时间可以被输入规模 (n) 的多...</div></div></div></a><a class="pagination-related" href="/2025/07/17/2025-07-17-211854/" title="机器学习算法概述：从原理到应用的全景探索"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">机器学习算法概述：从原理到应用的全景探索</div></div><div class="info-2"><div class="info-item-1">机器学习 (Machine Learning, ML) 作为人工智能领域的核心分支，正以前所未有的速度改变着我们的世界。从智能推荐系统、自动驾驶到疾病诊断，机器学习算法无处不在。但这些神奇的功能背后，究竟是哪些“魔法”在运作？作为一名技术和数学爱好者，深入理解机器学习算法的原理至关重要。 本文将带领大家系统地探索机器学习算法的广阔图景。我们将从算法的学习方式出发，将其划分为几个主要范畴：监督学习、无监督学习、半监督学习和强化学习，并对每个范畴内的核心算法进行深入浅出的介绍。 1. 机器学习的基石：学习范式概览 机器学习的本质是让计算机通过数据而不是明确的编程来学习。根据数据类型和学习目标的不同，机器学习算法通常被分为以下几大类： 1.1 监督学习 (Supervised Learning) 核心思想： 从带有标签（即已知输入和对应输出）的数据中学习一个映射函数。目标是预测新输入数据对应的输出。 常见任务：  回归 (Regression): 预测连续值输出，例如房价、股票价格。 分类 (Classification): 预测离散的类别标签，例如邮件是否为垃圾邮件、图片中是猫还是狗...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/07/17/2025-07-17-141851/" title="机器学习算法概述：从原理到实践的探索"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-17</div><div class="info-item-2">机器学习算法概述：从原理到实践的探索</div></div><div class="info-2"><div class="info-item-1">在当今数字驱动的世界里，机器学习（Machine Learning, ML）已不再是科幻小说中的概念，而是深入到我们生活的方方面面，从智能推荐系统、自动驾驶到医疗诊断和金融风控。它像一位无形的设计师，悄然重塑着我们的体验和效率。但机器学习究竟是什么？它背后的“魔力”源于何处？ 本文旨在为技术爱好者们提供一份高质量、有深度的机器学习算法概述。我们将深入探讨机器学习的核心范式，剖析各类经典算法的原理与应用，并揭示其背后的数学美学。无论您是初学者还是希望系统化知识的实践者，本文都将为您打开机器学习的精彩大门。 机器学习的基石：四大核心学习范式 机器学习的核心思想是让计算机系统通过数据“学习”，从而无需明确编程就能执行特定任务。根据数据类型和学习目标的不同，机器学习通常被划分为以下四大范式： 1. 监督学习 (Supervised Learning) 监督学习是机器学习中最常见、应用最广泛的一种范式。它的核心在于**“有监督”**，即模型通过带有标签（已知答案）的数据进行训练。你可以将其想象成一个学生，在老师（标签）的指导下，通过大量的练习（数据）来学习如何解决问题。 目标：从输入数据和...</div></div></div></a><a class="pagination-related" href="/2025/07/17/2025-07-17-152148/" title="无服务器架构解析：从概念到实践的深度探索"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-17</div><div class="info-item-2">无服务器架构解析：从概念到实践的深度探索</div></div><div class="info-2"><div class="info-item-1">在云计算的演进浪潮中，有一种架构范式正以其独特的魅力改变着我们开发和部署应用的方式，它就是“无服务器架构”（Serverless Architecture）。这个名字听起来有些反直觉——毕竟，没有服务器，应用程序又如何在空中运行呢？作为一名技术和数学的博主，我将带你深入探索无服务器架构的奥秘，从它的核心概念、组成部件，到其优势与挑战，并结合数学视角分析其成本效益，最终展望其未来。 引言：云计算的“终极抽象”之旅 回望软件开发的历史，我们经历了从物理机到虚拟机，再到容器化的演进。每一次变革都旨在提高资源利用率、简化部署和管理。  物理机时代：你拥有并维护自己的硬件，一切从零开始。 IaaS (Infrastructure as a Service)：云服务商提供虚拟机，你依然需要管理操作系统和运行时。 PaaS (Platform as a Service)：云服务商提供完整的运行时环境，你只需部署代码，但仍需关心平台配置和伸缩。 容器化 (Containerization)：如Docker和Kubernetes，提供了标准化的部署单元和强大的编排能力，但集群管理依然复杂。  而无...</div></div></div></a><a class="pagination-related" href="/2025/07/17/2025-07-17-182902/" title="机器学习算法概述：从原理到实践的深度剖析"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-17</div><div class="info-item-2">机器学习算法概述：从原理到实践的深度剖析</div></div><div class="info-2"><div class="info-item-1"> 引言：人工智能的引擎——机器学习 在当今数字驱动的世界里，“人工智能”和“机器学习”已不再是遥远的科幻概念，而是深刻地融入了我们生活的方方面面：从智能手机的面部识别解锁，到电商平台的个性化商品推荐，从自动驾驶汽车的路径规划，到医疗领域的疾病诊断辅助。机器学习，作为人工智能的核心引擎，正是赋予机器从数据中学习并做出决策能力的科学。 它本质上是一种通过数据而非显式编程来让计算机获得学习能力的范式。想象一下，你无需一步步告诉计算机如何识别猫，而是向它展示成千上万张猫的图片，它便能自己归纳出“猫”的特征。这便是机器学习的魔力。 本文旨在为技术爱好者提供一份高质量、有深度的机器学习算法概述。我们将深入探讨机器学习的三大主要范式，并详细介绍每个范畴下的核心算法，理解它们的原理、应用场景以及优缺点。让我们一起踏上这场探索之旅，揭开机器学习算法的神秘面纱。 机器学习的基石：三大核心学习范式 机器学习算法通常根据其学习方式和数据类型分为以下三大类：监督学习、无监督学习和强化学习。理解这三种范式是理解所有机器学习算法的基础。 1. 监督学习 (Supervised Learning) 核心思想： ...</div></div></div></a><a class="pagination-related" href="/2025/07/17/2025-07-17-191728/" title="图论入门：连接世界的数学之美"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-17</div><div class="info-item-2">图论入门：连接世界的数学之美</div></div><div class="info-2"><div class="info-item-1">引言 想象一下现代生活中的各种互联系统：社交网络中的好友关系，城市中错综复杂的道路，互联网上的信息流，甚至是生物体内的蛋白质相互作用网络。这些看似不同的系统，背后却隐藏着一个共同且强大的数学框架——图论。 图论（Graph Theory）是数学的一个分支，它研究的是点（顶点或节点）与点之间连接（边）的结构。它提供了一种抽象而直观的方式来建模和分析各种关系和连接问题。从计算机科学到运筹学，从物理学到生物学，图论都扮演着不可或缺的角色。 作为一名技术爱好者，掌握图论的基础知识，不仅能帮助你更好地理解各种算法背后的逻辑，还能为解决复杂的实际问题提供全新的视角。本文将带你步入图论的大门，从基本概念讲起，深入探讨图的表示方法、经典算法，并展望其在现实世界中的广泛应用。 图论的基础概念 在图论中，我们使用“图”来表示对象之间的关系。一个图 (G) 通常由两个集合定义：顶点集合 (V) 和边集合 (E)。 G=(V,E)G = (V, E)  G=(V,E)  顶点（Vertex / Node）：集合 (V) 中的元素，代表了我们要建模的实体或对象。例如，社交网络中的用户、城市中的十字路口。 ...</div></div></div></a><a class="pagination-related" href="/2025/07/17/2025-07-17-211854/" title="机器学习算法概述：从原理到应用的全景探索"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-17</div><div class="info-item-2">机器学习算法概述：从原理到应用的全景探索</div></div><div class="info-2"><div class="info-item-1">机器学习 (Machine Learning, ML) 作为人工智能领域的核心分支，正以前所未有的速度改变着我们的世界。从智能推荐系统、自动驾驶到疾病诊断，机器学习算法无处不在。但这些神奇的功能背后，究竟是哪些“魔法”在运作？作为一名技术和数学爱好者，深入理解机器学习算法的原理至关重要。 本文将带领大家系统地探索机器学习算法的广阔图景。我们将从算法的学习方式出发，将其划分为几个主要范畴：监督学习、无监督学习、半监督学习和强化学习，并对每个范畴内的核心算法进行深入浅出的介绍。 1. 机器学习的基石：学习范式概览 机器学习的本质是让计算机通过数据而不是明确的编程来学习。根据数据类型和学习目标的不同，机器学习算法通常被分为以下几大类： 1.1 监督学习 (Supervised Learning) 核心思想： 从带有标签（即已知输入和对应输出）的数据中学习一个映射函数。目标是预测新输入数据对应的输出。 常见任务：  回归 (Regression): 预测连续值输出，例如房价、股票价格。 分类 (Classification): 预测离散的类别标签，例如邮件是否为垃圾邮件、图片中是猫还是狗...</div></div></div></a><a class="pagination-related" href="/2025/07/17/2025-07-17-231957/" title="P vs NP 问题：计算世界的终极谜团"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-17</div><div class="info-item-2">P vs NP 问题：计算世界的终极谜团</div></div><div class="info-2"><div class="info-item-1"> 引言：百万美元的计算之谜 在计算机科学和数学的殿堂中，P vs NP 问题无疑是最耀眼、最深刻的未解之谜之一。它被克莱数学研究所列为七个“千禧年大奖问题”之一，悬赏一百万美元征求任何一个正确的解答。但这不仅仅是金钱的诱惑，这个问题的答案将彻底改变我们对计算能力的理解，甚至颠覆我们世界的运作方式。 P vs NP 问题，简而言之，就是在问一个直观的问题：如果一个问题的解决方案可以被快速验证（即，如果你被提供一个答案，你能很快确认它是否正确），那么这个问题的解决方案是否也能被快速找到？这个问题触及了计算的本质，它的答案将对密码学、人工智能、优化理论、药物发现乃至哲学产生深远影响。 本文将深入探讨 P 类问题和 NP 类问题的定义，剖析 P vs NP 问题的核心，介绍 NP-完全性这一关键概念，并展望如果 P=NP 或 P!=NP，世界将发生怎样的变化。 什么是P类问题？ P，代表“多项式时间”（Polynomial Time）。P 类问题指的是那些可以在多项式时间内被确定性图灵机解决的问题。 定义： 一个问题属于 P 类，意味着存在一个算法，其运行时间可以被输入规模 (n) 的多...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qmwneb946</div><div class="author-info-description">一个专注于技术分享的个人博客，涵盖编程、算法、系统设计等内容</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">18</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qmwneb946"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">代码与远方，技术与生活交织的篇章。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E7%9F%B3%EF%BC%9A%E7%A5%9E%E7%BB%8F%E5%85%83"><span class="toc-number">2.</span> <span class="toc-text">1. 神经网络的基石：神经元</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.</span> <span class="toc-text">1.1 激活函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84%EF%BC%9A%E5%B1%82%E4%B8%8E%E8%BF%9E%E6%8E%A5"><span class="toc-number">3.</span> <span class="toc-text">2. 网络的结构：层与连接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%AD%E7%BB%83%EF%BC%9A%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%89%BA%E6%9C%AF"><span class="toc-number">4.</span> <span class="toc-text">3. 神经网络的训练：学习的艺术</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-Loss-Function"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 损失函数 (Loss Function)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%9A%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-Gradient-Descent"><span class="toc-number">4.2.</span> <span class="toc-text">3.2 优化算法：梯度下降 (Gradient Descent)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-Backpropagation"><span class="toc-number">4.3.</span> <span class="toc-text">3.3 反向传播 (Backpropagation)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%8C%91%E6%88%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number">5.</span> <span class="toc-text">4. 挑战与解决方案</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E8%BF%87%E6%8B%9F%E5%90%88-Overfitting"><span class="toc-number">5.1.</span> <span class="toc-text">4.1 过拟合 (Overfitting)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8-Vanishing-and-Exploding-Gradients"><span class="toc-number">5.2.</span> <span class="toc-text">4.2 梯度消失与梯度爆炸 (Vanishing and Exploding Gradients)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%B1%95%E6%9C%9B%EF%BC%9A%E6%9C%AA%E6%9D%A5%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.</span> <span class="toc-text">5. 展望：未来的神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">7.</span> <span class="toc-text">结论</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/18/hello-world/" title="Hello World">Hello World</a><time datetime="2025-07-18T05:32:14.814Z" title="发表于 2025-07-18 13:32:14">2025-07-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/17/2025-07-18-052537/" title="P vs NP：计算机科学的千年之问与未解之谜">P vs NP：计算机科学的千年之问与未解之谜</a><time datetime="2025-07-17T21:25:37.000Z" title="发表于 2025-07-18 05:25:37">2025-07-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/17/2025-07-18-043836/" title="揭秘代码的炼金术：编译器是如何工作的？">揭秘代码的炼金术：编译器是如何工作的？</a><time datetime="2025-07-17T20:38:36.000Z" title="发表于 2025-07-18 04:38:36">2025-07-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/17/2025-07-18-032828/" title="赋能与变革：人工智能在软件开发中的深远作用">赋能与变革：人工智能在软件开发中的深远作用</a><time datetime="2025-07-17T19:28:28.000Z" title="发表于 2025-07-18 03:28:28">2025-07-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/17/2025-07-18-014322/" title="欧拉恒等式的优雅：数学与美的终极融合">欧拉恒等式的优雅：数学与美的终极融合</a><time datetime="2025-07-17T17:43:22.000Z" title="发表于 2025-07-18 01:43:22">2025-07-18</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2025 By qmwneb946</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.2</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'qmwneb946/blog',
      'data-repo-id': 'R_kgDOPM6cWw',
      'data-category-id': 'DIC_kwDOPM6cW84CtEzo',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme)
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Giscus' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Giscus' === 'Giscus' || !false) {
    if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>