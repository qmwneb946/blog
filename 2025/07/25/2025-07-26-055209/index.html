<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>情感的量化：自然语言处理中的情感分析深度探索 | qmwneb946 的博客</title><meta name="author" content="qmwneb946"><meta name="copyright" content="qmwneb946"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="您好，我是 qmwneb946，一位热衷于探索技术与数学奥秘的博主。今天，我们将一同踏上一段激动人心的旅程，深入自然语言处理（NLP）的核心领域之一：情感分析。在这个信息爆炸的时代，文本数据以惊人的速度增长，从社交媒体上的只言片语到海量的客户评论，它们承载着人类最宝贵的信息——情感。如何从这些浩瀚的非结构化数据中识别、提取并理解情感倾向，正是情感分析的魅力所在。 引言：文本数据中的“心跳” 人类的">
<meta property="og:type" content="article">
<meta property="og:title" content="情感的量化：自然语言处理中的情感分析深度探索">
<meta property="og:url" content="https://qmwneb946.dpdns.org/2025/07/25/2025-07-26-055209/index.html">
<meta property="og:site_name" content="qmwneb946 的博客">
<meta property="og:description" content="您好，我是 qmwneb946，一位热衷于探索技术与数学奥秘的博主。今天，我们将一同踏上一段激动人心的旅程，深入自然语言处理（NLP）的核心领域之一：情感分析。在这个信息爆炸的时代，文本数据以惊人的速度增长，从社交媒体上的只言片语到海量的客户评论，它们承载着人类最宝贵的信息——情感。如何从这些浩瀚的非结构化数据中识别、提取并理解情感倾向，正是情感分析的魅力所在。 引言：文本数据中的“心跳” 人类的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qmwneb946.dpdns.org/img/icon.png">
<meta property="article:published_time" content="2025-07-25T21:52:09.000Z">
<meta property="article:modified_time" content="2025-07-26T07:58:51.113Z">
<meta property="article:author" content="qmwneb946">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="计算机科学">
<meta property="article:tag" content="自然语言处理中的情感分析">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qmwneb946.dpdns.org/img/icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "情感的量化：自然语言处理中的情感分析深度探索",
  "url": "https://qmwneb946.dpdns.org/2025/07/25/2025-07-26-055209/",
  "image": "https://qmwneb946.dpdns.org/img/icon.png",
  "datePublished": "2025-07-25T21:52:09.000Z",
  "dateModified": "2025-07-26T07:58:51.113Z",
  "author": [
    {
      "@type": "Person",
      "name": "qmwneb946",
      "url": "https://github.com/qmwneb946"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qmwneb946.dpdns.org/2025/07/25/2025-07-26-055209/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '情感的量化：自然语言处理中的情感分析深度探索',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2845632165165414" crossorigin="anonymous"></script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="qmwneb946 的博客" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qmwneb946 的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">情感的量化：自然语言处理中的情感分析深度探索</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">情感的量化：自然语言处理中的情感分析深度探索<a class="post-edit-link" href="https://github.com/qmwneb946/blog/edit/main/source/_posts/2025-07-26-055209.md" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-25T21:52:09.000Z" title="发表于 2025-07-26 05:52:09">2025-07-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-26T07:58:51.113Z" title="更新于 2025-07-26 15:58:51">2025-07-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/">计算机科学</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><p>您好，我是 qmwneb946，一位热衷于探索技术与数学奥秘的博主。今天，我们将一同踏上一段激动人心的旅程，深入自然语言处理（NLP）的核心领域之一：情感分析。在这个信息爆炸的时代，文本数据以惊人的速度增长，从社交媒体上的只言片语到海量的客户评论，它们承载着人类最宝贵的信息——情感。如何从这些浩瀚的非结构化数据中识别、提取并理解情感倾向，正是情感分析的魅力所在。</p>
<h3 id="引言：文本数据中的“心跳”">引言：文本数据中的“心跳”</h3>
<p>人类的沟通，无论是语言还是文字，无不饱含着情感的色彩。每一条评论、每一篇博客、每一次社交互动，都可能蕴藏着对产品、服务、事件或人物的看法、态度和情绪。在数字化浪潮的推动下，这些情感以文本的形式大量沉积在互联网的每一个角落。对于企业而言，这些是洞察用户心声、优化产品服务的金矿；对于社会管理者而言，这是掌握舆论动态、预测社会趋势的晴雨表；对于个人而言，它甚至能帮助我们理解他人的感受。</p>
<p>然而，人工处理和分析如此庞大的文本数据，无异于大海捞针，效率低下且容易受到主观偏差的影响。这就是情感分析（Sentiment Analysis），或称意见挖掘（Opinion Mining），大显身手的时刻。它旨在利用计算方法自动识别和提取文本中表达的情感倾向，通常分为积极、消极或中性。</p>
<p>从最初基于词典的简单规则，到复杂的机器学习模型，再到如今由深度学习和大规模预训练语言模型驱动的前沿技术，情感分析在过去几十年中取得了显著的进步。它不再仅仅是判断“好”与“坏”，而是能够理解更细微的情绪、处理复杂的语言现象，甚至识别特定方面的情感。</p>
<p>在本文中，我将带领大家从情感分析的基本概念出发，逐步深入其传统方法，然后重点探讨深度学习如何彻底改变了这一领域。最后，我们将展望情感分析面临的挑战以及未来的发展方向。准备好了吗？让我们一起揭开文本情感的神秘面纱！</p>
<h2 id="第一部分：情感分析的基石">第一部分：情感分析的基石</h2>
<p>在深入探讨具体的实现方法之前，我们首先需要明确情感分析的定义、粒度以及它为何如此重要。</p>
<h3 id="什么是情感分析？">什么是情感分析？</h3>
<p>情感分析的核心任务是判断一段文本所表达的情感是积极的、消极的还是中性的。但其内涵远不止于此。</p>
<ul>
<li>
<p><strong>定义</strong>：情感分析是一种自然语言处理（NLP）技术，它通过计算方法自动识别、提取和量化文本中表达的主观信息（如观点、情绪、态度）。</p>
</li>
<li>
<p><strong>情感粒度</strong>：根据分析的范围，情感分析可以分为不同的粒度级别：</p>
<ul>
<li><strong>文档级（Document-level）</strong>：判断整篇文档（例如，一篇产品评论、一封邮件）的整体情感倾向。这是最粗粒度的分析。</li>
<li><strong>句子级（Sentence-level）</strong>：判断文档中每个独立句子的情感倾向。这比文档级更细致，因为一篇文档可能包含不同情感倾向的句子。</li>
<li><strong>方面级（Aspect-level）</strong>：也称为特征级情感分析。这是最精细的粒度，它不仅识别情感，还识别情感所针对的具体实体或方面。例如，在“这部手机的摄像头很棒，但电池续航太差了”这句话中，摄像头的情感是积极的，而电池的情感是消极的。</li>
</ul>
</li>
<li>
<p><strong>情感类型</strong>：</p>
<ul>
<li><strong>极性（Polarity）</strong>：最常见的分类，通常分为积极（Positive）、消极（Negative）和中性（Neutral）。</li>
<li><strong>情绪（Emotion）</strong>：更细致的分类，识别文本中表达的具体情绪，如喜悦、悲伤、愤怒、惊讶、恐惧、厌恶等。这通常需要更复杂的情感模型和标注数据。</li>
<li><strong>强度（Intensity）</strong>：量化情感的强度或程度，例如“非常满意”比“满意”具有更高的积极强度。</li>
</ul>
</li>
</ul>
<h3 id="为什么情感分析至关重要？">为什么情感分析至关重要？</h3>
<p>情感分析的价值体现在它能够将非结构化的文本数据转化为可量化的、有意义的洞察，从而赋能各个行业和领域。</p>
<ul>
<li>
<p><strong>商业应用</strong>：</p>
<ul>
<li><strong>客户反馈分析</strong>：企业可以自动处理海量的客户评论、社交媒体帖子、调查问卷和客户服务对话，快速识别产品/服务的优点和缺点，理解客户痛点，从而改进产品和提升服务质量。</li>
<li><strong>品牌声誉管理</strong>：实时监测社交媒体和新闻报道中关于品牌的提及，及时发现负面情绪，进行危机公关，维护品牌形象。</li>
<li><strong>市场调研与竞争分析</strong>：分析消费者对竞品或行业趋势的看法，为市场营销策略和产品开发提供数据支持。</li>
<li><strong>个性化推荐</strong>：根据用户情感偏好推荐内容或产品。</li>
</ul>
</li>
<li>
<p><strong>社会与政治应用</strong>：</p>
<ul>
<li><strong>舆情监控</strong>：政府或机构可以监测公众对政策、事件或人物的态度和情绪，及时了解民意，辅助决策。</li>
<li><strong>政治分析</strong>：分析政治候选人在社交媒体上的支持率和民众对其政策的看法。</li>
<li><strong>心理健康支持</strong>：通过分析社交媒体或聊天记录中的文本，识别潜在的心理健康风险，提供早期干预。</li>
</ul>
</li>
<li>
<p><strong>挑战</strong>：尽管情感分析潜力巨大，但它并非没有挑战。人类语言的复杂性、多样性和语境依赖性给情感分析带来了固有的难度：</p>
<ul>
<li><strong>否定词和程度副词</strong>：“不坏”的含义与“坏”截然不同；“非常满意”与“有点满意”的情感强度不同。</li>
<li><strong>讽刺与反语（Sarcasm/Irony）</strong>：这是情感分析最棘手的难题之一。例如，“这服务真是‘好极了’，我等了两个小时！” 显然是负面情感，但字面含义却是积极的。</li>
<li><strong>双关语与歧义</strong>：一个词在不同语境下可能有不同的含义和情感色彩。</li>
<li><strong>隐式情感</strong>：有些情感表达并非直接，而是通过描述事实或比喻来体现。</li>
<li><strong>领域特异性</strong>：在不同领域，同一个词可能具有不同的情感倾向。例如，“bug”在昆虫学中是中性的，但在软件开发中通常是负面的。</li>
<li><strong>情感的微妙性</strong>：介于积极和消极之间的复杂情感，如失望、困惑、惊讶等，难以准确捕捉。</li>
</ul>
</li>
</ul>
<p>正是这些挑战，推动着情感分析技术的不断演进和发展。接下来，我们将深入探讨几种主要的情感分析方法。</p>
<h2 id="第二部分：传统情感分析方法">第二部分：传统情感分析方法</h2>
<p>在深度学习兴起之前，情感分析主要依赖于基于词典的方法和传统的机器学习方法。这些方法虽然相对简单，但它们为理解情感分析奠定了基础。</p>
<h3 id="基于词典的方法-Lexicon-based-Approaches">基于词典的方法 (Lexicon-based Approaches)</h3>
<p>基于词典的方法是最直观的情感分析方法之一。它的核心思想是维护一个情感词典（Sentiment Lexicon），其中包含了大量词语及其对应的情感极性（积极/消极）和情感强度。</p>
<ul>
<li>
<p><strong>原理</strong>：</p>
<ol>
<li><strong>构建情感词典</strong>：情感词典通常包含以下几类词语：
<ul>
<li><strong>情感词</strong>：如“好”、“坏”、“棒”、“差”、“喜欢”、“讨厌”等。每个词被赋予一个情感分数或极性标签。</li>
<li><strong>否定词</strong>：如“不”、“没有”、“并非”等，它们可以反转情感词的极性。</li>
<li><strong>程度副词</strong>：如“非常”、“有点”、“太”、“几乎”等，它们可以增强或减弱情感词的强度。</li>
</ul>
</li>
<li><strong>情感计算</strong>：对于待分析的文本，算法会遍历文本中的词语。
<ul>
<li>如果遇到情感词，则累加其情感分数。</li>
<li>如果情感词前有否定词，则反转其分数。</li>
<li>如果情感词前有程度副词，则调整其分数。</li>
<li>最终，将所有词语的情感分数累加起来，根据总分判断文本的整体情感极性。例如，总分大于0为积极，小于0为消极，等于0为中性。</li>
</ul>
</li>
</ol>
</li>
<li>
<p><strong>情感分数计算示例</strong>：<br>
一个简单的情感计算公式可以表示为：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>S</mi><mo>=</mo><munder><mo>∑</mo><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>∈</mo><mi>T</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></munder><mo stretchy="false">(</mo><mtext>Polarity</mtext><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>×</mo><mtext>Modifier</mtext><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">S = \sum_{w_i \in Text} (\text{Polarity}(w_i) \times \text{Modifier}(w_i))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4444em;vertical-align:-1.3944em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8557em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.0269em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">x</span><span class="mord mathnormal mtight">t</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3944em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord text"><span class="mord">Polarity</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Modifier</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span></span></p>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span> 是文本的总情感分数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Polarity</mtext><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Polarity}(w_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Polarity</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 是词语 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">w_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的情感极性分数（例如，积极词为+1，消极词为-1），<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Modifier</mtext><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Modifier}(w_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Modifier</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 是由否定词或程度副词决定的修饰因子（例如，否定词为-1，程度副词为1.5等）。</p>
</li>
<li>
<p><strong>词典构建方式</strong>：</p>
<ul>
<li><strong>人工构建</strong>：最直接但耗时耗力的方式。</li>
<li><strong>半自动扩展</strong>：从少量种子词出发，利用同义词词典、WordNet等资源，或基于词语的共现统计（例如，与已知情感词频繁共同出现的词可能具有相似情感）来扩展词典。</li>
</ul>
</li>
<li>
<p><strong>优点</strong>：</p>
<ul>
<li><strong>简单且易于理解</strong>：原理直观，易于实现。</li>
<li><strong>可解释性强</strong>：可以直接追溯到是哪些词语贡献了情感分数。</li>
<li><strong>无需大量标注数据</strong>：只需要一个高质量的情感词典。</li>
</ul>
</li>
<li>
<p><strong>缺点</strong>：</p>
<ul>
<li><strong>依赖词典质量</strong>：词典的覆盖率和准确性直接影响效果。对于领域特异性词语、网络流行语等新词，词典难以覆盖。</li>
<li><strong>语境敏感性差</strong>：很难处理讽刺、双关语、以及词语在不同语境下情感反转的情况。</li>
<li><strong>无法捕捉复杂句式</strong>：对于复杂的句法结构和语义依赖，基于词典的方法往往力不从心。</li>
</ul>
</li>
<li>
<p><strong>代码示例（Python）</strong>：<br>
下面是一个非常简化的基于词典的情感分析示例。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简化的情感词典</span></span><br><span class="line">sentiment_lexicon = &#123;</span><br><span class="line">    <span class="string">&#x27;好&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;棒&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;喜欢&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;满意&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;优秀&#x27;</span>: <span class="number">1.5</span>,</span><br><span class="line">    <span class="string">&#x27;差&#x27;</span>: -<span class="number">1</span>, <span class="string">&#x27;坏&#x27;</span>: -<span class="number">2</span>, <span class="string">&#x27;讨厌&#x27;</span>: -<span class="number">1</span>, <span class="string">&#x27;失望&#x27;</span>: -<span class="number">1</span>, <span class="string">&#x27;糟糕&#x27;</span>: -<span class="number">1.5</span>,</span><br><span class="line">    <span class="string">&#x27;一般&#x27;</span>: <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 否定词列表</span></span><br><span class="line">negation_words = [<span class="string">&#x27;不&#x27;</span>, <span class="string">&#x27;没有&#x27;</span>, <span class="string">&#x27;并非&#x27;</span>, <span class="string">&#x27;没&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 程度副词列表及其修饰因子</span></span><br><span class="line">degree_adverbs = &#123;</span><br><span class="line">    <span class="string">&#x27;很&#x27;</span>: <span class="number">1.2</span>, <span class="string">&#x27;非常&#x27;</span>: <span class="number">1.5</span>, <span class="string">&#x27;特别&#x27;</span>: <span class="number">1.3</span>,</span><br><span class="line">    <span class="string">&#x27;有点&#x27;</span>: <span class="number">0.8</span>, <span class="string">&#x27;几乎&#x27;</span>: <span class="number">0.7</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">analyze_sentiment_lexicon</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基于简单词典的情感分析函数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    score = <span class="number">0</span></span><br><span class="line">    words = text.split() <span class="comment"># 简单分词，实际应用需要更复杂的NLP分词器</span></span><br><span class="line">    </span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(words):</span><br><span class="line">        word = words[i]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 检查是否是程度副词</span></span><br><span class="line">        degree_modifier = <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> degree_adverbs:</span><br><span class="line">            degree_modifier = degree_adverbs[word]</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> i &lt; <span class="built_in">len</span>(words): <span class="comment"># 继续看下一个词是否是情感词</span></span><br><span class="line">                word = words[i]</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 程度副词在句末，不处理</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 检查是否是否定词</span></span><br><span class="line">        is_negated = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> negation_words:</span><br><span class="line">            is_negated = <span class="literal">True</span></span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> i &lt; <span class="built_in">len</span>(words): <span class="comment"># 继续看下一个词是否是情感词</span></span><br><span class="line">                word = words[i]</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 否定词在句末，不处理</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> sentiment_lexicon:</span><br><span class="line">            current_score = sentiment_lexicon[word] * degree_modifier</span><br><span class="line">            <span class="keyword">if</span> is_negated:</span><br><span class="line">                current_score *= -<span class="number">1</span> <span class="comment"># 否定词反转情感</span></span><br><span class="line">            score += current_score</span><br><span class="line">        </span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> score &gt; <span class="number">0.1</span>: <span class="comment"># 设定阈值，避免微小波动</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;积极&quot;</span>, score</span><br><span class="line">    <span class="keyword">elif</span> score &lt; -<span class="number">0.1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;消极&quot;</span>, score</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;中性&quot;</span>, score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="built_in">print</span>(analyze_sentiment_lexicon(<span class="string">&quot;这部电影 不错 非常 好看&quot;</span>)) <span class="comment"># (&#x27;积极&#x27;, 3.7)</span></span><br><span class="line"><span class="built_in">print</span>(analyze_sentiment_lexicon(<span class="string">&quot;这个服务 太 差 了&quot;</span>)) <span class="comment"># (&#x27;消极&#x27;, -1.5)</span></span><br><span class="line"><span class="built_in">print</span>(analyze_sentiment_lexicon(<span class="string">&quot;我很喜欢这个产品 但 有点 贵&quot;</span>)) <span class="comment"># (&#x27;积极&#x27;, 1.4) (注意，当前模型无法处理“但是”这种转折)</span></span><br><span class="line"><span class="built_in">print</span>(analyze_sentiment_lexicon(<span class="string">&quot;我觉得一般般&quot;</span>)) <span class="comment"># (&#x27;中性&#x27;, 0.0)</span></span><br><span class="line"><span class="built_in">print</span>(analyze_sentiment_lexicon(<span class="string">&quot;一点 都 不好&quot;</span>)) <span class="comment"># (&#x27;消极&#x27;, -1.0)</span></span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：上述代码是一个极度简化的示例，旨在说明原理。在实际应用中，需要更专业的分词工具（如 Jieba）、更全面的情感词典和更复杂的规则（如处理连词、句法结构等）。</p>
</li>
</ul>
<h3 id="基于机器学习的方法-Machine-Learning-Approaches">基于机器学习的方法 (Machine Learning Approaches)</h3>
<p>传统的机器学习方法将情感分析视为一个文本分类问题。给定一段文本，模型需要将其归类到预定义的类别（如积极、消极、中性）。</p>
<ul>
<li>
<p><strong>原理</strong>：</p>
<ol>
<li><strong>数据收集与标注</strong>：需要大量的人工标注文本数据集，每个文本都附带其对应的情感标签。</li>
<li><strong>特征工程（Feature Engineering）</strong>：这是传统机器学习方法的关键。由于机器学习模型无法直接处理文本，需要将文本转换为数值特征向量。常见的特征包括：
<ul>
<li><strong>词袋模型 (Bag-of-Words, BoW)</strong>：忽略词序和语法，只统计词语在文本中出现的频率。</li>
<li><strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong>：衡量一个词在文档中的重要性，它既考虑词频（TF），也考虑词在整个语料库中的稀有程度（IDF）。<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>TF-IDF</mtext><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>TF</mtext><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo><mo>×</mo><mtext>IDF</mtext><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{TF-IDF}(t,d) = \text{TF}(t,d) \times \text{IDF}(t)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">TF-IDF</span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">d</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">TF</span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">d</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">IDF</span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>IDF</mtext><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mfrac><mi>N</mi><mrow><mtext>DF</mtext><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>+</mo><mn>1</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{IDF}(t) = \log \frac{N}{\text{DF}(t)+1}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">IDF</span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2963em;vertical-align:-0.936em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">DF</span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">1</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 是词，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span> 是文档，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 是文档总数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>DF</mtext><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{DF}(t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">DF</span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span> 是包含词 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 的文档数。</li>
<li><strong>N-grams</strong>：考虑词语的序列，例如 unigrams（单个词）、bigrams（两个连续词）、trigrams（三个连续词）。N-grams 可以捕捉局部语境。</li>
<li><strong>词性标注 (Part-of-Speech, POS)</strong>：将词语标记为名词、动词、形容词等，因为形容词和副词通常承载更多情感信息。</li>
<li><strong>情感词典特征</strong>：基于词典统计文本中积极词、消极词的数量，或基于词典的情感得分。</li>
</ul>
</li>
<li><strong>模型训练</strong>：将特征向量作为输入，利用各种分类算法训练模型。</li>
<li><strong>模型评估</strong>：使用未见过的数据评估模型的性能，常用指标包括准确率、精确率、召回率、F1分数。</li>
</ol>
</li>
<li>
<p><strong>常用分类器</strong>：</p>
<ul>
<li>
<p><strong>朴素贝叶斯 (Naive Bayes)</strong>：</p>
<ul>
<li><strong>原理</strong>：基于贝叶斯定理和特征条件独立性假设。假设文本中每个词的出现是独立的，并且与其他词的出现无关。</li>
<li><strong>公式</strong>：分类器根据训练数据计算在给定文本中每个类别出现的概率，以及每个词在给定类别下出现的概率。对于一个文本 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">D = \{w_1, w_2, \dots, w_n\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>，我们要预测其类别 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>。<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>C</mi><mi mathvariant="normal">∣</mi><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>D</mi><mi mathvariant="normal">∣</mi><mi>C</mi><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><mi>C</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(C|D) = \frac{P(D|C)P(C)}{P(D)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
由于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span> 对于所有类别都是常数，我们只需要最大化 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>D</mi><mi mathvariant="normal">∣</mi><mi>C</mi><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><mi>C</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(D|C)P(C)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span>。<br>
根据条件独立性假设：<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>D</mi><mi mathvariant="normal">∣</mi><mi>C</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mi>n</mi></msub><mi mathvariant="normal">∣</mi><mi>C</mi><mo stretchy="false">)</mo><mo>≈</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mi>C</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(D|C) = P(w_1, w_2, \dots, w_n|C) \approx \prod_{i=1}^{n} P(w_i|C)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span></span></p>
因此，朴素贝叶斯分类的目标是找到使后验概率最大的类别：<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>C</mi><mo>^</mo></mover><mo>=</mo><mi>arg</mi><mo>⁡</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>C</mi></munder><mi>P</mi><mo stretchy="false">(</mo><mi>C</mi><mo stretchy="false">)</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mi>C</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{C} = \arg\max_{C} P(C) \prod_{i=1}^{n} P(w_i|C)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9468em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em;"><span style="top:-2.3557em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7443em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span></span></p>
</li>
<li><strong>优点</strong>：简单、高效、在小数据集上表现良好。</li>
<li><strong>缺点</strong>：条件独立性假设在实际中很难成立，可能影响准确性。</li>
</ul>
</li>
<li>
<p><strong>支持向量机 (Support Vector Machines, SVM)</strong>：</p>
<ul>
<li><strong>原理</strong>：寻找一个超平面，将不同类别的样本尽可能地分开，并使分类间隔最大化。</li>
<li><strong>优点</strong>：在处理高维数据时表现出色，泛化能力强。</li>
<li><strong>缺点</strong>：对核函数和参数选择敏感，训练时间可能较长。</li>
</ul>
</li>
<li>
<p><strong>逻辑回归 (Logistic Regression)</strong>：</p>
<ul>
<li><strong>原理</strong>：一种广义线性模型，通过将线性回归的结果通过Sigmoid函数映射到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(0,1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span> 之间，从而进行分类。</li>
<li><strong>公式</strong>：对于二分类问题，模型输出 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mo stretchy="false">(</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>β</mi><mi>n</mi></msub><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">p = P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \dots + \beta_n x_n)}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1∣</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.3137em;vertical-align:-0.4686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.5898em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8575em;"><span style="top:-2.8575em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5357em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:-0.0528em;margin-right:0.1em;"><span class="pstrut" style="height:2.6444em;"></span><span class="mord mtight">0</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2996em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:-0.0528em;margin-right:0.1em;"><span class="pstrut" style="height:2.6444em;"></span><span class="mord mtight">1</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2996em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:0em;margin-right:0.1em;"><span class="pstrut" style="height:2.6444em;"></span><span class="mord mtight">1</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2996em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="minner mtight">⋯</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2306em;"><span style="top:-2.3em;margin-left:-0.0528em;margin-right:0.1em;"><span class="pstrut" style="height:2.5em;"></span><span class="mord mathnormal mtight">n</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2306em;"><span style="top:-2.3em;margin-left:0em;margin-right:0.1em;"><span class="pstrut" style="height:2.5em;"></span><span class="mord mathnormal mtight">n</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>。</li>
<li><strong>优点</strong>：实现简单，易于解释，在许多文本分类任务中表现良好。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>优点</strong>：</p>
<ul>
<li><strong>从数据中学习</strong>：能够自动从标注数据中学习模式，而不需要人工定义规则。</li>
<li><strong>泛化能力</strong>：相较于基于词典的方法，传统机器学习模型通常具有更好的泛化能力，能够处理未出现在词典中的词语。</li>
<li><strong>性能</strong>：在许多标准数据集上取得了不错的性能。</li>
</ul>
</li>
<li>
<p><strong>缺点</strong>：</p>
<ul>
<li><strong>依赖特征工程</strong>：特征工程是耗时且需要专业知识的过程，特征的质量直接决定了模型的上限。</li>
<li><strong>无法捕捉深层语义</strong>：这些模型通常将词视为独立的特征，无法很好地捕捉词语之间的复杂语义关系和句子的深层含义（例如，词序、语法结构、多义词等）。</li>
<li><strong>需要大量标注数据</strong>：高质量的标注数据是训练有效机器学习模型的先决条件。</li>
</ul>
</li>
<li>
<p><strong>代码示例（Python）</strong>：<br>
下面是使用Scikit-learn进行情感分类的简化示例，采用TF-IDF特征和朴素贝叶斯分类器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟情感数据</span></span><br><span class="line">texts = [</span><br><span class="line">    <span class="string">&quot;这部电影太棒了，我非常喜欢。&quot;</span>, <span class="comment"># 积极</span></span><br><span class="line">    <span class="string">&quot;服务态度很好，产品质量也高。&quot;</span>, <span class="comment"># 积极</span></span><br><span class="line">    <span class="string">&quot;完全是浪费时间，糟糕的体验。&quot;</span>, <span class="comment"># 消极</span></span><br><span class="line">    <span class="string">&quot;我对此很失望，不推荐。&quot;</span>, <span class="comment"># 消极</span></span><br><span class="line">    <span class="string">&quot;还行吧，没什么亮点也没什么槽点。&quot;</span>, <span class="comment"># 中性</span></span><br><span class="line">    <span class="string">&quot;非常满意这次购物。&quot;</span>, <span class="comment"># 积极</span></span><br><span class="line">    <span class="string">&quot;食物一般般，环境吵闹。&quot;</span>, <span class="comment"># 消极</span></span><br><span class="line">    <span class="string">&quot;一个不错的选择，值得尝试。&quot;</span>, <span class="comment"># 积极</span></span><br><span class="line">    <span class="string">&quot;差评，绝不会再来。&quot;</span>, <span class="comment"># 消极</span></span><br><span class="line">    <span class="string">&quot;可以接受，但没有预期的好。&quot;</span>, <span class="comment"># 中性</span></span><br><span class="line">]</span><br><span class="line">labels = [<span class="string">&quot;积极&quot;</span>, <span class="string">&quot;积极&quot;</span>, <span class="string">&quot;消极&quot;</span>, <span class="string">&quot;消极&quot;</span>, <span class="string">&quot;中性&quot;</span>, <span class="string">&quot;积极&quot;</span>, <span class="string">&quot;消极&quot;</span>, <span class="string">&quot;积极&quot;</span>, <span class="string">&quot;消极&quot;</span>, <span class="string">&quot;中性&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将文本标签转换为数字标签</span></span><br><span class="line">label_map = &#123;<span class="string">&quot;积极&quot;</span>: <span class="number">0</span>, <span class="string">&quot;消极&quot;</span>: <span class="number">1</span>, <span class="string">&quot;中性&quot;</span>: <span class="number">2</span>&#125;</span><br><span class="line">numeric_labels = [label_map[label] <span class="keyword">for</span> label <span class="keyword">in</span> labels]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(texts, numeric_labels, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 特征工程：使用TF-IDF将文本转换为数值向量</span></span><br><span class="line"><span class="comment"># 这里使用停用词和最小词频过滤</span></span><br><span class="line">vectorizer = TfidfVectorizer(max_features=<span class="number">1000</span>, stop_words=<span class="literal">None</span>, min_df=<span class="number">1</span>)</span><br><span class="line">X_train_vec = vectorizer.fit_transform(X_train)</span><br><span class="line">X_test_vec = vectorizer.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 模型训练：使用多项式朴素贝叶斯分类器</span></span><br><span class="line">classifier = MultinomialNB()</span><br><span class="line">classifier.fit(X_train_vec, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 模型预测与评估</span></span><br><span class="line">y_pred = classifier.predict(X_test_vec)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;准确率: <span class="subst">&#123;accuracy_score(y_test, y_pred):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n分类报告:&quot;</span>)</span><br><span class="line"><span class="comment"># 为了更好的可读性，反向映射标签</span></span><br><span class="line">target_names = [name <span class="keyword">for</span> name, val <span class="keyword">in</span> <span class="built_in">sorted</span>(label_map.items(), key=<span class="keyword">lambda</span> item: item[<span class="number">1</span>])]</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred, target_names=target_names))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测新文本</span></span><br><span class="line">new_texts = [<span class="string">&quot;这简直是完美的体验！&quot;</span>, <span class="string">&quot;简直糟透了。&quot;</span>, <span class="string">&quot;还可以吧。&quot;</span>]</span><br><span class="line">new_texts_vec = vectorizer.transform(new_texts)</span><br><span class="line">new_predictions = classifier.predict(new_texts_vec)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n新文本预测结果:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> text, pred_label_idx <span class="keyword">in</span> <span class="built_in">zip</span>(new_texts, new_predictions):</span><br><span class="line">    predicted_label = [name <span class="keyword">for</span> name, val <span class="keyword">in</span> label_map.items() <span class="keyword">if</span> val == pred_label_idx][<span class="number">0</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;文本: &#x27;<span class="subst">&#123;text&#125;</span>&#x27; -&gt; 预测情感: <span class="subst">&#123;predicted_label&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：这是一个非常小的数据集，因此准确率可能很高，但在实际应用中，数据集需要大得多，并且需要更精细的预处理步骤。</p>
</li>
</ul>
<h2 id="第三部分：深度学习时代的情感分析">第三部分：深度学习时代的情感分析</h2>
<p>随着计算能力的提升和大规模数据集的涌现，深度学习在NLP领域取得了突破性进展，彻底改变了情感分析的面貌。深度学习模型能够自动学习文本的深层语义特征，避免了繁琐的手动特征工程，并显著提高了性能。</p>
<h3 id="词嵌入-Word-Embeddings">词嵌入 (Word Embeddings)</h3>
<p>在深度学习模型中，文本数据不能直接作为输入。词嵌入（Word Embeddings）是连接文本和深度学习模型的重要桥梁，它将词语映射到低维、连续的向量空间中。</p>
<ul>
<li>
<p><strong>背景</strong>：</p>
<ul>
<li>传统的One-hot编码将每个词表示为一个高维稀疏向量，向量维度等于词汇表大小，且无法捕捉词语之间的语义关系（例如，“国王”和“女王”在语义上是相似的，但它们的One-hot向量是完全正交的）。</li>
<li>词嵌入旨在解决One-hot编码的这些问题，使得语义相似的词在向量空间中距离更近。</li>
</ul>
</li>
<li>
<p><strong>原理</strong>：</p>
<ul>
<li>通过神经网络在大量文本数据上进行无监督训练，学习每个词的分布式表示。</li>
<li>这些向量能够捕捉词语的语义和句法信息，例如，“国王 - 男人 + 女人 ≈ 女王”这样的类比关系可以在向量空间中通过向量加减运算体现。</li>
</ul>
</li>
<li>
<p><strong>代表性模型</strong>：</p>
<ul>
<li><strong>Word2Vec (Mikolov et al., 2013)</strong>：
<ul>
<li><strong>原理</strong>：包含两种模型架构——Skip-gram和CBOW（Continuous Bag-of-Words）。
<ul>
<li><strong>Skip-gram</strong>：给定中心词，预测其上下文词。目标是最大化在给定中心词的情况下，其上下文词的出现概率。</li>
</ul>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo>=</mo><munder><mo>∑</mo><mrow><mi>w</mi><mo>∈</mo><mi>C</mi></mrow></munder><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></munder><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi>c</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L = \sum_{w \in C} \sum_{c \in context(w)} \log P(c|w)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.566em;vertical-align:-1.516em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8557em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3217em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.809em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight">co</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">x</span><span class="mord mathnormal mtight">t</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span></span></p>
其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>c</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msubsup><mi>v</mi><mi>c</mi><mrow><mo mathvariant="normal">′</mo><mi>T</mi></mrow></msubsup><msub><mi>v</mi><mi>w</mi></msub><mo stretchy="false">)</mo></mrow><mrow><msub><mo>∑</mo><mrow><msup><mi>w</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>∈</mo><mi>V</mi></mrow></msub><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msubsup><mi>v</mi><msup><mi>w</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mrow><mo mathvariant="normal">′</mo><mi>T</mi></mrow></msubsup><msub><mi>v</mi><mi>w</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(c|w) = \frac{\exp(v_c&#x27;^T v_w)}{\sum_{w&#x27; \in V} \exp(v_{w&#x27;}&#x27;^T v_w)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.8672em;vertical-align:-0.7136em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1536em;"><span style="top:-2.607em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2854em;"><span style="top:-2.2854em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.6068em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8496em;"><span style="top:-2.8496em;margin-right:0.1em;"><span class="pstrut" style="height:2.5556em;"></span><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3494em;"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8329em;"><span style="top:-2.1488em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.6068em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8496em;"><span style="top:-2.8496em;margin-right:0.1em;"><span class="pstrut" style="height:2.5556em;"></span><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span><span style="top:-2.9516em;margin-right:0.0714em;"><span class="pstrut" style="height:2.6068em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.458em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.5102em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9191em;"><span style="top:-2.214em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">c</span></span></span><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7136em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">v_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是词 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 的输入向量，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>v</mi><mi>c</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow><annotation encoding="application/x-tex">v_c&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9989em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span> 是词 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">c</span></span></span></span> 的输出向量，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span> 是词汇表。
<ul>
<li><strong>CBOW</strong>：给定上下文词，预测中心词。目标是最大化在给定上下文词的情况下，中心词的出现概率。</li>
</ul>
</li>
<li><strong>训练</strong>：通常采用负采样（Negative Sampling）或层级Softmax（Hierarchical Softmax）来提高训练效率。</li>
</ul>
</li>
<li><strong>GloVe (Global Vectors for Word Representation)</strong>：
<ul>
<li>结合了全局矩阵分解（如LSA）和局部上下文窗口（如Word2Vec）的优点，通过构建词语的共现矩阵来学习词向量。</li>
</ul>
</li>
<li><strong>FastText</strong>：
<ul>
<li>将词分解为字符级别的N-grams，因此可以处理OOV（Out-Of-Vocabulary）词，并对形态学丰富的语言（如中文）表现良好。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>优点</strong>：</p>
<ul>
<li><strong>捕捉词义和语义关系</strong>：向量空间中的距离反映了词语的语义相似性。</li>
<li><strong>降维</strong>：将高维稀疏表示转换为低维稠密表示。</li>
<li><strong>泛化能力</strong>：训练好的词嵌入可以作为其他NLP任务的预训练层，提高模型性能。</li>
</ul>
</li>
</ul>
<h3 id="循环神经网络-Recurrent-Neural-Networks-RNN-及其变体">循环神经网络 (Recurrent Neural Networks - RNN) 及其变体</h3>
<p>情感分析的文本是序列数据，词语之间存在顺序和依赖关系。RNNs因其处理序列数据的能力而成为情感分析的有力工具。</p>
<ul>
<li>
<p><strong>为什么需要RNNs</strong>：传统的前馈神经网络无法捕捉序列中的时间依赖性或上下文信息。RNNs通过引入循环连接，使信息可以在序列中传递和保留。</p>
</li>
<li>
<p><strong>基本RNN</strong>：</p>
<ul>
<li><strong>结构</strong>：在每个时间步 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span>，RNN会接收当前输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和前一个时间步的隐藏状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9028em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span>，然后输出当前隐藏状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和可选的输出 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">y_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</li>
<li><strong>公式</strong>：<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msub><mi>W</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>W</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><msub><mi>b</mi><mi>h</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_t = \sigma(W_{hh}h_{t-1} + W_{xh}x_t + b_h)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">hh</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mord mathnormal mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><msub><mi>W</mi><mrow><mi>h</mi><mi>y</mi></mrow></msub><msub><mi>h</mi><mi>t</mi></msub><mo>+</mo><msub><mi>b</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">y_t = W_{hy}h_t + b_y
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span></p>
其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span> 是激活函数（如 tanh），<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span> 是权重矩阵，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span> 是偏置向量。</li>
<li><strong>局限性</strong>：
<ul>
<li><strong>梯度消失/爆炸问题</strong>：在处理长序列时，反向传播过程中梯度会指数级衰减或增长，导致模型难以学习到长期依赖关系。</li>
<li><strong>只能捕捉短距离依赖</strong>：由于梯度问题，RNNs通常难以记住很久以前的信息。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>长短期记忆网络 (Long Short-Term Memory - LSTM)</strong>：</p>
<ul>
<li><strong>背景</strong>：为解决传统RNN的长期依赖问题而设计。</li>
<li><strong>原理</strong>：引入了“门（Gate）”机制和“细胞状态（Cell State）”。细胞状态类似于信息高速公路，允许信息在序列中畅通无阻地流动，而门控机制（遗忘门、输入门、输出门）则负责控制信息在细胞状态中的流动、增加和移除。</li>
<li><strong>门控机制简述</strong>：
<ul>
<li><strong>遗忘门 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">f_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>)</strong>：决定从细胞状态中丢弃哪些信息。</li>
<li><strong>输入门 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>i</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">i_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">i</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>)</strong>：决定向细胞状态中添加哪些新信息。</li>
<li><strong>输出门 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>o</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">o_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>)</strong>：决定基于当前细胞状态输出什么信息。</li>
</ul>
</li>
<li><strong>优点</strong>：有效缓解了梯度消失问题，能够学习和记忆长距离依赖关系，在许多NLP任务中表现出色。</li>
</ul>
</li>
<li>
<p><strong>门控循环单元 (Gated Recurrent Unit - GRU)</strong>：</p>
<ul>
<li><strong>原理</strong>：LSTM的简化版，将遗忘门和输入门合并为更新门，并将细胞状态和隐藏状态合并。参数更少，训练更快，但在某些任务上性能与LSTM相当。</li>
</ul>
</li>
<li>
<p><strong>双向RNN/LSTM/GRU (Bidirectional RNN/LSTM/GRU)</strong>：</p>
<ul>
<li><strong>原理</strong>：在一个序列上同时运行两个独立的RNNs——一个从前往后处理，另一个从后往前处理。这样，每个时间步的隐藏状态不仅包含之前的信息，也包含之后的信息，从而捕捉到更完整的上下文语义。对于情感分析，这尤为重要，因为情感词的极性可能受到其前后词语的影响。</li>
</ul>
</li>
<li>
<p><strong>基于RNN/LSTM的情感分析模型结构</strong>：</p>
<ol>
<li><strong>词嵌入层 (Embedding Layer)</strong>：将输入的文本词语转换为预训练或随机初始化的词向量。</li>
<li><strong>RNN/LSTM/GRU 层</strong>：处理词向量序列，提取序列特征，捕捉上下文依赖。通常使用双向LSTM或GRU以获得更丰富的上下文信息。</li>
<li><strong>池化层 (Pooling Layer)</strong>（可选）：如最大池化或平均池化，将RNN/LSTM输出的序列特征聚合为固定长度的向量。</li>
<li><strong>全连接层 (Dense Layer)</strong>：接收池化后的特征向量，进行非线性变换。</li>
<li><strong>Softmax 输出层</strong>：将全连接层的输出映射到各个情感类别的概率分布。</li>
</ol>
</li>
<li>
<p><strong>代码示例（Python with Keras）</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Embedding, LSTM, Dense, Bidirectional, Dropout</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟情感数据</span></span><br><span class="line">texts = [</span><br><span class="line">    <span class="string">&quot;这部电影太棒了，我非常喜欢。&quot;</span>, <span class="comment"># 积极</span></span><br><span class="line">    <span class="string">&quot;服务态度很好，产品质量也高。&quot;</span>, <span class="comment"># 积极</span></span><br><span class="line">    <span class="string">&quot;完全是浪费时间，糟糕的体验。&quot;</span>, <span class="comment"># 消极</span></span><br><span class="line">    <span class="string">&quot;我对此很失望，不推荐。&quot;</span>, <span class="comment"># 消极</span></span><br><span class="line">    <span class="string">&quot;还行吧，没什么亮点也没什么槽点。&quot;</span>, <span class="comment"># 中性</span></span><br><span class="line">    <span class="string">&quot;非常满意这次购物。&quot;</span>, <span class="comment"># 积极</span></span><br><span class="line">    <span class="string">&quot;食物一般般，环境吵闹。&quot;</span>, <span class="comment"># 消极</span></span><br><span class="line">    <span class="string">&quot;一个不错的选择，值得尝试。&quot;</span>, <span class="comment"># 积极</span></span><br><span class="line">    <span class="string">&quot;差评，绝不会再来。&quot;</span>, <span class="comment"># 消极</span></span><br><span class="line">    <span class="string">&quot;可以接受，但没有预期的好。&quot;</span>, <span class="comment"># 中性</span></span><br><span class="line">]</span><br><span class="line">labels = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>] <span class="comment"># 0:积极, 1:消极, 2:中性</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 文本预处理与序列化</span></span><br><span class="line">tokenizer = Tokenizer(num_words=<span class="literal">None</span>, oov_token=<span class="string">&quot;&lt;unk&gt;&quot;</span>) <span class="comment"># 不限制词数，处理未知词</span></span><br><span class="line">tokenizer.fit_on_texts(texts)</span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line">vocab_size = <span class="built_in">len</span>(word_index) + <span class="number">1</span> <span class="comment"># 词汇表大小</span></span><br><span class="line"></span><br><span class="line">sequences = tokenizer.texts_to_sequences(texts)</span><br><span class="line">max_len = <span class="built_in">max</span>([<span class="built_in">len</span>(seq) <span class="keyword">for</span> seq <span class="keyword">in</span> sequences]) <span class="comment"># 获取最长序列长度</span></span><br><span class="line">padded_sequences = pad_sequences(sequences, maxlen=max_len, padding=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备标签</span></span><br><span class="line">labels = np.array(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集（小数据量，仅为演示）</span></span><br><span class="line"><span class="comment"># 通常需要更多数据，这里简化</span></span><br><span class="line">X_train, y_train = padded_sequences, labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 构建Bi-LSTM模型</span></span><br><span class="line">embedding_dim = <span class="number">100</span> <span class="comment"># 词嵌入维度</span></span><br><span class="line"></span><br><span class="line">model = Sequential([</span><br><span class="line">    Embedding(vocab_size, embedding_dim, input_length=max_len), <span class="comment"># 嵌入层</span></span><br><span class="line">    Bidirectional(LSTM(<span class="number">64</span>, return_sequences=<span class="literal">False</span>)), <span class="comment"># 双向LSTM层，返回最终状态</span></span><br><span class="line">    Dropout(<span class="number">0.5</span>), <span class="comment"># 防止过拟合</span></span><br><span class="line">    Dense(<span class="number">32</span>, activation=<span class="string">&#x27;relu&#x27;</span>), <span class="comment"># 全连接层</span></span><br><span class="line">    Dense(<span class="number">3</span>, activation=<span class="string">&#x27;softmax&#x27;</span>) <span class="comment"># 输出层，3个情感类别</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 训练模型</span></span><br><span class="line"><span class="comment"># 实际应用中需要更多epoch和验证集</span></span><br><span class="line">model.fit(X_train, y_train, epochs=<span class="number">20</span>, batch_size=<span class="number">2</span>, verbose=<span class="number">0</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 预测新文本</span></span><br><span class="line">new_texts = [<span class="string">&quot;这简直是完美的体验！&quot;</span>, <span class="string">&quot;简直糟透了。&quot;</span>, <span class="string">&quot;还可以吧。&quot;</span>, <span class="string">&quot;真让人高兴！&quot;</span>]</span><br><span class="line">new_sequences = tokenizer.texts_to_sequences(new_texts)</span><br><span class="line">new_padded_sequences = pad_sequences(new_sequences, maxlen=max_len, padding=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line"></span><br><span class="line">predictions = model.predict(new_padded_sequences)</span><br><span class="line">predicted_labels = np.argmax(predictions, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">label_map_inv = &#123;<span class="number">0</span>: <span class="string">&quot;积极&quot;</span>, <span class="number">1</span>: <span class="string">&quot;消极&quot;</span>, <span class="number">2</span>: <span class="string">&quot;中性&quot;</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n新文本预测结果:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> text, pred_idx <span class="keyword">in</span> <span class="built_in">zip</span>(new_texts, predicted_labels):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;文本: &#x27;<span class="subst">&#123;text&#125;</span>&#x27; -&gt; 预测情感: <span class="subst">&#123;label_map_inv[pred_idx]&#125;</span> (概率: <span class="subst">&#123;predictions[<span class="built_in">list</span>(new_texts).index(text)][pred_idx]:<span class="number">.2</span>f&#125;</span>)&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：这个代码示例同样使用了极小的数据集和简单的模型训练，仅用于概念演示。在真实场景中，需要更大的数据集、更复杂的预处理（如中文分词）、更长的训练时间以及超参数调优。</p>
</li>
</ul>
<h3 id="卷积神经网络-Convolutional-Neural-Networks-CNN-for-Text">卷积神经网络 (Convolutional Neural Networks - CNN) for Text</h3>
<p>CNNs在图像处理领域取得了巨大成功，但它们也被有效地应用于文本分类任务。尽管文本是序列数据，CNN可以通过卷积核提取文本的局部特征（类似于N-grams）。</p>
<ul>
<li>
<p><strong>如何用于文本</strong>：</p>
<ol>
<li><strong>词嵌入层</strong>：将文本转换为词嵌入矩阵。</li>
<li><strong>卷积层</strong>：使用不同尺寸的卷积核（滤波器）在词嵌入矩阵上滑动，提取局部特征（例如，表示一个词或短语）。每个卷积核可以看作一个模式检测器。</li>
<li><strong>池化层</strong>：通常使用最大池化，从每个卷积核的输出中提取最重要的特征（例如，最高激活值），从而得到一个固定长度的特征向量。</li>
<li><strong>全连接层和Softmax</strong>：将池化后的特征向量输入全连接层，最后通过Softmax层进行分类。</li>
</ol>
</li>
<li>
<p><strong>优点</strong>：</p>
<ul>
<li><strong>并行计算</strong>：卷积操作可以并行进行，训练效率高。</li>
<li><strong>捕捉局部模式</strong>：擅长捕捉文本中的局部特征，如词组、短语等。</li>
</ul>
</li>
<li>
<p><strong>缺点</strong>：</p>
<ul>
<li>不如RNNs（尤其是LSTM/GRU）擅长捕捉长距离依赖关系和词序信息，因为卷积核的感受野有限。但在实践中，多层卷积或使用大尺寸卷积核可以在一定程度上弥补。</li>
</ul>
</li>
</ul>
<h3 id="注意力机制-Attention-Mechanism">注意力机制 (Attention Mechanism)</h3>
<p>随着序列长度的增加，RNNs在编码长序列信息时容易丢失细节。注意力机制的出现，极大地提升了模型处理长序列和关注重要信息的能力。</p>
<ul>
<li><strong>背景</strong>：在编码-解码（Encoder-Decoder）模型中，传统方法是编码器将整个输入序列压缩成一个固定长度的上下文向量，解码器再基于此向量生成输出。这在长序列时成为瓶颈。</li>
<li><strong>原理</strong>：注意力机制允许模型在生成输出时，动态地“关注”输入序列中与当前输出最相关的部分。它为输入序列中的每个元素计算一个权重（注意力分数），表示其对当前任务的重要性。这些权重用于加权求和，生成一个“上下文向量”，该向量更能代表与任务相关的信息。</li>
<li><strong>Self-Attention（自注意力机制）</strong>：
<ul>
<li>这是Transformer模型的核心组成部分。</li>
<li>它允许模型在处理序列中的一个元素时，同时关注序列中的所有其他元素，并根据它们之间的相关性分配权重。这意味着模型可以发现任意两个词之间的语义关系，而不仅仅是相邻词。</li>
</ul>
</li>
<li><strong>如何增强情感分析</strong>：通过注意力机制，模型可以自动识别文本中对情感判断贡献最大的词语或短语（例如，“非常棒”、“糟糕透顶”），并给予它们更高的权重，从而提高情感分析的准确性和可解释性。</li>
</ul>
<h3 id="Transformer-模型与预训练语言模型">Transformer 模型与预训练语言模型</h3>
<p>Transformer模型是近年来NLP领域最具颠覆性的创新，它完全抛弃了RNN和CNN的循环或卷积结构，完全依赖注意力机制（特别是多头自注意力）。在此基础上发展的预训练语言模型（Pre-trained Language Models, PLMs）将NLP带入了一个全新的时代。</p>
<ul>
<li>
<p><strong>Transformer 模型</strong>：</p>
<ul>
<li><strong>“Attention Is All You Need”</strong>：Vaswani等人在2017年的论文中提出了Transformer，其核心在于多头自注意力（Multi-head Self-Attention）和位置编码（Positional Encoding）。</li>
<li><strong>结构</strong>：由编码器（Encoder）和解码器（Decoder）组成，每个部分都包含多层自注意力机制和前馈网络。</li>
<li><strong>自注意力计算公式</strong>：<br>
给定查询（Query）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span>、键（Key）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 和值（Value）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>，注意力权重计算如下：<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4483em;vertical-align:-0.93em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em;"><span style="top:-2.2528em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p>
其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是键向量的维度，用于缩放，防止内积过大导致Softmax梯度过小。</li>
<li><strong>位置编码</strong>：由于Transformer不包含循环或卷积，无法捕捉词序信息，因此引入位置编码来为每个词在序列中的位置提供信息。</li>
<li><strong>优点</strong>：
<ul>
<li><strong>并行化能力强</strong>：不再是顺序处理，大大加快训练速度。</li>
<li><strong>捕捉长距离依赖</strong>：自注意力机制可以一步到位地连接序列中任意位置的词语，克服了RNNs的长期依赖问题。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>预训练语言模型 (Pre-trained Language Models - PLMs)</strong>：</p>
<ul>
<li><strong>背景</strong>：Transformer的强大能力使得在大规模无标注语料上进行“预训练”成为可能。模型通过预测下一个词（如GPT）或掩码词（如BERT）来学习语言的通用表示。</li>
<li><strong>“预训练+微调 (Pre-train + Fine-tune)”范式</strong>：
<ol>
<li><strong>预训练</strong>：在大规模无标注文本语料（如维基百科、书籍）上进行自监督学习，让模型学习语言的语法、语义和通用知识。这一阶段通常需要巨大的计算资源。</li>
<li><strong>微调</strong>：将预训练模型加载到下游的特定任务（如情感分析）上，在小规模标注数据集上进行有监督训练，调整模型参数以适应特定任务。这一阶段相对计算量小。</li>
</ol>
</li>
<li><strong>代表性模型</strong>：
<ul>
<li><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>：
<ul>
<li><strong>原理</strong>：Google于2018年发布，是双向的Transformer编码器。通过两个无监督任务进行预训练：
<ul>
<li><strong>掩码语言模型 (Masked Language Model, MLM)</strong>：随机遮蔽输入中15%的词，然后预测被遮蔽的词。这使得BERT能够学习到词语在双向上下文中的表示。</li>
<li><strong>下一句预测 (Next Sentence Prediction, NSP)</strong>：判断两个句子是否是原文中连续的。这使得BERT能够学习句子间的关系。</li>
</ul>
</li>
<li><strong>用于情感分析</strong>：在BERT的输出中，通常取第一个特殊标记 <code>[CLS]</code> 的输出向量，将其送入一个简单的全连接分类层，然后进行微调。由于BERT学习到了丰富的语言上下文信息，微调后在情感分析任务上表现出色。</li>
</ul>
</li>
<li><strong>RoBERTa (Robustly optimized BERT approach)</strong>：Facebook AI改进了BERT的训练策略，例如更大的批次大小、更长的训练时间、动态掩码等，取得了更好的性能。</li>
<li><strong>XLNet</strong>：结合了BERT的双向上下文能力和Autoregressive模型的优点。</li>
<li><strong>GPT 系列 (Generative Pre-trained Transformer)</strong>：
<ul>
<li>由OpenAI开发，是基于Transformer解码器的自回归模型。</li>
<li>主要用于文本生成任务（预测下一个词），但也展现出强大的零样本（Zero-shot）和少样本（Few-shot）学习能力。虽然主要用于生成，但通过适当的提示工程，也可用于情感分类。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>优点</strong>：</p>
<ul>
<li><strong>强大的上下文理解能力</strong>：得益于双向Transformer和大规模预训练，能够捕捉词语在复杂语境中的深层语义。</li>
<li><strong>迁移学习</strong>：预训练模型学习到的通用语言知识可以有效地迁移到各种下游任务，即使是小数据集也能取得好效果。</li>
<li><strong>State-of-the-art性能</strong>：在各种NLP基准测试中取得了领先的性能。</li>
<li><strong>降低特征工程负担</strong>：模型自动学习特征，大大简化了开发流程。</li>
</ul>
</li>
<li>
<p><strong>缺点</strong>：</p>
<ul>
<li><strong>计算资源大</strong>：预训练阶段需要巨大的计算资源（GPU/TPU）。</li>
<li><strong>模型复杂</strong>：模型参数量巨大，推理速度相对较慢。</li>
<li><strong>可解释性差</strong>：作为“黑箱”模型，难以直观理解其决策过程。</li>
<li><strong>对偏见敏感</strong>：如果训练数据中存在偏见，模型可能会习得并放大这些偏见。</li>
</ul>
</li>
<li>
<p><strong>代码示例（Python with Hugging Face Transformers）</strong>：<br>
使用预训练的BERT模型进行情感分析的微调。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, precision_recall_fscore_support</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 模拟情感数据</span></span><br><span class="line">texts = [</span><br><span class="line">    <span class="string">&quot;这部电影太棒了，我非常喜欢。&quot;</span>, <span class="comment"># 积极</span></span><br><span class="line">    <span class="string">&quot;服务态度很好，产品质量也高。&quot;</span>, <span class="comment"># 积极</span></span><br><span class="line">    <span class="string">&quot;完全是浪费时间，糟糕的体验。&quot;</span>, <span class="comment"># 消极</span></span><br><span class="line">    <span class="string">&quot;我对此很失望，不推荐。&quot;</span>, <span class="comment"># 消极</span></span><br><span class="line">    <span class="string">&quot;还行吧，没什么亮点也没什么槽点。&quot;</span>, <span class="comment"># 中性</span></span><br><span class="line">    <span class="string">&quot;非常满意这次购物。&quot;</span>, <span class="comment"># 积极</span></span><br><span class="line">    <span class="string">&quot;食物一般般，环境吵闹。&quot;</span>, <span class="comment"># 消极</span></span><br><span class="line">    <span class="string">&quot;一个不错的选择，值得尝试。&quot;</span>, <span class="comment"># 积极</span></span><br><span class="line">    <span class="string">&quot;差评，绝不会再来。&quot;</span>, <span class="comment"># 消极</span></span><br><span class="line">    <span class="string">&quot;可以接受，但没有预期的好。&quot;</span>, <span class="comment"># 中性</span></span><br><span class="line">]</span><br><span class="line">labels = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>] <span class="comment"># 0:积极, 1:消极, 2:中性</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 加载预训练模型和分词器</span></span><br><span class="line"><span class="comment"># 选择一个中文预训练模型，例如 &#x27;bert-base-chinese&#x27;</span></span><br><span class="line">model_name = <span class="string">&quot;bert-base-chinese&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=<span class="number">3</span>) <span class="comment"># 3个类别</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 数据集准备</span></span><br><span class="line"><span class="comment"># Hugging Face `datasets`库非常方便</span></span><br><span class="line">data = &#123;<span class="string">&quot;text&quot;</span>: texts, <span class="string">&quot;label&quot;</span>: labels&#125;</span><br><span class="line">dataset = Dataset.from_dict(data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_function</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&quot;text&quot;</span>], padding=<span class="string">&quot;max_length&quot;</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">tokenized_dataset = dataset.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集 (这里为了演示简化，实际中应有更多数据)</span></span><br><span class="line">train_dataset = tokenized_dataset.shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">8</span>)) <span class="comment"># 8个样本作为训练</span></span><br><span class="line">eval_dataset = tokenized_dataset.shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">8</span>, <span class="number">10</span>)) <span class="comment"># 2个样本作为验证</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 定义评估指标</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_metrics</span>(<span class="params">p</span>):</span><br><span class="line">    predictions, labels = p</span><br><span class="line">    predictions = np.argmax(predictions, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;accuracy&quot;</span>: accuracy_score(labels, predictions)&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 配置训练参数</span></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">&quot;./results&quot;</span>,</span><br><span class="line">    num_train_epochs=<span class="number">10</span>,  <span class="comment"># 训练轮次</span></span><br><span class="line">    per_device_train_batch_size=<span class="number">2</span>, <span class="comment"># 批处理大小</span></span><br><span class="line">    per_device_eval_batch_size=<span class="number">2</span>,</span><br><span class="line">    warmup_steps=<span class="number">100</span>,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">    logging_dir=<span class="string">&quot;./logs&quot;</span>,</span><br><span class="line">    logging_steps=<span class="number">10</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">&quot;epoch&quot;</span>, <span class="comment"># 每个epoch评估一次</span></span><br><span class="line">    save_strategy=<span class="string">&quot;epoch&quot;</span>, <span class="comment"># 每个epoch保存一次</span></span><br><span class="line">    load_best_model_at_end=<span class="literal">True</span>, <span class="comment"># 训练结束后加载最佳模型</span></span><br><span class="line">    metric_for_best_model=<span class="string">&quot;accuracy&quot;</span>, <span class="comment"># 以准确率作为最佳模型指标</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 创建Trainer并开始训练</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=train_dataset,</span><br><span class="line">    eval_dataset=eval_dataset,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 评估模型</span></span><br><span class="line">eval_results = trainer.evaluate()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\n评估结果: <span class="subst">&#123;eval_results&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. 使用微调后的模型进行预测</span></span><br><span class="line">label_map_inv = &#123;<span class="number">0</span>: <span class="string">&quot;积极&quot;</span>, <span class="number">1</span>: <span class="string">&quot;消极&quot;</span>, <span class="number">2</span>: <span class="string">&quot;中性&quot;</span>&#125;</span><br><span class="line">test_texts = [<span class="string">&quot;这手机真棒！&quot;</span>, <span class="string">&quot;餐厅服务很糟糕，饭菜也很差。&quot;</span>, <span class="string">&quot;感觉一般般。&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_sentiment</span>(<span class="params">texts_to_predict</span>):</span><br><span class="line">    inputs = tokenizer(texts_to_predict, padding=<span class="string">&quot;max_length&quot;</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">128</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">    <span class="comment"># 将输入移动到模型所在的设备 (CPU 或 GPU)</span></span><br><span class="line">    inputs = &#123;k: v.to(model.device) <span class="keyword">for</span> k, v <span class="keyword">in</span> inputs.items()&#125;</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">    logits = outputs.logits</span><br><span class="line">    probabilities = torch.softmax(logits, dim=<span class="number">1</span>).cpu().numpy()</span><br><span class="line">    predicted_class_ids = torch.argmax(logits, dim=<span class="number">1</span>).cpu().numpy()</span><br><span class="line">    </span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> i, text <span class="keyword">in</span> <span class="built_in">enumerate</span>(texts_to_predict):</span><br><span class="line">        pred_label = label_map_inv[predicted_class_ids[i]]</span><br><span class="line">        prob_score = probabilities[i][predicted_class_ids[i]]</span><br><span class="line">        results.append(<span class="string">f&quot;文本: &#x27;<span class="subst">&#123;text&#125;</span>&#x27; -&gt; 预测情感: <span class="subst">&#123;pred_label&#125;</span> (置信度: <span class="subst">&#123;prob_score:<span class="number">.2</span>f&#125;</span>)&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n新文本预测结果:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> predict_sentiment(test_texts):</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：在真实场景中，训练Transformer模型需要更强大的硬件（GPU），更大的数据集，更长时间的训练，以及细致的超参数调整才能达到良好效果。此示例仅为概念演示。</p>
</li>
</ul>
<h2 id="第四部分：情感分析的进阶话题与挑战">第四部分：情感分析的进阶话题与挑战</h2>
<p>情感分析并非止步于简单的极性判断。随着研究的深入和应用场景的复杂化，一系列进阶话题和更深层次的挑战浮出水面。</p>
<h3 id="方面级情感分析-Aspect-Based-Sentiment-Analysis-ABSA">方面级情感分析 (Aspect-Based Sentiment Analysis - ABSA)</h3>
<p>文档级和句子级情感分析只能给出文本的总体情感，但很多时候，用户需要更细致的洞察。例如，一条手机评论可能说“屏幕很棒，但电池续航很差”，整体可能是中性或略偏消极，但这掩盖了用户对屏幕的积极态度和对电池的消极态度。方面级情感分析（ABSA）旨在解决这个问题。</p>
<ul>
<li>
<p><strong>定义</strong>：ABSA的目标是识别文本中表达情感的具体实体或方面，并对这些方面进行情感分类。它通常涉及两个子任务：</p>
<ol>
<li><strong>方面词抽取（Aspect Term Extraction, ATE）</strong>：从文本中识别出表示产品或服务具体方面的词语或短语（如“屏幕”、“电池续航”、“服务态度”）。</li>
<li><strong>方面情感分类（Aspect Sentiment Classification, ASC）</strong>：对每个抽取的方面词，判断其在文本中的情感极性。</li>
</ol>
</li>
<li>
<p><strong>重要性</strong>：</p>
<ul>
<li>提供更精细、更有价值的客户洞察。</li>
<li>帮助企业精准定位产品优缺点，指导产品改进和营销策略。</li>
</ul>
</li>
<li>
<p><strong>挑战</strong>：</p>
<ul>
<li><strong>方面词识别</strong>：方面词可能以多种形式出现（单字、短语、同义词），且可能与普通词语混淆。</li>
<li><strong>方面与情感的对齐</strong>：一个情感词可能影响多个方面，或者一个方面可能受到多个情感词的影响。</li>
<li><strong>隐式方面</strong>：有些方面并未直接提及，而是通过上下文暗示。</li>
</ul>
</li>
<li>
<p><strong>常用方法</strong>：</p>
<ul>
<li><strong>基于规则或词典</strong>：预定义方面词典和规则。</li>
<li><strong>序列标注模型</strong>：将ATE视为序列标注问题（如B-Aspect, I-Aspect, O），常使用Bi-LSTM-CRF。</li>
<li><strong>深度学习模型</strong>：结合词嵌入、Bi-LSTM、注意力机制等，在模型中同时考虑方面信息和情感信息。例如，使用Attention机制让模型在判断某个方面的情感时，更多地关注文本中与该方面相关的词语。</li>
<li><strong>图神经网络 (Graph Neural Networks, GNN)</strong>：将句子的句法依赖关系构建成图，利用GNN在图上传播信息，从而更好地捕捉方面与情感词之间的关系。</li>
</ul>
</li>
</ul>
<h3 id="多模态情感分析-Multimodal-Sentiment-Analysis">多模态情感分析 (Multimodal Sentiment Analysis)</h3>
<p>人类表达情感的方式是多样的，除了文字，还包括语音语调、面部表情、肢体语言等。多模态情感分析旨在结合来自不同模态的信息，以获得更全面、更准确的情感理解。</p>
<ul>
<li>
<p><strong>模态类型</strong>：</p>
<ul>
<li><strong>文本 (Text)</strong>：文字内容。</li>
<li><strong>语音 (Audio)</strong>：语速、音调、音量、韵律等。</li>
<li><strong>视觉 (Visual)</strong>：面部表情、眼神、手势、姿态等。</li>
</ul>
</li>
<li>
<p><strong>挑战</strong>：</p>
<ul>
<li><strong>数据融合</strong>：如何有效地融合来自不同模态的异构数据是关键。常见方法包括特征级融合（将各模态特征拼接）和决策级融合（各模态独立预测后，再进行投票或加权）。</li>
<li><strong>跨模态学习</strong>：不同模态之间可能存在互补或矛盾的信息，如何学习它们之间的复杂关系。</li>
<li><strong>模态缺失</strong>：在实际应用中，某一模态的数据可能缺失或质量不佳。</li>
</ul>
</li>
</ul>
<h3 id="跨领域情感分析-Cross-Domain-Sentiment-Analysis">跨领域情感分析 (Cross-Domain Sentiment Analysis)</h3>
<p>情感分析模型通常在一个特定领域（如电影评论）上训练，然后应用于同一领域的数据。然而，如果将模型直接应用于不同领域（如电子产品评论），性能可能会显著下降，因为不同领域有不同的词汇、表达习惯和情感倾向。跨领域情感分析旨在解决这个问题。</p>
<ul>
<li><strong>目标</strong>：在源领域（有大量标注数据）训练一个模型，使其能够在目标领域（标注数据稀缺或没有标注数据）上表现良好。</li>
<li><strong>挑战</strong>：
<ul>
<li><strong>领域差异</strong>：词汇分布、情感极性、句法结构等方面的差异。</li>
<li><strong>数据稀疏性</strong>：目标领域标注数据不足。</li>
</ul>
</li>
<li><strong>常用方法</strong>：
<ul>
<li><strong>领域适应（Domain Adaptation）</strong>：利用迁移学习的思想，通过最小化源领域和目标领域之间的分布差异，使模型能够更好地泛化到目标领域。</li>
<li><strong>无监督领域适应</strong>：无需目标领域的标注数据，仅利用目标领域的无标注数据进行适应。</li>
<li><strong>对抗性训练</strong>：训练一个判别器来区分源领域和目标领域的数据，同时训练特征提取器来欺骗判别器，使其无法区分数据来源，从而提取领域不变的特征。</li>
</ul>
</li>
</ul>
<h3 id="情感分析的挑战与未来方向">情感分析的挑战与未来方向</h3>
<p>尽管取得了巨大进步，情感分析仍面临许多复杂而开放的挑战：</p>
<ul>
<li><strong>讽刺、反讽与双关语的识别</strong>：这是长期以来困扰情感分析的难题，需要更深层次的语境理解和常识推理能力。</li>
<li><strong>隐式情感与情绪</strong>：如何从描述性文本中推断出隐含的情感，例如“我昨晚又失眠了”，隐含着负面情绪。</li>
<li><strong>多语言情感分析</strong>：不同语言有不同的语法、文化背景和情感表达方式，跨语言的情感分析仍然具有挑战性。</li>
<li><strong>可解释性与鲁棒性</strong>：随着深度学习模型的复杂度增加，模型的“黑箱”特性使得其决策过程难以解释。提高模型的可解释性和对对抗性攻击的鲁棒性是重要研究方向。</li>
<li><strong>小样本学习与零样本学习</strong>：在特定领域或新兴主题中，标注数据往往非常稀缺。如何通过少量甚至零样本进行有效的情感分析，是未来研究的热点。</li>
<li><strong>伦理与偏见</strong>：训练数据可能反映社会中的刻板印象和偏见，导致模型在情感判断上出现歧视。如何识别、量化和消除模型中的偏见，确保其公平性和公正性，是情感分析乃至整个AI领域的重要伦理挑战。</li>
</ul>
<h3 id="结论">结论</h3>
<p>情感分析，作为自然语言处理领域的一个核心分支，已经从早期的规则和统计方法，发展到如今由深度学习和大规模预训练模型驱动的强大技术。我们见证了从简单的极性判断到方面级情感分析，再到多模态和跨领域情感理解的飞跃。Transformer模型和BERT等预训练语言模型的出现，更是将情感分析的性能推向了新的高度，使得计算机能够以前所未有的深度理解人类的情感。</p>
<p>情感分析的应用价值毋庸置疑，它正在改变着企业理解客户、政府洞察民意、乃至个人理解彼此的方式。然而，人类语言的丰富性和复杂性决定了情感分析仍然是一个充满挑战且不断演进的领域。讽刺、隐喻、情感的细微差别以及数据中的偏见，都要求我们不断探索更智能、更鲁棒、更公平的模型。</p>
<p>未来，情感分析将继续朝着更精细化、更智能化、更具可解释性的方向发展。结合多模态信息、融入常识知识、探索更有效的小样本和零样本学习方法，以及解决模型中的偏见问题，将是这一领域研究的关键焦点。</p>
<p>作为技术爱好者，我们有幸生活在一个充满无限可能的数据时代。情感分析不仅仅是算法和模型的堆砌，它更是连接人类情感与机器智能的桥梁。我鼓励大家保持好奇心，继续深入探索，用我们所掌握的知识和技术，让机器更好地理解情感，从而构建一个更加智能和共情的世界。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://github.com/qmwneb946">qmwneb946</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://qmwneb946.dpdns.org/2025/07/25/2025-07-26-055209/">https://qmwneb946.dpdns.org/2025/07/25/2025-07-26-055209/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://opensource.org/licenses/MIT" target="_blank">MIT License</a> 许可协议。转载请注明来源 <a href="https://qmwneb946.dpdns.org" target="_blank">qmwneb946 的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/">计算机科学</a><a class="post-meta__tags" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/">自然语言处理中的情感分析</a></div><div class="post-share"><div class="social-share" data-image="/img/icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/07/25/2025-07-26-055304/" title="光的指引：深入解析可见光定位技术 (VLP)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">光的指引：深入解析可见光定位技术 (VLP)</div></div><div class="info-2"><div class="info-item-1">大家好，我是你们的老朋友 qmwneb946。今天，我们要深入探讨一项充满未来感的定位技术——可见光定位 (Visible Light Positioning, VLP)。在这个万物互联的时代，精准定位已经成为不可或缺的基础能力。GPS在室外表现卓越，但一旦进入室内，其信号衰减、多径效应等问题便使其束手无策。而VLP，正是解决室内“定位盲区”问题的一把利剑，它利用我们日常生活中无处不在的光线，为我们描绘出一幅全新的室内导航图景。 引言：在光影中寻找方向 想象一下，在巨大的工厂车间里，数千台AGV（自动导引车）精准无误地穿梭；在复杂的医院内部，医护人员能迅速找到急需的医疗设备；在拥挤的商场里，顾客可以被精确引导到特定商品面前；甚至在地下矿井或水下环境，光线也能成为指引方向的信标。这些场景的实现，都离不开高精度、高可靠的室内定位技术。 传统定位技术如Wi-Fi、蓝牙、UWB等各有优劣，但往往受限于电磁干扰、带宽、精度或部署成本。而可见光定位，以其独特的优势——利用LED照明灯具作为定位信标，通过光信号进行数据传输和位置感知——正逐步崭露头角。它不仅能提供厘米级的定位精度，还兼具通信、...</div></div></div></a><a class="pagination-related" href="/2025/07/25/2025-07-26-053714/" title="小样本学习的元学习方法：让AI学会“举一反三”"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">小样本学习的元学习方法：让AI学会“举一反三”</div></div><div class="info-2"><div class="info-item-1">你好，各位求知若渴的技术爱好者们！我是你们的老朋友 qmwneb946。今天，我们要深入探讨一个迷人且极具挑战性的领域：小样本学习 (Few-Shot Learning, FSL)，以及它背后的强大引擎——元学习 (Meta-Learning)。 在人工智能的黄金时代，深度学习以其惊人的数据拟合能力，在图像识别、自然语言处理等领域取得了里程碑式的成就。然而，这些辉煌往往建立在海量标注数据之上。想象一下，当我们面临医疗图像诊断、罕见物种识别、或是机器人需要快速学习新任务等场景时，标注数据稀缺成了横亘在AI面前的“卡脖子”难题。难道我们的智能系统只能被“大数据”喂养，而无法像人类一样“举一反三”，从寥寥数例中习得新知吗？ 小样本学习正是为了解决这一痛点而生。它旨在赋予机器学习模型在仅有少量甚至一个示例的情况下，快速学习新概念或新任务的能力。而元学习，作为一种“学会学习”的范式，正是实现这一宏伟目标的关键钥匙。它不仅仅是让模型学会识别猫狗，更是让模型学会如何去学习识别新的动物。 在这篇博客中，我将带你穿越小样本学习和元学习的奥秘。我们将从FSL的定义和挑战出发，逐步揭示元学习的核心思想...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/07/18/2025-07-18-082418/" title="机器学习算法的公平性问题：技术挑战与伦理困境"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">机器学习算法的公平性问题：技术挑战与伦理困境</div></div><div class="info-2"><div class="info-item-1">引言 机器学习 (ML) 正在迅速改变我们的世界，从医疗保健到金融，再到刑事司法系统，它的应用几乎无处不在。然而，随着 ML 系统的广泛部署，一个越来越令人担忧的问题浮出水面：公平性。  算法的输出可能反映并放大现有的社会偏见，导致对某些群体的不公平待遇。本文将深入探讨机器学习算法中的公平性问题，分析其技术根源和伦理困境，并探讨一些可能的解决方案。 偏见是如何进入机器学习模型的？ 机器学习模型的公平性问题并非源于算法本身的恶意，而是源于其训练数据的偏见。  这些偏见可能来自多种来源： 数据收集与标注  样本选择偏差 (Sampling Bias):  如果训练数据未能充分代表所有群体，模型就会学习到一个有偏的表示。例如，如果一个用于预测贷款偿还能力的模型主要基于白人申请人的数据，它可能会对少数族裔申请人产生不公平的负面预测。 测量偏差 (Measurement Bias):  数据收集过程中的错误或不一致也会引入偏见。例如，在犯罪预测模型中，如果某些社区的执法力度更大，导致该社区的犯罪数据被过度记录，模型就会对该社区产生负面偏见。 标注偏差 (Label Bias):  人工标注...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082438/" title="云计算中的数据安全与隐私：挑战与应对"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">云计算中的数据安全与隐私：挑战与应对</div></div><div class="info-2"><div class="info-item-1">云计算为企业和个人提供了强大的计算资源和数据存储能力，但也带来了新的安全与隐私挑战。本文将深入探讨云计算环境下的数据安全与隐私问题，分析其背后的技术机制，并提出一些有效的应对策略。 云计算安全风险剖析 云计算环境中，数据安全与隐私面临着多种威胁，主要包括： 数据泄露与丢失 这是最常见的风险之一。  数据可能由于云提供商的内部安全漏洞、恶意攻击（例如SQL注入、DDoS攻击）、员工失误或意外事件（例如硬件故障）而泄露或丢失。  对于敏感数据，例如医疗记录、金融信息和个人身份信息，这种风险尤为严重。 数据违规 数据违规是指未经授权访问或使用数据的情况。这可能导致数据被篡改、删除或用于非法目的。  法规遵从性（例如 GDPR, CCPA）的压力也使得数据违规的代价越来越高。 权限管理不足 缺乏细粒度的访问控制机制可能导致数据被未授权的个人或应用程序访问。  复杂的云环境中，权限的管理和审核是一个极大的挑战。 数据完整性问题 云环境中的数据完整性需要得到保障，确保数据没有被未经授权的修改或破坏。  这需要使用诸如哈希算法和数字签名等技术来验证数据的完整性。 数据合规性 不同国家和地区对数...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082429/" title="区块链技术与数字版权保护：一场技术与法律的博弈"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">区块链技术与数字版权保护：一场技术与法律的博弈</div></div><div class="info-2"><div class="info-item-1">大家好，我是你们的技术博主X，今天我们来聊一个非常热门的话题：区块链技术如何应用于数字版权保护。在数字内容飞速发展的时代，版权侵权问题日益严峻，传统的版权保护机制显得力不从心。而区块链技术，凭借其去中心化、不可篡改、透明等特性，为解决这一难题提供了新的思路。 区块链技术概述 首先，让我们简单回顾一下区块链技术的基本原理。区块链是一个由多个区块组成的链式数据库，每个区块包含一系列经过加密验证的交易记录。这些交易记录一旦被写入区块链，就无法被篡改或删除，保证了数据的完整性和安全性。  其核心技术包括：  密码学:  确保数据的安全性和完整性，例如哈希算法和数字签名。 共识机制:  例如工作量证明（PoW）和权益证明（PoS），用于维护区块链的统一性和安全性，防止恶意攻击。 分布式账本: 数据分布在多个节点上，提高了系统的容错性和安全性。  区块链如何保护数字版权 区块链技术可以为数字版权保护提供多种方案，主要体现在以下几个方面： 版权登记与确权 传统的版权登记流程繁琐且耗时，而区块链可以提供一个快速、透明的版权登记平台。创作者可以将作品的哈希值（作品的数字指纹）记录到区块链上，以此证...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082500/" title="物联网设备的网络安全协议：挑战与解决方案"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">物联网设备的网络安全协议：挑战与解决方案</div></div><div class="info-2"><div class="info-item-1">物联网 (IoT) 设备正以前所未有的速度渗透到我们生活的方方面面，从智能家居到工业自动化，再到医疗保健。然而，这种广泛的连接也带来了巨大的安全风险。由于物联网设备通常资源受限，安全性设计常常被忽视，导致它们成为网络攻击的理想目标。本文将深入探讨物联网设备面临的网络安全挑战，以及用于增强其安全性的各种协议和技术。 物联网安全面临的挑战 物联网设备的安全挑战与传统IT系统大相径庭，主要体现在以下几个方面： 资源受限 许多物联网设备具有有限的处理能力、内存和存储空间。这使得部署复杂的加密算法和安全协议变得困难，同时也增加了运行时开销。  运行资源消耗较大的安全软件可能会影响设备的性能甚至导致其崩溃。 设备异构性 物联网生态系统由各种各样的设备组成，这些设备运行不同的操作系统，使用不同的编程语言，并具有不同的安全特性。这种异构性使得实施统一的安全策略变得极其复杂。  很难找到一个适用于所有设备的通用安全解决方案。 数据隐私与安全 物联网设备通常会收集大量敏感数据，例如个人健康信息、位置数据和财务信息。保护这些数据的隐私和安全至关重要，但由于设备自身的安全缺陷和数据传输过程中的漏洞，这成...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082528/" title="量子计算对现代密码学的威胁：后量子密码学的挑战与机遇"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">量子计算对现代密码学的威胁：后量子密码学的挑战与机遇</div></div><div class="info-2"><div class="info-item-1">量子计算的飞速发展为许多领域带来了革命性的变革，但也对现有的密码体系构成了前所未有的挑战。本文将深入探讨量子计算如何威胁现代密码学，以及我们如何应对这一挑战。 量子计算的优势与密码学的困境 经典计算机基于比特，其值只能是 0 或 1。而量子计算机利用量子比特，可以同时表示 0 和 1 的叠加态，这使得它们能够进行并行计算，处理能力远超经典计算机。  这种巨大的计算能力为解决某些目前被认为是“不可解”的问题提供了可能性，其中就包括许多现代密码学的基石。 例如，RSA 算法，广泛应用于电子商务和安全通信，其安全性依赖于大数分解的困难性。经典计算机分解一个很大的数需要指数级的时间，因此被认为是安全的。然而，Shor 算法，一个在量子计算机上运行的算法，能够以多项式时间分解大数。这意味着，一台足够强大的量子计算机能够轻易破解 RSA 加密，从而威胁到大量的在线交易、数据安全以及国家安全。 同样，椭圆曲线密码学 (ECC)，另一种广泛使用的密码算法，其安全性也依赖于某些数学问题的复杂性。然而，量子计算机也能够有效地解决这些问题，例如离散对数问题。 Shor 算法与 Grover 算法：量子...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082537/" title="图论算法在社交网络分析中的应用"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">图论算法在社交网络分析中的应用</div></div><div class="info-2"><div class="info-item-1">社交网络已经成为我们生活中不可或缺的一部分。从Facebook和Twitter到微信和微博，这些平台连接着数十亿用户，产生着海量的数据。而理解这些数据，挖掘其背后的规律和价值，就需要借助强大的数学工具——图论。本文将深入探讨图论算法在社交网络分析中的多种应用。 社交网络的图表示 在图论中，社交网络可以被自然地表示为图 G=(V,E)G = (V, E)G=(V,E)，其中 VVV 代表用户集合（节点），EEE 代表用户之间的关系集合（边）。例如，在Facebook中，每个用户是一个节点，如果两个用户是朋友，则在他们之间存在一条无向边；在Twitter中，如果用户A关注用户B，则存在一条从A指向B的有向边。边的权重可以表示关系的强度（例如，朋友关系的亲密度，或者互动频率）。  这种图表示为我们分析社交网络提供了坚实的基础。 核心图论算法及其应用 社区发现 社区发现旨在将社交网络划分成多个紧密连接的社区（也称为集群）。这对于理解用户群体、推荐系统以及病毒式营销等都至关重要。常用的算法包括：  Louvain算法:  一种贪婪的启发式算法，通过迭代优化模块度来寻找最佳社区结构。模块度 ...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qmwneb946</div><div class="author-info-description">一个专注于技术分享的个人博客，涵盖编程、算法、系统设计等内容</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">1352</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1356</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qmwneb946"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qmwneb946" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:qmwneb946@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">代码与远方，技术与生活交织的篇章。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%95%E8%A8%80%EF%BC%9A%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E4%B8%AD%E7%9A%84%E2%80%9C%E5%BF%83%E8%B7%B3%E2%80%9D"><span class="toc-number">1.</span> <span class="toc-text">引言：文本数据中的“心跳”</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%9A%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%9A%84%E5%9F%BA%E7%9F%B3"><span class="toc-number"></span> <span class="toc-text">第一部分：情感分析的基石</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%EF%BC%9F"><span class="toc-number">1.</span> <span class="toc-text">什么是情感分析？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E8%87%B3%E5%85%B3%E9%87%8D%E8%A6%81%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">为什么情感分析至关重要？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%9A%E4%BC%A0%E7%BB%9F%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95"><span class="toc-number"></span> <span class="toc-text">第二部分：传统情感分析方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E8%AF%8D%E5%85%B8%E7%9A%84%E6%96%B9%E6%B3%95-Lexicon-based-Approaches"><span class="toc-number">1.</span> <span class="toc-text">基于词典的方法 (Lexicon-based Approaches)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%B9%E6%B3%95-Machine-Learning-Approaches"><span class="toc-number">2.</span> <span class="toc-text">基于机器学习的方法 (Machine Learning Approaches)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%97%B6%E4%BB%A3%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90"><span class="toc-number"></span> <span class="toc-text">第三部分：深度学习时代的情感分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5-Word-Embeddings"><span class="toc-number">1.</span> <span class="toc-text">词嵌入 (Word Embeddings)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Recurrent-Neural-Networks-RNN-%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93"><span class="toc-number">2.</span> <span class="toc-text">循环神经网络 (Recurrent Neural Networks - RNN) 及其变体</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Convolutional-Neural-Networks-CNN-for-Text"><span class="toc-number">3.</span> <span class="toc-text">卷积神经网络 (Convolutional Neural Networks - CNN) for Text</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Attention-Mechanism"><span class="toc-number">4.</span> <span class="toc-text">注意力机制 (Attention Mechanism)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer-%E6%A8%A1%E5%9E%8B%E4%B8%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.</span> <span class="toc-text">Transformer 模型与预训练语言模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%EF%BC%9A%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%9A%84%E8%BF%9B%E9%98%B6%E8%AF%9D%E9%A2%98%E4%B8%8E%E6%8C%91%E6%88%98"><span class="toc-number"></span> <span class="toc-text">第四部分：情感分析的进阶话题与挑战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E9%9D%A2%E7%BA%A7%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90-Aspect-Based-Sentiment-Analysis-ABSA"><span class="toc-number">1.</span> <span class="toc-text">方面级情感分析 (Aspect-Based Sentiment Analysis - ABSA)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90-Multimodal-Sentiment-Analysis"><span class="toc-number">2.</span> <span class="toc-text">多模态情感分析 (Multimodal Sentiment Analysis)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B7%A8%E9%A2%86%E5%9F%9F%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90-Cross-Domain-Sentiment-Analysis"><span class="toc-number">3.</span> <span class="toc-text">跨领域情感分析 (Cross-Domain Sentiment Analysis)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%9A%84%E6%8C%91%E6%88%98%E4%B8%8E%E6%9C%AA%E6%9D%A5%E6%96%B9%E5%90%91"><span class="toc-number">4.</span> <span class="toc-text">情感分析的挑战与未来方向</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">5.</span> <span class="toc-text">结论</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/26/hello-world/" title="Hello World">Hello World</a><time datetime="2025-07-26T07:58:51.118Z" title="发表于 2025-07-26 15:58:51">2025-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/26/%E5%8D%9A%E5%BC%88%E8%AE%BA%E5%9F%BA%E7%A1%80/" title="博弈论基础">博弈论基础</a><time datetime="2025-07-26T07:58:51.118Z" title="发表于 2025-07-26 15:58:51">2025-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/25/2025-07-26-075557/" title="细胞命运的守护者：深入探索蛋白质降解途径的精妙调控">细胞命运的守护者：深入探索蛋白质降解途径的精妙调控</a><time datetime="2025-07-25T23:55:57.000Z" title="发表于 2025-07-26 07:55:57">2025-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/25/2025-07-26-075347/" title="揭秘微观世界的无限可能：单细胞基因组测序技术深度解析">揭秘微观世界的无限可能：单细胞基因组测序技术深度解析</a><time datetime="2025-07-25T23:53:47.000Z" title="发表于 2025-07-26 07:53:47">2025-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/25/2025-07-26-075236/" title="细胞极性：生命微观世界的精巧蓝图与动态调控">细胞极性：生命微观世界的精巧蓝图与动态调控</a><time datetime="2025-07-25T23:52:36.000Z" title="发表于 2025-07-26 07:52:36">2025-07-26</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2025 By qmwneb946</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.2</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'qmwneb946/blog',
      'data-repo-id': 'R_kgDOPM6cWw',
      'data-category-id': 'DIC_kwDOPM6cW84CtEzo',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme)
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Giscus' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Giscus' === 'Giscus' || !false) {
    if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>