<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>AR 中的三维场景理解：连接虚拟与现实的桥梁 | qmwneb946 的博客</title><meta name="author" content="qmwneb946"><meta name="copyright" content="qmwneb946"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="引言 想象一下这样的场景：你戴上AR眼镜，眼前普通的客厅瞬间变身星际战场，激光束在咖啡桌旁呼啸而过，外星飞船在沙发上空盘旋，而你的宠物狗则好奇地追逐着一个虚拟的能量球。这一切之所以能够如此逼真且沉浸，不仅仅是因为卓越的图形渲染，更因为它背后强大的“三维场景理解”能力。 增强现实（Augmented Reality, AR）的魅力在于将虚拟信息无缝叠加到现实世界中。但要实现真正的“无缝”，虚拟内容绝">
<meta property="og:type" content="article">
<meta property="og:title" content="AR 中的三维场景理解：连接虚拟与现实的桥梁">
<meta property="og:url" content="https://qmwneb946.dpdns.org/2025/07/25/2025-07-26-050118/index.html">
<meta property="og:site_name" content="qmwneb946 的博客">
<meta property="og:description" content="引言 想象一下这样的场景：你戴上AR眼镜，眼前普通的客厅瞬间变身星际战场，激光束在咖啡桌旁呼啸而过，外星飞船在沙发上空盘旋，而你的宠物狗则好奇地追逐着一个虚拟的能量球。这一切之所以能够如此逼真且沉浸，不仅仅是因为卓越的图形渲染，更因为它背后强大的“三维场景理解”能力。 增强现实（Augmented Reality, AR）的魅力在于将虚拟信息无缝叠加到现实世界中。但要实现真正的“无缝”，虚拟内容绝">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qmwneb946.dpdns.org/img/icon.png">
<meta property="article:published_time" content="2025-07-25T21:01:18.000Z">
<meta property="article:modified_time" content="2025-07-26T08:21:24.401Z">
<meta property="article:author" content="qmwneb946">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="计算机科学">
<meta property="article:tag" content="AR中的三维场景理解">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qmwneb946.dpdns.org/img/icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "AR 中的三维场景理解：连接虚拟与现实的桥梁",
  "url": "https://qmwneb946.dpdns.org/2025/07/25/2025-07-26-050118/",
  "image": "https://qmwneb946.dpdns.org/img/icon.png",
  "datePublished": "2025-07-25T21:01:18.000Z",
  "dateModified": "2025-07-26T08:21:24.401Z",
  "author": [
    {
      "@type": "Person",
      "name": "qmwneb946",
      "url": "https://github.com/qmwneb946"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://qmwneb946.dpdns.org/2025/07/25/2025-07-26-050118/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'AR 中的三维场景理解：连接虚拟与现实的桥梁',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2845632165165414" crossorigin="anonymous"></script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="qmwneb946 的博客" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">qmwneb946 的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">AR 中的三维场景理解：连接虚拟与现实的桥梁</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">AR 中的三维场景理解：连接虚拟与现实的桥梁<a class="post-edit-link" href="https://github.com/qmwneb946/blog/edit/main/source/_posts/2025-07-26-050118.md" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-25T21:01:18.000Z" title="发表于 2025-07-26 05:01:18">2025-07-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-26T08:21:24.401Z" title="更新于 2025-07-26 16:21:24">2025-07-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/">计算机科学</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h2 id="引言">引言</h2>
<p>想象一下这样的场景：你戴上AR眼镜，眼前普通的客厅瞬间变身星际战场，激光束在咖啡桌旁呼啸而过，外星飞船在沙发上空盘旋，而你的宠物狗则好奇地追逐着一个虚拟的能量球。这一切之所以能够如此逼真且沉浸，不仅仅是因为卓越的图形渲染，更因为它背后强大的“三维场景理解”能力。</p>
<p>增强现实（Augmented Reality, AR）的魅力在于将虚拟信息无缝叠加到现实世界中。但要实现真正的“无缝”，虚拟内容绝不能只是漂浮在空中，它们必须知道自己身处何地，周围有哪些物体，这些物体的形状、位置甚至材质是什么。这就引出了AR领域最核心也最具挑战性的议题之一：三维场景理解。</p>
<p>三维场景理解是AR系统将2D像素数据转化为丰富的3D世界模型的过程。它赋予了AR系统“看懂”环境的能力，从而实现：</p>
<ul>
<li><strong>精确的跟踪与定位 (Tracking &amp; Localization)</strong>：知道用户和AR设备在真实世界中的位置和姿态。</li>
<li><strong>真实的遮挡 (Realistic Occlusion)</strong>：虚拟物体可以被真实物体遮挡，反之亦然。</li>
<li><strong>自然的交互 (Natural Interaction)</strong>：虚拟物体能够与真实环境中的表面、物体进行物理上的交互。</li>
<li><strong>沉浸的光照 (Immersive Lighting)</strong>：虚拟物体的光照能够匹配真实世界的光照条件。</li>
<li><strong>智能的内容放置 (Intelligent Content Placement)</strong>：虚拟内容能够被合理地放置在真实世界的表面或物体上。</li>
</ul>
<p>简而言之，没有三维场景理解，AR就只是简单的信息叠加，而有了它，AR才能真正成为连接虚拟与现实的强大工具。本文将深入探讨AR中三维场景理解的各个关键技术环节，从最基础的几何感知，到复杂的语义理解，再到未来的发展趋势。</p>
<h2 id="从二维像素到三维空间：基础感知">从二维像素到三维空间：基础感知</h2>
<p>一切始于相机捕获的二维图像。AR系统如何从这些扁平的像素中重建出立体世界呢？这需要一套精密的几何学和计算机视觉技术。</p>
<h3 id="相机模型与投影">相机模型与投影</h3>
<p>要理解三维空间，首先要理解相机是如何将三维世界投影到二维图像上的。针孔相机模型（Pinhole Camera Model）是计算机视觉中最常用的模型，它描述了这一投影过程。</p>
<p>一个三维世界点 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>l</mi><mi>d</mi></mrow></msub><mo>=</mo><mo stretchy="false">[</mo><msub><mi>X</mi><mi>w</mi></msub><mo separator="true">,</mo><msub><mi>Y</mi><mi>w</mi></msub><mo separator="true">,</mo><msub><mi>Z</mi><mi>w</mi></msub><msup><mo stretchy="false">]</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">P_{world} = [X_w, Y_w, Z_w]^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">or</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span> 通过相机透视投影，最终落在图像平面上的二维点 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi>i</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi></mrow></msub><mo>=</mo><mo stretchy="false">[</mo><mi>u</mi><mo separator="true">,</mo><mi>v</mi><msup><mo stretchy="false">]</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">p_{image} = [u, v]^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ima</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>。这个过程可以通过相机参数来描述：</p>
<ul>
<li><strong>内参 (Intrinsic Parameters)</strong>：描述相机本身的特性，如焦距 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>x</mi></msub><mo separator="true">,</mo><msub><mi>f</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">f_x, f_y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>)、主点坐标 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>x</mi></msub><mo separator="true">,</mo><msub><mi>c</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">c_x, c_y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>) 和畸变参数。这些参数通常被封装在一个内参矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 中：<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>K</mi><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>f</mi><mi>x</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>c</mi><mi>x</mi></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>f</mi><mi>y</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>c</mi><mi>y</mi></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">K = \begin{pmatrix}
f_x &amp; 0 &amp; c_x \\
0 &amp; f_y &amp; c_y \\
0 &amp; 0 &amp; 1
\end{pmatrix}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.6em;vertical-align:-1.55em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.05em;"><span class="pstrut" style="height:5.6em;"></span><span style="width:0.875em;height:3.600em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.875em" height="3.600em" viewBox="0 0 875 3600"><path d="M863,9c0,-2,-2,-5,-6,-9c0,0,-17,0,-17,0c-12.7,0,-19.3,0.3,-20,1
c-5.3,5.3,-10.3,11,-15,17c-242.7,294.7,-395.3,682,-458,1162c-21.3,163.3,-33.3,349,
-36,557 l0,84c0.2,6,0,26,0,60c2,159.3,10,310.7,24,454c53.3,528,210,
949.7,470,1265c4.7,6,9.7,11.7,15,17c0.7,0.7,7,1,19,1c0,0,18,0,18,0c4,-4,6,-7,6,-9
c0,-2.7,-3.3,-8.7,-10,-18c-135.3,-192.7,-235.5,-414.3,-300.5,-665c-65,-250.7,-102.5,
-544.7,-112.5,-882c-2,-104,-3,-167,-3,-189
l0,-92c0,-162.7,5.7,-314,17,-454c20.7,-272,63.7,-513,129,-723c65.3,
-210,155.3,-396.3,270,-559c6.7,-9.3,10,-15.3,10,-18z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.01em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-1.81em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-3.01em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.81em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.01em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.81em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.05em;"><span class="pstrut" style="height:5.6em;"></span><span style="width:0.875em;height:3.600em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.875em" height="3.600em" viewBox="0 0 875 3600"><path d="M76,0c-16.7,0,-25,3,-25,9c0,2,2,6.3,6,13c21.3,28.7,42.3,60.3,
63,95c96.7,156.7,172.8,332.5,228.5,527.5c55.7,195,92.8,416.5,111.5,664.5
c11.3,139.3,17,290.7,17,454c0,28,1.7,43,3.3,45l0,9
c-3,4,-3.3,16.7,-3.3,38c0,162,-5.7,313.7,-17,455c-18.7,248,-55.8,469.3,-111.5,664
c-55.7,194.7,-131.8,370.3,-228.5,527c-20.7,34.7,-41.7,66.3,-63,95c-2,3.3,-4,7,-6,11
c0,7.3,5.7,11,17,11c0,0,11,0,11,0c9.3,0,14.3,-0.3,15,-1c5.3,-5.3,10.3,-11,15,-17
c242.7,-294.7,395.3,-681.7,458,-1161c21.3,-164.7,33.3,-350.7,36,-558
l0,-144c-2,-159.3,-10,-310.7,-24,-454c-53.3,-528,-210,-949.7,
-470,-1265c-4.7,-6,-9.7,-11.7,-15,-17c-0.7,-0.7,-6.7,-1,-18,-1z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
</li>
<li><strong>外参 (Extrinsic Parameters)</strong>：描述相机在世界坐标系中的位姿，包括旋转矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span> 和平移向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span>。它们将世界坐标系中的点转换到相机坐标系中。</li>
</ul>
<p>投影过程可以概括为：</p>
<ol>
<li>将世界坐标系下的三维点 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>l</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{world}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">or</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 转换到相机坐标系下的三维点 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>c</mi><mi>a</mi><mi>m</mi><mi>e</mi><mi>r</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{camera}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">am</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">er</span><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>P</mi><mrow><mi>c</mi><mi>a</mi><mi>m</mi><mi>e</mi><mi>r</mi><mi>a</mi></mrow></msub><mo>=</mo><mi>R</mi><msub><mi>P</mi><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>l</mi><mi>d</mi></mrow></msub><mo>+</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">P_{camera} = R P_{world} + t
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">am</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">er</span><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">or</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span></span></p>
</li>
<li>通过内参矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 将 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>c</mi><mi>a</mi><mi>m</mi><mi>e</mi><mi>r</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{camera}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">am</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">er</span><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 投影到图像平面上的像素坐标 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi>i</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">p_{image}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ima</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>：<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mo>⋅</mo><msub><mi>p</mi><mrow><mi>i</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi></mrow></msub><mo>=</mo><mi>K</mi><msub><mi>P</mi><mrow><mi>c</mi><mi>a</mi><mi>m</mi><mi>e</mi><mi>r</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">s \cdot p_{image} = K P_{camera}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4445em;"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ima</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">am</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">er</span><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span> 是一个尺度因子。</li>
</ol>
<p>在实际AR应用中，我们通常需要逆向这个过程：给定图像中的像素点，结合相机参数，推断出其对应的三维空间信息。</p>
<h3 id="深度感知：获取三维维度">深度感知：获取三维维度</h3>
<p>二维图像只提供了宽度和高度信息，要构建三维模型，我们还需要深度（或距离）信息。获取深度是三维场景理解的基础。</p>
<h4 id="双目视觉-Stereo-Vision">双目视觉 (Stereo Vision)</h4>
<p>灵感来源于人眼。通过两台（或更多）相机从不同视角拍摄同一场景，利用视差（disparity）原理计算深度。视差是指同一个点在两张图像中的像素位置差异。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Z</mi><mo>=</mo><mfrac><mrow><mi>B</mi><mo>⋅</mo><mi>f</mi></mrow><mi>d</mi></mfrac></mrow><annotation encoding="application/x-tex">Z = \frac{B \cdot f}{d}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0574em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">d</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span></span></span></span> 是深度，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span> 是基线（两相机间距），<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> 是焦距，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span> 是视差。</p>
<p><strong>优点</strong>：被动式，可以在各种光照下工作。<br>
<strong>缺点</strong>：计算量大，对纹理匮乏区域（如白墙）效果差，对相机标定精度要求高。</p>
<h4 id="结构光-Structured-Light">结构光 (Structured Light)</h4>
<p>通过主动投射已知图案（如红外点阵或条纹）到物体表面，然后使用相机捕获反射的图案。由于投射图案的几何形状已知，通过分析图案在物体表面的形变，可以精确计算深度。<br>
<strong>代表设备</strong>：初代Microsoft Kinect、Apple的TrueDepth相机（用于Face ID）。<br>
<strong>优点</strong>：高精度，抗环境光干扰能力强。<br>
<strong>缺点</strong>：容易受到强光影响，在室外使用受限，分辨率通常不高。</p>
<h4 id="飞行时间-Time-of-Flight-ToF">飞行时间 (Time-of-Flight, ToF)</h4>
<p>ToF相机通过发射光脉冲（如红外光），并测量光从发射到被物体反射回来的时间差来计算距离。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>距离</mtext><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>⋅</mo><mtext>速度</mtext><mo>⋅</mo><mtext>时间</mtext></mrow><annotation encoding="application/x-tex">距离 = \frac{1}{2} \cdot 速度 \cdot 时间
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">距离</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0074em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">速度</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">时间</span></span></span></span></span></p>
<p><strong>代表设备</strong>：许多手机的后置ToF传感器，如iPad Pro、Samsung Galaxy系列。<br>
<strong>优点</strong>：直接测量深度，计算简单，对光照不敏感，可以获得较好的深度图。<br>
<strong>缺点</strong>：精度受限于时间测量单元，分辨率相对较低，容易受多径效应影响。</p>
<h4 id="单目深度估计-Monocular-Depth-Estimation">单目深度估计 (Monocular Depth Estimation)</h4>
<p>利用单个2D图像估计场景深度，这在几何上是一个病态问题。然而，深度学习的兴起使得这一技术成为可能。通过训练神经网络学习图像中的语义和结构信息，从而推断出场景的深度图。<br>
<strong>方法</strong>：</p>
<ul>
<li><strong>有监督学习</strong>：需要大量带有真实深度标注的数据集进行训练。</li>
<li><strong>自监督学习</strong>：利用多帧图像之间的几何一致性（如极线几何）进行训练，无需真实深度标注，大大扩展了训练数据的来源。</li>
</ul>
<p><strong>优点</strong>：只需要普通RGB相机，成本低，适用性广。<br>
<strong>缺点</strong>：精度通常不如主动式深度传感器，对场景复杂度和泛化能力有要求。</p>
<p>无论是哪种深度感知技术，它们都为AR系统提供了关键的第三维度信息，为后续的场景理解奠定了基础。</p>
<h2 id="核心：同步定位与地图构建-SLAM">核心：同步定位与地图构建 (SLAM)</h2>
<p>有了深度信息，AR系统开始尝试构建一个对自身位置和环境的实时理解。这正是同步定位与地图构建（Simultaneous Localization and Mapping, SLAM）技术的核心任务。SLAM让AR设备能够在未知环境中同时确定自己的位置和姿态（Localization），并增量式地构建环境的三维地图（Mapping）。</p>
<h3 id="SLAM-的基本原理">SLAM 的基本原理</h3>
<p>SLAM系统通常包含以下几个关键模块：</p>
<ol>
<li><strong>传感器数据采集</strong>：通常是RGB相机、深度相机或惯性测量单元（IMU）的数据流。</li>
<li><strong>视觉里程计 (Visual Odometry, VO)</strong> / <strong>前端</strong>：处理连续帧图像，估计相机在短时间内的运动（位姿变化），并生成局部地图。这是实时性的关键。
<ul>
<li><strong>特征点法</strong>：在图像中检测并跟踪显著的特征点（如SIFT, ORB），通过这些点的匹配来计算相机运动。</li>
<li><strong>直接法</strong>：不提取特征点，而是直接利用图像像素的灰度信息来优化相机位姿，对纹理要求低，但对光照变化敏感（如LSD-SLAM, DSO）。</li>
</ul>
</li>
<li><strong>后端优化</strong>：整合前端估计的位姿和局部地图，进行全局优化，修正累积误差。通常使用图优化（Graph Optimization）方法，将相机位姿和地图点作为节点，视觉测量作为边，通过最小化重投影误差来优化整个图。</li>
<li><strong>回环检测 (Loop Closure Detection)</strong>：识别相机是否回到了之前访问过的位置。一旦检测到回环，系统就能进行全局位姿图优化，消除长时间运行导致的漂移，保持地图的一致性。</li>
<li><strong>地图构建 (Mapping)</strong>：根据优化后的相机位姿和特征点，构建环境的三维表示。这可以是稀疏点云、稠密点云、网格模型（Mesh）或体素（Voxel）地图。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 伪代码：SLAM系统核心流程概念</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SLAMSystem</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, camera_params, imu_params=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.camera_params = camera_params</span><br><span class="line">        <span class="variable language_">self</span>.<span class="built_in">map</span> = Map() <span class="comment"># 存储地图点、关键帧等</span></span><br><span class="line">        <span class="variable language_">self</span>.current_pose = Pose() <span class="comment"># 当前相机位姿</span></span><br><span class="line">        <span class="variable language_">self</span>.keyframe_db = [] <span class="comment"># 关键帧数据库</span></span><br><span class="line">        <span class="variable language_">self</span>.loop_detector = LoopDetector()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_frame</span>(<span class="params">self, image, imu_data=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 1. 视觉里程计 (前端)</span></span><br><span class="line">        <span class="comment"># 估计当前帧与上一帧之间的相对运动</span></span><br><span class="line">        relative_pose = <span class="variable language_">self</span>.visual_odometry.track(image, <span class="variable language_">self</span>.last_image)</span><br><span class="line">        <span class="variable language_">self</span>.current_pose = <span class="variable language_">self</span>.current_pose.apply_transform(relative_pose)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果有IMU数据，进行视觉-惯性融合</span></span><br><span class="line">        <span class="keyword">if</span> imu_data:</span><br><span class="line">            <span class="variable language_">self</span>.current_pose = <span class="variable language_">self</span>.imu_integrator.fuse(<span class="variable language_">self</span>.current_pose, imu_data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 地图更新与关键帧选择</span></span><br><span class="line">        <span class="comment"># 根据当前位姿和观测，更新地图点</span></span><br><span class="line">        <span class="comment"># 如果是关键帧，则添加到关键帧数据库</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.is_keyframe(image, <span class="variable language_">self</span>.current_pose):</span><br><span class="line">            <span class="variable language_">self</span>.keyframe_db.add(image, <span class="variable language_">self</span>.current_pose)</span><br><span class="line">            <span class="variable language_">self</span>.<span class="built_in">map</span>.add_new_points(image, <span class="variable language_">self</span>.current_pose)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 回环检测</span></span><br><span class="line">        <span class="comment"># 检查当前帧是否与历史关键帧形成回环</span></span><br><span class="line">        loop_candidate = <span class="variable language_">self</span>.loop_detector.detect(image, <span class="variable language_">self</span>.keyframe_db)</span><br><span class="line">        <span class="keyword">if</span> loop_candidate:</span><br><span class="line">            <span class="comment"># 4. 后端优化 (图优化)</span></span><br><span class="line">            <span class="comment"># 进行全局的Bundle Adjustment或位姿图优化</span></span><br><span class="line">            <span class="variable language_">self</span>.backend_optimizer.optimize(<span class="variable language_">self</span>.<span class="built_in">map</span>, <span class="variable language_">self</span>.keyframe_db, loop_candidate)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.last_image = image</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.current_pose, <span class="variable language_">self</span>.<span class="built_in">map</span>.get_current_view()</span><br></pre></td></tr></table></figure>
<h3 id="SLAM-的类型">SLAM 的类型</h3>
<h4 id="视觉-SLAM-Visual-SLAM-V-SLAM">视觉 SLAM (Visual SLAM, V-SLAM)</h4>
<p>仅使用相机作为主要传感器。</p>
<ul>
<li><strong>特征点法 (Feature-based SLAM)</strong>：如著名的ORB-SLAM系列，通过提取和匹配ORB特征点来跟踪和建图，对图像噪声和运动模糊具有一定鲁棒性。</li>
<li><strong>直接法 (Direct SLAM)</strong>：如LSD-SLAM、DSO（Direct Sparse Odometry），直接利用图像的像素灰度信息进行位姿估计，避免了特征点提取和描述子的计算，在纹理丰富的场景中表现优秀，但对光照变化和相机内参非常敏感。</li>
</ul>
<h4 id="视觉惯性-SLAM-Visual-Inertial-SLAM-VI-SLAM">视觉惯性 SLAM (Visual-Inertial SLAM, VI-SLAM)</h4>
<p>融合了相机和惯性测量单元（IMU）的数据。IMU提供角速度和线加速度信息，可以弥补纯视觉SLAM在快速运动、光照变化或纹理缺失时的不足，提供更稳定的短期位姿估计。</p>
<ul>
<li><strong>松耦合 (Loose Coupling)</strong>：视觉和IMU数据分别处理，结果再进行融合。</li>
<li><strong>紧耦合 (Tight Coupling)</strong>：视觉和IMU数据在同一个优化框架中共同优化，如MSCKF (Multi-State Constraint Kalman Filter)、VINS-Mono。这种方法通常精度更高，鲁棒性更好，也是目前主流的VI-SLAM方案。</li>
</ul>
<h4 id="语义-SLAM-Semantic-SLAM">语义 SLAM (Semantic SLAM)</h4>
<p>这是SLAM发展的重要方向，将传统的几何SLAM与语义信息（如物体识别、场景分类）结合起来。它不仅知道“我在哪里”和“环境长什么样”，更知道“环境里有什么物体”，以及这些物体是什么类型。这为更高层次的AR交互奠定了基础。</p>
<h3 id="移动端-AR-中的-SLAM">移动端 AR 中的 SLAM</h3>
<p>苹果的ARKit和谷歌的ARCore是当前移动设备上主流的AR开发平台，它们都内置了先进的VI-SLAM能力：</p>
<ul>
<li>它们能够实时跟踪设备位姿。</li>
<li>能够检测水平面和垂直面。</li>
<li>能够估计环境光照。</li>
<li>ARKit 3及更高版本，以及ARCore 1.15及更高版本，还支持People Occlusion（人物遮挡）和Motion Capture（动作捕捉），这依赖于更深层次的场景理解，包括对人体骨骼和深度的估计。</li>
</ul>
<p>SLAM是AR体验的基石，它提供了虚拟内容在真实世界中稳定“附着”的能力。</p>
<h2 id="超越几何：语义场景理解">超越几何：语义场景理解</h2>
<p>仅仅知道环境的几何形状是不够的。为了实现更智能、更自然的AR体验，系统需要理解场景中“有什么”，以及这些“什么”之间有什么关系。这就是语义场景理解（Semantic Scene Understanding）的核心。</p>
<h3 id="为什么需要语义理解？">为什么需要语义理解？</h3>
<p>考虑以下AR场景：</p>
<ul>
<li><strong>智能放置</strong>：用户希望将一个虚拟杯子放在桌子上，而不是悬浮在空中或插入墙壁。系统需要识别出“桌子”这个物体。</li>
<li><strong>真实遮挡</strong>：虚拟角色应该被真实房间的墙壁或家具遮挡。系统需要区分前景物体和背景。</li>
<li><strong>情境感知</strong>：虚拟宠物需要避开椅子，并能跳到沙发上。系统需要知道哪些是可通行区域，哪些是障碍物。</li>
<li><strong>多用户共享</strong>：在多人AR游戏中，不同用户看到的虚拟内容应该在同一个语义环境中对齐。</li>
</ul>
<p>这些都要求AR系统具备超越点云和网格的语义信息。深度学习在这一领域发挥了革命性的作用。</p>
<h3 id="深度学习在场景理解中的应用">深度学习在场景理解中的应用</h3>
<h4 id="目标检测-Object-Detection">目标检测 (Object Detection)</h4>
<p>识别图像中特定物体的位置并用边界框标注。</p>
<ul>
<li><strong>任务</strong>：在2D图像中定位并分类物体。</li>
<li><strong>主流算法</strong>：
<ul>
<li><strong>两阶段检测器</strong>：如Faster R-CNN，先生成候选区域，再进行分类和边界框回归。</li>
<li><strong>单阶段检测器</strong>：如YOLO (You Only Look Once)、SSD，直接在图像中预测边界框和类别，速度更快。</li>
</ul>
</li>
<li><strong>AR应用</strong>：用于识别场景中的特定物体，如椅子、桌子、门等。</li>
</ul>
<h4 id="语义分割-Semantic-Segmentation">语义分割 (Semantic Segmentation)</h4>
<p>对图像中的每个像素进行分类，将其归属于某个预定义的语义类别（如“地板”、“墙壁”、“椅子”）。</p>
<ul>
<li><strong>任务</strong>：像素级的分类。</li>
<li><strong>主流算法</strong>：FCN (Fully Convolutional Networks)、U-Net、DeepLab系列等。这些网络通常包含编码器-解码器结构，能够输出与输入图像尺寸相同的像素级预测图。</li>
<li><strong>AR应用</strong>：
<ul>
<li><strong>精确的表面检测</strong>：识别地板、桌面等可放置虚拟内容的表面。</li>
<li><strong>背景/前景分离</strong>：实现人物或物体遮挡。</li>
<li><strong>区域属性识别</strong>：区分可通行区域和障碍物。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 伪代码：语义分割概念</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleSemanticSegmentationModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleSemanticSegmentationModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 简化版编码器-解码器结构</span></span><br><span class="line">        <span class="variable language_">self</span>.encoder = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">            <span class="comment"># ... 更多编码层 ...</span></span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.decoder = nn.Sequential(</span><br><span class="line">            <span class="comment"># ... 解码层，通常包括上采样 ...</span></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">64</span>, <span class="number">32</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, num_classes, kernel_size=<span class="number">1</span>) <span class="comment"># 最终输出每个像素的类别得分</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.encoder(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.decoder(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设已经训练好的模型和图像</span></span><br><span class="line"><span class="comment"># model = SimpleSemanticSegmentationModel(num_classes=NUM_CLASSES)</span></span><br><span class="line"><span class="comment"># model.load_state_dict(torch.load(&#x27;semantic_segmentation_model.pth&#x27;))</span></span><br><span class="line"><span class="comment"># model.eval()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 示例图像预处理</span></span><br><span class="line"><span class="comment"># transform = transforms.Compose([</span></span><br><span class="line"><span class="comment">#     transforms.ToTensor(),</span></span><br><span class="line"><span class="comment">#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])</span></span><br><span class="line"><span class="comment"># ])</span></span><br><span class="line"><span class="comment"># image = Image.open(&quot;your_room_image.jpg&quot;).convert(&quot;RGB&quot;)</span></span><br><span class="line"><span class="comment"># input_tensor = transform(image).unsqueeze(0) # 添加batch维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 预测</span></span><br><span class="line"><span class="comment"># with torch.no_grad():</span></span><br><span class="line"><span class="comment">#     output = model(input_tensor) # output shape: [1, num_classes, H, W]</span></span><br><span class="line"><span class="comment"># predicted_segmentation = torch.argmax(output, dim=1).squeeze(0) # [H, W]的类别索引图</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 根据类别索引渲染分割结果...</span></span><br></pre></td></tr></table></figure>
<h4 id="实例分割-Instance-Segmentation">实例分割 (Instance Segmentation)</h4>
<p>在语义分割的基础上，不仅区分像素类别，还区分同类物体的不同实例。例如，能够识别出图像中有“椅子A”和“椅子B”，而不是只有“椅子”。</p>
<ul>
<li><strong>任务</strong>：为每个物体实例生成独立的像素掩码。</li>
<li><strong>主流算法</strong>：Mask R-CNN是代表性算法，它在Faster R-CNN的基础上增加了并行的FCN分支来预测实例掩码。</li>
<li><strong>AR应用</strong>：更精细的交互，比如选中某个特定的椅子，而不是所有椅子。</li>
</ul>
<h4 id="全景分割-Panoptic-Segmentation">全景分割 (Panoptic Segmentation)</h4>
<p>这是语义分割和实例分割的结合，旨在为图像中的每个像素分配一个“事物”实例ID或一个“东西”类别ID。</p>
<ul>
<li><strong>任务</strong>：统一了对可数物体（实例）和不可数背景（语义）的理解。</li>
<li><strong>AR应用</strong>：提供最全面的场景理解，是未来高级AR体验的基石。</li>
</ul>
<h3 id="3D-语义分割">3D 语义分割</h3>
<p>除了2D图像的语义理解，直接对SLAM构建的3D点云或网格进行语义分割也至关重要。</p>
<ul>
<li><strong>点云语义分割</strong>：直接对三维点云中的每个点进行分类。
<ul>
<li><strong>主流网络</strong>：PointNet、PointNet++、KPConv等，专门为处理非结构化点云数据而设计。</li>
</ul>
</li>
<li><strong>体素/网格语义分割</strong>：将点云转换为规则的体素网格或多边形网格，然后在其上进行卷积操作进行语义分割。</li>
</ul>
<p>这些3D语义信息可以直接与SLAM地图融合，形成<strong>语义地图 (Semantic Map)</strong>，例如，地图中不仅有几何点，还有“地面”、“墙壁”、“桌子”等标签。</p>
<h3 id="场景图生成-Scene-Graph-Generation">场景图生成 (Scene Graph Generation)</h3>
<p>更进一步，场景图生成旨在理解场景中物体之间的关系（例如，“杯子在桌子上”、“人站在地板上”）。场景图用节点表示物体，用边表示它们之间的关系，为AR系统提供了高级别的语境理解。</p>
<p>通过这些深度学习驱动的语义理解技术，AR系统从一个单纯的几何定位器，转变为一个能够“看懂”世界的智能系统，从而为虚拟内容与现实环境的深度融合铺平道路。</p>
<h2 id="交互与真实感：场景理解的应用">交互与真实感：场景理解的应用</h2>
<p>三维场景理解的最终目标是提升AR体验的真实感和交互性。这些底层的技术成果如何被应用到我们所见的AR效果中呢？</p>
<h3 id="遮挡管理-Occlusion-Management">遮挡管理 (Occlusion Management)</h3>
<p>这是AR中最基础也最直观的真实感体现。当一个虚拟物体被真实物体遮挡时，它应该部分或全部消失。</p>
<ul>
<li><strong>基于深度缓冲 (Depth Buffer-based Occlusion)</strong>：如果AR设备能够获取到精确的深度图（如ToF或结构光相机），虚拟物体的渲染可以利用深度测试。如果虚拟像素的深度大于真实世界对应像素的深度，那么该虚拟像素就不应该被渲染。</li>
<li><strong>语义遮挡 (Semantic Occlusion)</strong>：在没有精确深度图的情况下，或为了更智能的遮挡，可以利用语义分割来判断哪些区域是前景物体（如人物、家具），哪些是背景。比如，当ARKit检测到人体时，它能够自动生成一个人物遮挡的掩码，使得虚拟内容可以被人物遮挡。</li>
</ul>
<h3 id="真实感光照与着色-Realistic-Lighting-and-Shading">真实感光照与着色 (Realistic Lighting and Shading)</h3>
<p>虚拟物体要融入真实场景，其光照必须与真实环境保持一致。</p>
<ul>
<li><strong>环境光估计 (Environmental Lighting Estimation)</strong>：AR系统通过分析相机图像或使用专门的光照传感器，估计真实世界的光照条件，包括主光源的方向、强度以及环境光的颜色和强度。
<ul>
<li><strong>球谐函数 (Spherical Harmonics)</strong>：一种常用的数学工具，用于表示环境光在各个方向的分布。</li>
<li><strong>环境光探头 (Environmental Light Probe)</strong>：通常是一个全景图，捕获环境的全部光照信息。</li>
</ul>
</li>
<li><strong>全局光照 (Global Illumination)</strong>：高级AR渲染器会考虑光线在虚拟和真实物体之间的多次反弹，模拟更真实的明暗和色彩溢出效果。</li>
<li><strong>材质估计 (Material Estimation)</strong>：如果AR系统能识别出真实物体的材质（例如，“这是玻璃”、“这是木头”），它就可以更好地模拟光线与这些材质的交互，从而影响虚拟物体的反射和折射。</li>
</ul>
<h3 id="物理模拟-Physics-Simulation">物理模拟 (Physics Simulation)</h3>
<p>虚拟物体不仅要看起来真实，还要动起来真实。这意味着它们需要遵守物理定律，并与真实世界的物体发生碰撞和反作用。</p>
<ul>
<li><strong>碰撞检测 (Collision Detection)</strong>：利用SLAM构建的几何地图和语义地图，AR系统能够识别出虚拟物体何时与真实世界的表面（如地板、桌面）或物体发生接触。</li>
<li><strong>物理引擎集成</strong>：将Unity、Unreal Engine等游戏引擎中内置的物理引擎与AR场景理解结合，使得虚拟物体能够根据重力下落、在表面上滚动、相互碰撞，甚至与真实物体相互作用。例如，一个虚拟球落在真实桌面上并弹跳。</li>
</ul>
<h3 id="AR-内容放置与交互-AR-Content-Placement-and-Interaction">AR 内容放置与交互 (AR Content Placement and Interaction)</h3>
<p>场景理解极大地提升了AR内容的智能放置和用户交互体验。</p>
<ul>
<li><strong>智能放置</strong>：
<ul>
<li><strong>平面检测 (Plane Detection)</strong>：ARKit和ARCore都内置了实时平面检测功能，能够识别水平面（如地板、桌面）和垂直面（如墙壁）。用户可以直接将虚拟物体“吸附”到这些表面上。</li>
<li><strong>语义区域放置</strong>：更高级的系统可以根据语义信息，将虚拟花瓶自动放置在“桌子”上，将虚拟画作放置在“墙壁”上，而不会放置在“天花板”上。</li>
</ul>
</li>
<li><strong>空间锚点 (Spatial Anchors)</strong>：将虚拟内容“锚定”到真实世界中的特定位置，即使设备移动或离开并返回，虚拟内容也能保持在原地。这通常结合了SLAM的回环检测和全局优化。</li>
<li><strong>手势与空间交互</strong>：结合手部追踪和场景理解，用户可以直接用手与虚拟物体或真实世界中的物体进行交互，例如，用手推动一个虚拟方块，或指向真实世界中的某个物体，系统能理解用户的意图。</li>
</ul>
<p>通过这些应用，三维场景理解将AR从一种新奇的技术演示，提升为一种真正具有实用价值和沉浸感的体验。</p>
<h2 id="挑战与未来方向">挑战与未来方向</h2>
<p>尽管三维场景理解在AR领域取得了显著进展，但它仍然面临诸多挑战，并且有广阔的未来发展空间。</p>
<h3 id="当前挑战">当前挑战</h3>
<ol>
<li><strong>鲁棒性与泛化能力</strong>：
<ul>
<li><strong>复杂环境</strong>：在弱光、强反光、无纹理、动态变化或高度杂乱的环境中，保持SLAM和深度感知的鲁棒性依然困难。</li>
<li><strong>动态场景</strong>：当前多数SLAM系统假定环境是静态的。对于移动的人、打开的门或变形的物体（如布料），处理起来仍然很棘手。</li>
</ul>
</li>
<li><strong>计算资源与能耗</strong>：
<ul>
<li><strong>移动设备限制</strong>：高质量的三维场景理解需要大量的计算资源和内存。如何在移动AR设备（如智能手机、轻量级AR眼镜）上实现实时、低功耗、高精度的性能，是一个持续的挑战。</li>
<li><strong>实时性</strong>：AR体验对实时性要求极高，任何卡顿或延迟都会破坏沉浸感。</li>
</ul>
</li>
<li><strong>长期稳定性与地图重用</strong>：
<ul>
<li><strong>漂移积累</strong>：尽管有回环检测，长时间运行的SLAM系统仍可能出现地图漂移。</li>
<li><strong>地图重用</strong>：如何在不同会话、不同用户之间共享和重用地图，以实现持续性的AR体验，是“持久化AR”的关键。</li>
</ul>
</li>
<li><strong>精细化理解</strong>：
<ul>
<li><strong>材质属性</strong>：理解物体的表面材质（如光滑、粗糙、透明）对于真实感渲染至关重要，但这远比识别物体种类更难。</li>
<li><strong>物体功能</strong>：理解物体的功能（例如，“椅子是用来坐的”，“门是用来进出的”）将使得AR交互更加智能。</li>
</ul>
</li>
<li><strong>隐私问题</strong>：
<ul>
<li><strong>环境扫描</strong>：AR设备持续扫描和理解用户周围的环境，这引发了数据收集和隐私保护的担忧。如何平衡功能与隐私是社会和技术层面都需要解决的问题。</li>
</ul>
</li>
</ol>
<h3 id="未来发展方向">未来发展方向</h3>
<ol>
<li><strong>神经隐式场景表示 (Neural Implicit Scene Representations)</strong>：
<ul>
<li><strong>Neural Radiance Fields (NeRF)</strong>：代表性技术，能够从少量2D图像中重建出极其逼真的3D场景，包括复杂的几何和光照信息。未来，将NeRF或其他神经表示与实时SLAM结合，有望生成更真实、更丰富的AR场景。</li>
</ul>
</li>
<li><strong>大模型与基础模型 (Foundation Models for Scene Understanding)</strong>：
<ul>
<li>类比于自然语言处理领域的GPT模型，未来可能会出现能够通用理解和生成复杂3D场景的基础模型。这些模型将能够处理多模态数据（图像、视频、点云、文本），并具备强大的泛化能力。</li>
</ul>
</li>
<li><strong>实时语义三维建图</strong>：
<ul>
<li>将高精度的几何SLAM与实时、稠密的语义分割深度融合，构建包含丰富语义信息的实时三维地图。这不仅仅是叠加标签，而是语义与几何的有机结合。</li>
</ul>
</li>
<li><strong>边缘计算与专用硬件</strong>：
<ul>
<li>随着AR眼镜等设备的普及，对在设备本地进行复杂计算的需求日益增加。未来将有更多AI芯片和边缘计算解决方案，专门优化AR场景理解任务，以降低延迟和能耗。</li>
</ul>
</li>
<li><strong>协同 AR 与数字孪生 (Collaborative AR &amp; Digital Twins)</strong>：
<ul>
<li>多用户共享同一AR体验，需要共享和实时同步的场景理解。数字孪生技术将真实世界与虚拟模型紧密关联，为AR提供了持续、精确的虚拟副本，使得AR体验能够跨越时间和空间。</li>
</ul>
</li>
<li><strong>具身智能 (Embodied AI) 与 AR</strong>：
<ul>
<li>结合机器人技术和AR，让AI代理（Agent）能够理解真实世界并与用户、环境进行物理交互，将是AR的终极形态之一。</li>
</ul>
</li>
</ol>
<h2 id="结论">结论</h2>
<p>三维场景理解是AR技术的核心与灵魂。它将AR从简单的信息显示推向了真正的沉浸式体验，使得虚拟内容不再是简单的叠加，而是能够与真实世界进行深度融合和智能交互的有机组成部分。</p>
<p>从最初的相机模型和深度感知，到复杂的SLAM技术，再到由深度学习驱动的语义理解，以及最终体现在遮挡、光照、物理交互等层面。每一步的进步都让我们离构建一个无缝连接物理与数字世界的未来更近一步。</p>
<p>当然，前方的道路依然充满挑战，但随着计算机视觉、深度学习、传感器技术和计算硬件的不断突破，我们有理由相信，未来的AR将拥有超乎想象的场景理解能力，彻底改变我们与数字世界和现实世界的交互方式。作为一名技术爱好者，我对此感到无比兴奋，并期待着亲身参与和见证这一激动人心的变革。虚拟与现实的边界将变得模糊，而三维场景理解正是连接它们的桥梁。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://github.com/qmwneb946">qmwneb946</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://qmwneb946.dpdns.org/2025/07/25/2025-07-26-050118/">https://qmwneb946.dpdns.org/2025/07/25/2025-07-26-050118/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://opensource.org/licenses/MIT" target="_blank">MIT License</a> 许可协议。转载请注明来源 <a href="https://qmwneb946.dpdns.org" target="_blank">qmwneb946 的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/">计算机科学</a><a class="post-meta__tags" href="/tags/AR%E4%B8%AD%E7%9A%84%E4%B8%89%E7%BB%B4%E5%9C%BA%E6%99%AF%E7%90%86%E8%A7%A3/">AR中的三维场景理解</a></div><div class="post-share"><div class="social-share" data-image="/img/icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/07/25/2025-07-26-050225/" title="量子计算在金融建模的应用：一场重塑华尔街的科技革命"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">量子计算在金融建模的应用：一场重塑华尔街的科技革命</div></div><div class="info-2"><div class="info-item-1"> 亲爱的技术爱好者们，你们好！我是 qmwneb946，一个对技术和数学充满热情的博主。今天，我们将共同踏上一段激动人心的旅程，探索一个正在悄然改变我们对计算和金融理解的交叉领域：量子计算在金融建模中的应用。 在过去几十年里，经典的计算机已经彻底改变了金融业。从高频交易到复杂的风险管理，从期权定价到投资组合优化，一切都离不开强大的计算能力。然而，随着金融市场的日益复杂、数据量的爆炸式增长以及对计算精度和速度的极限追求，经典的计算方法也逐渐暴露出其固有的局限性。许多金融问题，本质上是NP-hard问题，其计算复杂度随着问题规模的增长呈指数级上升，即使是当今最强大的超级计算机也束手无策。 正是在这样的背景下，量子计算——这一基于量子力学原理的新兴计算范式——以其独特的并行计算能力和处理复杂问题的潜力，走进了金融界的视野。它不仅仅是更快的经典计算机，而是一种全新的计算方式，承诺能够解决经典计算机无法触及的问题。那么，量子计算究竟如何赋能金融建模？它将带来哪些颠覆性的变革？又面临着怎样的挑战？本文将深入探讨这些问题，带你一窥这场即将重塑华尔街的科技革命。 量子计算基础回顾：从比特到量子比...</div></div></div></a><a class="pagination-related" href="/2025/07/25/2025-07-26-050026/" title="沉浸式治愈：VR中的虚拟现实疗法深度探索"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">沉浸式治愈：VR中的虚拟现实疗法深度探索</div></div><div class="info-2"><div class="info-item-1">引言 想象一下，你深陷恐惧症的泥沼，却无需面对真实的触发物；你承受着难以忍受的慢性疼痛，却能在一个奇妙的世界中找到片刻的宁静；你正在从严重的创伤中恢复，而一个虚拟的教练正耐心引导你重塑身体与认知能力。这一切，并非科幻小说中的场景，而是正在成为现实的“虚拟现实疗法”（Virtual Reality Therapy, VRT）。 作为一名热爱技术与数学的博主，qmwneb946 始终对那些能够跨越学科界限，为人类福祉带来实际改变的创新充满好奇。虚拟现实（VR）技术，通常被我们与游戏、娱乐和社交联系在一起，如今正以其独特的沉浸式体验，在医疗健康领域大放异彩。它不仅仅是提供一个“眼前的假象”，更在于构建一个可控、安全、高度个性化的数字疗愈空间。 本文将深入探讨VR疗法的核心概念、其在不同临床领域的应用、背后的技术栈以及我们当前面临的挑战与未来的展望。我们将剖析VR如何利用其独有的“存在感”和“沉浸感”来影响人类的心理和生理，并揭示驱动这些疗法成功的技术与数学原理。 VR疗法的核心概念与优势 虚拟现实疗法之所以能够有效，源于VR技术本身所具备的几个独特且强大的特性。这些特性使其在传统疗法无...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/07/18/2025-07-18-082429/" title="区块链技术与数字版权保护：一场技术与法律的博弈"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">区块链技术与数字版权保护：一场技术与法律的博弈</div></div><div class="info-2"><div class="info-item-1">大家好，我是你们的技术博主X，今天我们来聊一个非常热门的话题：区块链技术如何应用于数字版权保护。在数字内容飞速发展的时代，版权侵权问题日益严峻，传统的版权保护机制显得力不从心。而区块链技术，凭借其去中心化、不可篡改、透明等特性，为解决这一难题提供了新的思路。 区块链技术概述 首先，让我们简单回顾一下区块链技术的基本原理。区块链是一个由多个区块组成的链式数据库，每个区块包含一系列经过加密验证的交易记录。这些交易记录一旦被写入区块链，就无法被篡改或删除，保证了数据的完整性和安全性。  其核心技术包括：  密码学:  确保数据的安全性和完整性，例如哈希算法和数字签名。 共识机制:  例如工作量证明（PoW）和权益证明（PoS），用于维护区块链的统一性和安全性，防止恶意攻击。 分布式账本: 数据分布在多个节点上，提高了系统的容错性和安全性。  区块链如何保护数字版权 区块链技术可以为数字版权保护提供多种方案，主要体现在以下几个方面： 版权登记与确权 传统的版权登记流程繁琐且耗时，而区块链可以提供一个快速、透明的版权登记平台。创作者可以将作品的哈希值（作品的数字指纹）记录到区块链上，以此证...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082418/" title="机器学习算法的公平性问题：技术挑战与伦理困境"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">机器学习算法的公平性问题：技术挑战与伦理困境</div></div><div class="info-2"><div class="info-item-1">引言 机器学习 (ML) 正在迅速改变我们的世界，从医疗保健到金融，再到刑事司法系统，它的应用几乎无处不在。然而，随着 ML 系统的广泛部署，一个越来越令人担忧的问题浮出水面：公平性。  算法的输出可能反映并放大现有的社会偏见，导致对某些群体的不公平待遇。本文将深入探讨机器学习算法中的公平性问题，分析其技术根源和伦理困境，并探讨一些可能的解决方案。 偏见是如何进入机器学习模型的？ 机器学习模型的公平性问题并非源于算法本身的恶意，而是源于其训练数据的偏见。  这些偏见可能来自多种来源： 数据收集与标注  样本选择偏差 (Sampling Bias):  如果训练数据未能充分代表所有群体，模型就会学习到一个有偏的表示。例如，如果一个用于预测贷款偿还能力的模型主要基于白人申请人的数据，它可能会对少数族裔申请人产生不公平的负面预测。 测量偏差 (Measurement Bias):  数据收集过程中的错误或不一致也会引入偏见。例如，在犯罪预测模型中，如果某些社区的执法力度更大，导致该社区的犯罪数据被过度记录，模型就会对该社区产生负面偏见。 标注偏差 (Label Bias):  人工标注...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082438/" title="云计算中的数据安全与隐私：挑战与应对"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">云计算中的数据安全与隐私：挑战与应对</div></div><div class="info-2"><div class="info-item-1">云计算为企业和个人提供了强大的计算资源和数据存储能力，但也带来了新的安全与隐私挑战。本文将深入探讨云计算环境下的数据安全与隐私问题，分析其背后的技术机制，并提出一些有效的应对策略。 云计算安全风险剖析 云计算环境中，数据安全与隐私面临着多种威胁，主要包括： 数据泄露与丢失 这是最常见的风险之一。  数据可能由于云提供商的内部安全漏洞、恶意攻击（例如SQL注入、DDoS攻击）、员工失误或意外事件（例如硬件故障）而泄露或丢失。  对于敏感数据，例如医疗记录、金融信息和个人身份信息，这种风险尤为严重。 数据违规 数据违规是指未经授权访问或使用数据的情况。这可能导致数据被篡改、删除或用于非法目的。  法规遵从性（例如 GDPR, CCPA）的压力也使得数据违规的代价越来越高。 权限管理不足 缺乏细粒度的访问控制机制可能导致数据被未授权的个人或应用程序访问。  复杂的云环境中，权限的管理和审核是一个极大的挑战。 数据完整性问题 云环境中的数据完整性需要得到保障，确保数据没有被未经授权的修改或破坏。  这需要使用诸如哈希算法和数字签名等技术来验证数据的完整性。 数据合规性 不同国家和地区对数...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082500/" title="物联网设备的网络安全协议：挑战与解决方案"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">物联网设备的网络安全协议：挑战与解决方案</div></div><div class="info-2"><div class="info-item-1">物联网 (IoT) 设备正以前所未有的速度渗透到我们生活的方方面面，从智能家居到工业自动化，再到医疗保健。然而，这种广泛的连接也带来了巨大的安全风险。由于物联网设备通常资源受限，安全性设计常常被忽视，导致它们成为网络攻击的理想目标。本文将深入探讨物联网设备面临的网络安全挑战，以及用于增强其安全性的各种协议和技术。 物联网安全面临的挑战 物联网设备的安全挑战与传统IT系统大相径庭，主要体现在以下几个方面： 资源受限 许多物联网设备具有有限的处理能力、内存和存储空间。这使得部署复杂的加密算法和安全协议变得困难，同时也增加了运行时开销。  运行资源消耗较大的安全软件可能会影响设备的性能甚至导致其崩溃。 设备异构性 物联网生态系统由各种各样的设备组成，这些设备运行不同的操作系统，使用不同的编程语言，并具有不同的安全特性。这种异构性使得实施统一的安全策略变得极其复杂。  很难找到一个适用于所有设备的通用安全解决方案。 数据隐私与安全 物联网设备通常会收集大量敏感数据，例如个人健康信息、位置数据和财务信息。保护这些数据的隐私和安全至关重要，但由于设备自身的安全缺陷和数据传输过程中的漏洞，这成...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082528/" title="量子计算对现代密码学的威胁：后量子密码学的挑战与机遇"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">量子计算对现代密码学的威胁：后量子密码学的挑战与机遇</div></div><div class="info-2"><div class="info-item-1">量子计算的飞速发展为许多领域带来了革命性的变革，但也对现有的密码体系构成了前所未有的挑战。本文将深入探讨量子计算如何威胁现代密码学，以及我们如何应对这一挑战。 量子计算的优势与密码学的困境 经典计算机基于比特，其值只能是 0 或 1。而量子计算机利用量子比特，可以同时表示 0 和 1 的叠加态，这使得它们能够进行并行计算，处理能力远超经典计算机。  这种巨大的计算能力为解决某些目前被认为是“不可解”的问题提供了可能性，其中就包括许多现代密码学的基石。 例如，RSA 算法，广泛应用于电子商务和安全通信，其安全性依赖于大数分解的困难性。经典计算机分解一个很大的数需要指数级的时间，因此被认为是安全的。然而，Shor 算法，一个在量子计算机上运行的算法，能够以多项式时间分解大数。这意味着，一台足够强大的量子计算机能够轻易破解 RSA 加密，从而威胁到大量的在线交易、数据安全以及国家安全。 同样，椭圆曲线密码学 (ECC)，另一种广泛使用的密码算法，其安全性也依赖于某些数学问题的复杂性。然而，量子计算机也能够有效地解决这些问题，例如离散对数问题。 Shor 算法与 Grover 算法：量子...</div></div></div></a><a class="pagination-related" href="/2025/07/18/2025-07-18-082537/" title="图论算法在社交网络分析中的应用"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="info-item-2">图论算法在社交网络分析中的应用</div></div><div class="info-2"><div class="info-item-1">社交网络已经成为我们生活中不可或缺的一部分。从Facebook和Twitter到微信和微博，这些平台连接着数十亿用户，产生着海量的数据。而理解这些数据，挖掘其背后的规律和价值，就需要借助强大的数学工具——图论。本文将深入探讨图论算法在社交网络分析中的多种应用。 社交网络的图表示 在图论中，社交网络可以被自然地表示为图 G=(V,E)G = (V, E)G=(V,E)，其中 VVV 代表用户集合（节点），EEE 代表用户之间的关系集合（边）。例如，在Facebook中，每个用户是一个节点，如果两个用户是朋友，则在他们之间存在一条无向边；在Twitter中，如果用户A关注用户B，则存在一条从A指向B的有向边。边的权重可以表示关系的强度（例如，朋友关系的亲密度，或者互动频率）。  这种图表示为我们分析社交网络提供了坚实的基础。 核心图论算法及其应用 社区发现 社区发现旨在将社交网络划分成多个紧密连接的社区（也称为集群）。这对于理解用户群体、推荐系统以及病毒式营销等都至关重要。常用的算法包括：  Louvain算法:  一种贪婪的启发式算法，通过迭代优化模块度来寻找最佳社区结构。模块度 ...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">qmwneb946</div><div class="author-info-description">一个专注于技术分享的个人博客，涵盖编程、算法、系统设计等内容</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">1357</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1361</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/qmwneb946"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/qmwneb946" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:qmwneb946@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">代码与远方，技术与生活交织的篇章。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E4%BA%8C%E7%BB%B4%E5%83%8F%E7%B4%A0%E5%88%B0%E4%B8%89%E7%BB%B4%E7%A9%BA%E9%97%B4%EF%BC%9A%E5%9F%BA%E7%A1%80%E6%84%9F%E7%9F%A5"><span class="toc-number">2.</span> <span class="toc-text">从二维像素到三维空间：基础感知</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E6%9C%BA%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%8A%95%E5%BD%B1"><span class="toc-number">2.1.</span> <span class="toc-text">相机模型与投影</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E6%84%9F%E7%9F%A5%EF%BC%9A%E8%8E%B7%E5%8F%96%E4%B8%89%E7%BB%B4%E7%BB%B4%E5%BA%A6"><span class="toc-number">2.2.</span> <span class="toc-text">深度感知：获取三维维度</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8C%E7%9B%AE%E8%A7%86%E8%A7%89-Stereo-Vision"><span class="toc-number">2.2.1.</span> <span class="toc-text">双目视觉 (Stereo Vision)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E6%9E%84%E5%85%89-Structured-Light"><span class="toc-number">2.2.2.</span> <span class="toc-text">结构光 (Structured Light)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A3%9E%E8%A1%8C%E6%97%B6%E9%97%B4-Time-of-Flight-ToF"><span class="toc-number">2.2.3.</span> <span class="toc-text">飞行时间 (Time-of-Flight, ToF)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%95%E7%9B%AE%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1-Monocular-Depth-Estimation"><span class="toc-number">2.2.4.</span> <span class="toc-text">单目深度估计 (Monocular Depth Estimation)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%EF%BC%9A%E5%90%8C%E6%AD%A5%E5%AE%9A%E4%BD%8D%E4%B8%8E%E5%9C%B0%E5%9B%BE%E6%9E%84%E5%BB%BA-SLAM"><span class="toc-number">3.</span> <span class="toc-text">核心：同步定位与地图构建 (SLAM)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SLAM-%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="toc-number">3.1.</span> <span class="toc-text">SLAM 的基本原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SLAM-%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="toc-number">3.2.</span> <span class="toc-text">SLAM 的类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%86%E8%A7%89-SLAM-Visual-SLAM-V-SLAM"><span class="toc-number">3.2.1.</span> <span class="toc-text">视觉 SLAM (Visual SLAM, V-SLAM)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%86%E8%A7%89%E6%83%AF%E6%80%A7-SLAM-Visual-Inertial-SLAM-VI-SLAM"><span class="toc-number">3.2.2.</span> <span class="toc-text">视觉惯性 SLAM (Visual-Inertial SLAM, VI-SLAM)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%AD%E4%B9%89-SLAM-Semantic-SLAM"><span class="toc-number">3.2.3.</span> <span class="toc-text">语义 SLAM (Semantic SLAM)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A7%BB%E5%8A%A8%E7%AB%AF-AR-%E4%B8%AD%E7%9A%84-SLAM"><span class="toc-number">3.3.</span> <span class="toc-text">移动端 AR 中的 SLAM</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B6%85%E8%B6%8A%E5%87%A0%E4%BD%95%EF%BC%9A%E8%AF%AD%E4%B9%89%E5%9C%BA%E6%99%AF%E7%90%86%E8%A7%A3"><span class="toc-number">4.</span> <span class="toc-text">超越几何：语义场景理解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E8%AF%AD%E4%B9%89%E7%90%86%E8%A7%A3%EF%BC%9F"><span class="toc-number">4.1.</span> <span class="toc-text">为什么需要语义理解？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E5%9C%BA%E6%99%AF%E7%90%86%E8%A7%A3%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">4.2.</span> <span class="toc-text">深度学习在场景理解中的应用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-Object-Detection"><span class="toc-number">4.2.1.</span> <span class="toc-text">目标检测 (Object Detection)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2-Semantic-Segmentation"><span class="toc-number">4.2.2.</span> <span class="toc-text">语义分割 (Semantic Segmentation)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2-Instance-Segmentation"><span class="toc-number">4.2.3.</span> <span class="toc-text">实例分割 (Instance Segmentation)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%A8%E6%99%AF%E5%88%86%E5%89%B2-Panoptic-Segmentation"><span class="toc-number">4.2.4.</span> <span class="toc-text">全景分割 (Panoptic Segmentation)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3D-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2"><span class="toc-number">4.3.</span> <span class="toc-text">3D 语义分割</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E5%9B%BE%E7%94%9F%E6%88%90-Scene-Graph-Generation"><span class="toc-number">4.4.</span> <span class="toc-text">场景图生成 (Scene Graph Generation)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%A4%E4%BA%92%E4%B8%8E%E7%9C%9F%E5%AE%9E%E6%84%9F%EF%BC%9A%E5%9C%BA%E6%99%AF%E7%90%86%E8%A7%A3%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">5.</span> <span class="toc-text">交互与真实感：场景理解的应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%81%AE%E6%8C%A1%E7%AE%A1%E7%90%86-Occlusion-Management"><span class="toc-number">5.1.</span> <span class="toc-text">遮挡管理 (Occlusion Management)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9C%9F%E5%AE%9E%E6%84%9F%E5%85%89%E7%85%A7%E4%B8%8E%E7%9D%80%E8%89%B2-Realistic-Lighting-and-Shading"><span class="toc-number">5.2.</span> <span class="toc-text">真实感光照与着色 (Realistic Lighting and Shading)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%A9%E7%90%86%E6%A8%A1%E6%8B%9F-Physics-Simulation"><span class="toc-number">5.3.</span> <span class="toc-text">物理模拟 (Physics Simulation)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AR-%E5%86%85%E5%AE%B9%E6%94%BE%E7%BD%AE%E4%B8%8E%E4%BA%A4%E4%BA%92-AR-Content-Placement-and-Interaction"><span class="toc-number">5.4.</span> <span class="toc-text">AR 内容放置与交互 (AR Content Placement and Interaction)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8C%91%E6%88%98%E4%B8%8E%E6%9C%AA%E6%9D%A5%E6%96%B9%E5%90%91"><span class="toc-number">6.</span> <span class="toc-text">挑战与未来方向</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%93%E5%89%8D%E6%8C%91%E6%88%98"><span class="toc-number">6.1.</span> <span class="toc-text">当前挑战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AA%E6%9D%A5%E5%8F%91%E5%B1%95%E6%96%B9%E5%90%91"><span class="toc-number">6.2.</span> <span class="toc-text">未来发展方向</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">7.</span> <span class="toc-text">结论</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/26/hello-world/" title="Hello World">Hello World</a><time datetime="2025-07-26T08:21:24.408Z" title="发表于 2025-07-26 16:21:24">2025-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/26/%E5%8D%9A%E5%BC%88%E8%AE%BA%E5%9F%BA%E7%A1%80/" title="博弈论基础">博弈论基础</a><time datetime="2025-07-26T08:21:24.408Z" title="发表于 2025-07-26 16:21:24">2025-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/26/2025-07-26-081818/" title="深入解析量子信息处理的物理实现：从原理到前沿">深入解析量子信息处理的物理实现：从原理到前沿</a><time datetime="2025-07-26T00:18:18.000Z" title="发表于 2025-07-26 08:18:18">2025-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/26/2025-07-26-081652/" title="金融风险的传染模型：洞悉系统性危机的数学之美与工程实践">金融风险的传染模型：洞悉系统性危机的数学之美与工程实践</a><time datetime="2025-07-26T00:16:52.000Z" title="发表于 2025-07-26 08:16:52">2025-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/26/2025-07-26-081535/" title="动力系统中的分形吸引子：混沌之美与秩序">动力系统中的分形吸引子：混沌之美与秩序</a><time datetime="2025-07-26T00:15:35.000Z" title="发表于 2025-07-26 08:15:35">2025-07-26</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2025 By qmwneb946</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.2</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'qmwneb946/blog',
      'data-repo-id': 'R_kgDOPM6cWw',
      'data-category-id': 'DIC_kwDOPM6cW84CtEzo',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme)
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Giscus' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Giscus' === 'Giscus' || !false) {
    if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>